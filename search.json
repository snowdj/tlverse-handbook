[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Targeted Learning R: Causal Data Science tlverse Software\nEcosystem fully reproducible, open source, electronic handbook \napplying Targeted Learning methodology practice using software stack\nprovided tlverse ecosystem. work \ndraft phase publicly available solicit input community. \nview contribute, visit GitHub\nrepository.\n","code":""},{"path":"index.html","id":"outline","chapter":"Welcome","heading":"Outline","text":"contents handbook meant serve reference guide \napplied research teaching short courses illustrating successful\napplications Targeted Learning statistical paradigm. section\nintroduces set distinct causal inference questions, often motivated \ncase study, alongside statistical methodology open source software \nassessing scientific (causal) claim interest. set materials\ncurrently includesMotivation: need statistical\nrevolutionThe Roadmap introductory case study: WASH Benefits Bangladesh datasetIntroduction tlverse software\necosystemCross-validation origami\npackageEnsemble machine learning \nsl3 packageTargeted learning causal inference \ntmle3 packageOptimal treatments regimes \ntmle3mopttx packageStochastic treatment regimes \ntmle3shift packageCausal mediation analysis \ntmle3mediate packageCoda: need statistical\nrevolution","code":""},{"path":"index.html","id":"what-this-book-is-not","chapter":"Welcome","heading":"What this book is not","text":"book focus providing -depth technically sophisticated\ndescriptions modern statistical methodology recent advancements \nTargeted Learning. Instead, goal convey key details \nstate---art statistical techniques manner clear, complete, \nintuitive, simultaneously avoiding cognitive burden carried \nextraneous details (e.g., mathematically niche theoretical arguments). aim\npresentations herein serve coherent reference researchers\n– applied methodologists domain specialists alike – empower \ndeploy central statistical tools Targeted Learning manner efficient\nscientific pursuits. mathematically sophisticated treatment \ntopics, inclusive -depth technical details, field \nTargeted Learning, interested reader invited consult van der Laan Rose (2011)\nvan der Laan Rose (2018), among numerous works, appropriate. primary\nliterature causal inference, machine learning, non/semi-parametric\nstatistical theory include many recent advances Targeted Learning\nrelated areas. background causal inference, Hernán Robins (2022) serves\nintroductory modern reference.","code":""},{"path":"index.html","id":"about-the-authors","chapter":"Welcome","heading":"About the authors","text":"","code":""},{"path":"index.html","id":"mark-van-der-laan","chapter":"Welcome","heading":"Mark van der Laan","text":"Mark van der Laan Professor Biostatistics Statistics UC Berkeley\nco-director UC Berkeley’s Center Targeted Machine Learning \nCausal Inference. research interests include\nstatistical methods computational biology, survival analysis, censored data,\nadaptive designs, targeted maximum likelihood estimation, causal inference,\ndata-adaptive loss-based learning, multiple testing. research group\ndeveloped loss-based super learning semiparametric models, based \ncross-validation, generic optimal tool estimation \ninfinite-dimensional parameters, nonparametric density estimation \nprediction censored uncensored data. Building work, \nresearch group developed targeted maximum likelihood estimation target\nparameter data-generating distribution arbitrary semiparametric \nnonparametric models, generic optimal methodology statistical \ncausal inference. Since mid-2017, Mark’s group focused part \ndevelopment centralized, principled set software tools targeted\nlearning, tlverse.","code":""},{"path":"index.html","id":"jeremy-coyle","chapter":"Welcome","heading":"Jeremy Coyle","text":"Jeremy Coyle, PhD, consulting data scientist statistical programmer,\ncurrently leading software development effort produced \ntlverse ecosystem R packages related software tools. Jeremy earned \nPhD Biostatistics UC Berkeley 2017, primarily supervision\nAlan Hubbard.","code":""},{"path":"index.html","id":"nima-hejazi","chapter":"Welcome","heading":"Nima Hejazi","text":"Nima Hejazi Assistant Professor Biostatistics\nHarvard T.H. Chan School Public\nHealth. obtained PhD \nBiostatistics UC Berkeley, working Mark van der Laan Alan Hubbard,\nheld NSF Mathematical Sciences Postdoctoral Research Fellowship\nafterwards. Nima’s research interests blend causal inference, machine learning,\nnon- semi-parametric inference, computational statistics, areas \nrecent emphasis included causal mediation analysis; efficient estimation\nbiased, outcome-dependent sampling designs; sieve estimation \ncausal machine learning. methodological work motivated principally \nscientific collaborations clinical trials observational studies \ninfectious diseases, infectious disease epidemiology, computational\nbiology. Nima also passionate high-performance statistical computing\nopen source software design applied statistics statistical data\nscience.","code":""},{"path":"index.html","id":"ivana-malenica","chapter":"Welcome","heading":"Ivana Malenica","text":"Ivana Malenica Postdoctoral Researcher \nDepartment Statistics Harvard\nWojcicki Troper Data Science Fellow Harvard Data Science\nInitiative. obtained PhD \nBiostatistics UC Berkeley working Mark van der Laan, \nBerkeley Institute Data Science NIH Biomedical Big Data Fellow. \nresearch interests span non/semi-parametric theory, causal inference machine\nlearning, emphasis personalized health dependent settings. \ncurrent work involves causal inference time network dependence,\nonline learning, optimal individualized treatment, reinforcement learning, \nadaptive sequential designs.","code":""},{"path":"index.html","id":"rachael-phillips","chapter":"Welcome","heading":"Rachael Phillips","text":"Rachael Phillips PhD student biostatistics, advised Alan Hubbard \nMark van der Laan. MA Biostatistics, BS Biology, BA \nMathematics. student targeted learning, Rachael integrates causal\ninference, machine learning, statistical theory answer causal questions\nstatistical confidence. motivated issues arising healthcare,\nespecially interested clinical algorithm frameworks guidelines.\nRelated , also interested experimental design;\nhuman-computer interaction; statistical analysis pre-specification, automation,\nreproducibility; open-source software.","code":""},{"path":"index.html","id":"alan-hubbard","chapter":"Welcome","heading":"Alan Hubbard","text":"Alan Hubbard Professor Biostatistics UC Berkeley, current chair \nDivision Biostatistics UC Berkeley School Public Health, head \ndata analytics core UC Berkeley’s SuperFund research program, \nco-director UC Berkeley’s Center Targeted Machine Learning Causal\nInference. current research interests include\ncausal inference, variable importance analysis, statistical machine learning,\nestimation inference data-adaptive statistical target parameters, \ntargeted minimum loss-based estimation. Research group generally\nmotivated applications problems computational biology, epidemiology,\nprecision medicine.","code":""},{"path":"index.html","id":"reproduciblity","chapter":"Welcome","heading":"Reproduciblity","text":"tlverse software ecosystem growing collection packages, several \nquite early software lifecycle. team best \nmaintain backwards compatibility. work reaches completion, \nspecific versions tlverse packages used archived tagged \nproduce .book written using bookdown, complete\nsource available GitHub.\nversion book built R version 4.3.1 (2023-06-16),\npandoc version 2.19.2, \nfollowing packages:","code":""},{"path":"index.html","id":"learning-resources","chapter":"Welcome","heading":"Learning resources","text":"effectively utilize handbook, reader need fully trained\nstatistician begin understanding applying methods. However, \nhighly recommended reader understanding basic statistical\nconcepts confounding, probability distributions, confidence intervals,\nhypothesis tests, regression. Advanced knowledge mathematical statistics\nmay useful necessary. Familiarity R programming\nlanguage essential. also recommend understanding introductory\ncausal inference.learning R programming language recommend following (free)\nintroductory resources:Software Carpentry’s Programming \nRSoftware Carpentry’s R Reproducible Scientific\nAnalysisGarret Grolemund Hadley Wickham’s R Data\nScienceFor general, modern introduction causal inference, recommendMiguel . Hernán James M. Robins’ Causal Inference: \n(2022)Jason . Roy’s Crash Course Causality: Inferring Causal Effects \nObservational Data \nCourseraFeel free suggest \nresource!","code":""},{"path":"index.html","id":"want-to-help","chapter":"Welcome","heading":"Want to help?","text":"feedback book welcome. Feel free open \nissue, make Pull\nRequest spot typo.","code":""},{"path":"introduction.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"“One enemy robust science humanity – appetite \nright, tendency find patterns noise, see supporting\nevidence already believe true, ignore facts \nfit.”— Nature Editorial (Anonymous) (2015b)Scientific research unique point history. need improve\nrigor reproducibility greater ever. Corroboration moves science\nforward, yet growing alarm results reproduced \nvalidated, suggesting many discoveries may false less robust \ninitially claimed (Baker, 2016). failure meet need result \ndeclines rate scientific progress, tarnishing \nreputation scientific enterprise whole, erosion public’s\ntrust scientific findings (Munafò et al., 2017; Nature Editorial (Anonymous), 2015a).“key question want answer seeing results scientific\nstudy whether can trust data analysis.”— Peng (2015)Unfortunately, current state, culture statistical data analysis\nenables, rather precludes, manner human bias may affect \nresults (ideally objective) data analytic efforts. significant degree \nhuman bias enters statistical data analysis efforts form improper\nmodel selection. procedures estimation hypothesis testing \nderived based choice statistical model; thus, obtaining valid\nestimates statistical inference relies critically chosen model\ncontaining accurate representation process generated data.Consider, example, hypothetical study treatment assigned \ngroup patients – treatment assigned randomly, \ncharacteristics individuals (.e., “baseline covariates”) taken \naccount making treatment assignment decision? ’s , light \npatient characteristics heterogeneity clinician decision-making, \npatients assigned treatment arm uniformly receiving treatment?\nknowledge can – indeed, must – incorporated choice \nstatistical model. Alternatively, data arise observational\nstudy (“quasi-experiment”), (limited) control\ntreatment assignment mechanism. cases, available knowledge\ndata-generating process (DGP) even limited. \nsituations, statistical model contain possible distributions \ndata. practice, however, models selected based scientific\nknowledge available DGP; instead, models often selected based \n(1) philosophical leanings analyst, (2) relative convenience \nimplementation statistical methods admissible within choice model, \n(3) results significance testing (.e., p-values).practice “cargo-cult statistics — ritualistic miming statistics\nrather conscientious practice,” (Stark Saltelli, 2018) characterized \narbitrary modeling choices, even choices often result different\nanswers research question. opposed original purpose \nsafeguarding scientific process – providing formal techniques \nevaluating veracity claim using properly designed experiments data\ncollection procedures – Statistics increasingly often used “aid abet\nweak science, role can perform well used mechanically \nritual[istically]” (Stark Saltelli, 2018). current trend deriving scientific\ndiscoveries way abusing statistical methods helps explain modern\nepidemic false findings scientific research suffering\n(van der Laan Starmans, 2014).“suggest weak statistical understanding probably due \ninadequate”statistics lite” education. approach build \nappropriate mathematical fundamentals provide scientifically\nrigorous introduction statistics. Hence, students’ knowledge may remain\nimprecise, patchy, prone serious misunderstandings. approach\nachieves, however, providing students false confidence able\nuse inferential tools whereas usually interpret p-value\nprovided black box statistical software. educational problem\nremains unaddressed, poor statistical practices prevail regardless \nprocedures measures may favored /banned editorials.”— Szucs Ioannidis (2017)team University California, Berkeley uniquely positioned \nprovide education. Spearheaded Professor Mark van der Laan, \nnow spreading rapidly students colleagues greatly\nenriched field, aptly named “Targeted Learning” paradigm emphasizes \nfocus upon (.e., “targeting ”) scientific question motivating given\nstudy dataset. philosophy Targeted Learning runs counter \ncurrent cultural problem “convenience statistics,” opens door \nbiased estimation, misleading data analytic results, erroneous discoveries.\nTargeted Learning (TL) embraces fundamentals formalized field \nStatistics, notably including dual notions statistical model must\nrepresent real knowledge data-generating experiment target\nparameter (particular feature data-generating probability distribution)\nrepresents seek learn data (van der Laan Starmans, 2014). way,\nTL defines ground truth establishes principled standard inference,\nthereby curtailing opportunities --human biases (e.g., hindsight\nbias, confirmation bias, outcome bias) infiltrate efforts \nobjective data analysis.“key effective classical [statistical] inference \nwell-defined questions analysis plan tests questions.”— Nosek et al. (2018)Inspired loosely R.. Fisher’s influential classic Statistical Methods \nResearch Workers (Fisher, 1946), handbook aims provide\npractical training students, researchers, industry professionals, \nacademicians sciences (broadly considered – whether biological,\nphysical, economic, social), medicine public health, statistics, \nnumerous allied disciplines, equipping necessary knowledge\nskills utilize methodological developments TL. Targeted\nLearning paradigm encompasses principled set techniques, united single\nphilosophy, developing answers queries confidence, utilizing\nadvances causal inference, state---art non/semi-parametric statistical\ntheory, machine learning — every data analysis \nrealistic, reflecting appropriately known (unknown) \nprocess generated data,\n\nremaining fully compatible guiding principles computational\nreproducibility.Just conscientious use modern statistical methodology necessary \nensure scientific practice thrives — robust, well-tested software plays \ncritical role allowing practitioners access published results \ngiven scientific investigation. concur view put forth \nBuckheit Donoho (1995) “article…scientific publication \nscholarship , merely advertising scholarship. actual\nscholarship complete software development environment complete\nset instructions generated figures,” making availability \nadoption robust statistical software key enhancing transparency \ninherent (assumed) aspect scientific process.statistical methodology readily accessible practice, \ncrucial accompanied user-friendly software\n(Pullenayegum et al., 2016; Stromberg et al., 2004). tlverse software\necosystem, composed set packages R language environment \nstatistical computing (R Core Team, 2021), developed fulfill need TL\nmethodological framework. suite software tools\nfacilitate computationally reproducible efficient analyses, also \ntool TL education. Rather focusing implementing specific estimator\nsmall set related estimators, design paradigm tlverse\necosystem focuses exposing statistical framework Targeted Learning\n: software packages tlverse ecosystem directly model key\nobjects defined mathematical theoretical framework Targeted\nLearning. ’s , tlverse software packages share core set \ndesign principles centered extensibility, allowing used \nconjunction used cohesively building blocks \nformulating sophisticated statistical analyses. introduction TL\nframework, recommend Coyle et al. (2021)’s recent review\npaper.handbook, reader embark journey tlverse\necosystem. Guided R programming exercises, case studies, \nintuition-building explanations, readers learn use toolbox \napplying TL statistical methodology, translate real-world\ncausal analyses. preliminaries required prior learning endeavor\n– , provide list recommended learning resources.","code":""},{"path":"tlverse.html","id":"tlverse","chapter":"1 About the tlverse","heading":"1 About the tlverse","text":"","code":""},{"path":"tlverse.html","id":"what-is-the-tlverse","chapter":"1 About the tlverse","heading":"1.1 What is the tlverse?","text":"tlverse new framework Targeted Learning R, inspired \ntidyverse ecosystem R packages.analogy tidyverse:tidyverse opinionated collection R packages designed data\nscience. packages share underlying design philosophy, grammar, data\nstructures., tlverse isAn opinionated collection R packages Targeted Learning sharing \nunderlying design philosophy, grammar, core set data structures. \ntlverse aims provide tools building Targeted Learning-based\ndata analyses implementing novel, state---art Targeted Learning\nmethods.","code":""},{"path":"tlverse.html","id":"anatomy-of-the-tlverse","chapter":"1 About the tlverse","heading":"1.2 Anatomy of the tlverse","text":"Targeted Learning methods targeted maximum likelihood (minimum\nloss-based) estimators (.e., TMLEs). construction Targeted Learning\nestimator proceeds two-stage process:Flexibly learning particular components data-generating distribution\noften machine learning (e.g., Super Learning), resulting initial\nestimates nuisance parameters.Use carefully constructed parametric model-based update, via maximum\nlikelihood estimation (.e., MLE), incorporating initial estimates\nproduced prior step produce TML estimator.packages making core components tlverse software ecosystem\n– sl3 tmle3 – address two goals, respectively. Together, \ngeneral functionality exposed allows one build specific TMLEs\ntailored exactly particular statistical estimation problem.software packages make core tlverse aresl3: Modern Super Machine Learning\n? modern object-oriented implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.\n? design leverages modern ideas faster computation, \neasily extensible forward-looking, forms one cornerstones \ntlverse.\n? modern object-oriented implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.? design leverages modern ideas faster computation, \neasily extensible forward-looking, forms one cornerstones \ntlverse.tmle3: Engine Targeted Learning\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.\n? common interface engine accommodates current algorithmic\napproaches Targeted Learning yet remains flexible enough engine \npower implementation emerging statistical techniques \ndeveloped.\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.? common interface engine accommodates current algorithmic\napproaches Targeted Learning yet remains flexible enough engine \npower implementation emerging statistical techniques \ndeveloped.Beyond engines provide driving force behind tlverse, \nsupporting packages play important roles background:origami: Generalized Framework \nCross-Validation (Coyle Hejazi, 2018)\n? generalized framework flexible cross-validation.\n? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner ensemble modeling algorithm construction TML\nestimators.\n? generalized framework flexible cross-validation.? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner ensemble modeling algorithm construction TML\nestimators.delayed: Parallelization Framework \nDependent Tasks\n? framework delayed computations (.e., futures) based task\ndependencies.\n? Efficient allocation compute resources essential deploying\ncomputationally intensive algorithms large scale.\n? framework delayed computations (.e., futures) based task\ndependencies.? Efficient allocation compute resources essential deploying\ncomputationally intensive algorithms large scale.key principle tlverse extensibility. , software\necosystem aims support development novel Targeted Learning estimators\nreach maturity. achieve degree flexibility, follow \nmodel implementing new classes estimators, distinct causal inference\nproblems separate packages, rely upon core machinery\nprovided sl3 tmle3. currently three examples:tmle3mopttx: Optimal Treatments\ntlverse\n? Learn optimal rule estimate mean outcome rule.\n? Optimal treatments powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.\n? Learn optimal rule estimate mean outcome rule.? Optimal treatments powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.tmle3shift: Stochastic Shift\nInterventions based Modified Treatment Policies tlverse\n? Stochastic shift interventions evaluating changes \ncontinuous-valued treatments.\n? treatment variables binary categorical. Estimating \ntotal effects intervening continuous-valued treatments provides way\nprobe effect changes shifts treatment variable.\n? Stochastic shift interventions evaluating changes \ncontinuous-valued treatments.? treatment variables binary categorical. Estimating \ntotal effects intervening continuous-valued treatments provides way\nprobe effect changes shifts treatment variable.tmle3mediate: Causal Mediation\nAnalysis tlverse\n? Techniques evaluating direct indirect effects \ntreatments mediating variables.\n? Evaluating total effect treatment provide\ninformation pathways may operate. mediating\nvariables collected, one can instead evaluate direct indirect\neffect parameters speak action mechanism treatment.\n? Techniques evaluating direct indirect effects \ntreatments mediating variables.? Evaluating total effect treatment provide\ninformation pathways may operate. mediating\nvariables collected, one can instead evaluate direct indirect\neffect parameters speak action mechanism treatment.","code":""},{"path":"tlverse.html","id":"primer-on-the-r6-class-system","chapter":"1 About the tlverse","heading":"1.3 Primer on the R6 Class System","text":"tlverse designed using basic object oriented programming (OOP)\nprinciples R6 OOP framework. \n’ve tried make easy use tlverse packages without worrying much\nOOP, helpful intuition tlverse \nstructured. , briefly outline key concepts OOP. Readers\nfamiliar OOP basics invited skip section.","code":""},{"path":"tlverse.html","id":"classes-fields-and-methods","chapter":"1 About the tlverse","heading":"1.3.1 Classes, Fields, and Methods","text":"key concept OOP object, collection data functions\ncorresponds conceptual unit. Objects two main types \nelements:fields, can thought nouns, information object,\nandmethods, can thought verbs, actions object can\nperform.Objects members classes, define specific fields \nmethods . Classes can inherit elements classes (sometimes called\nbase classes) – accordingly, classes similar, exactly \n, can share parts definitions.Many different implementations OOP exist, variations \nconcepts implemented used. R several different implementations,\nincluding S3, S4, reference classes, R6. tlverse uses R6\nimplementation. R6, methods fields class object accessed using\n$ operator. thorough introduction R’s various OOP systems,\nsee http://adv-r..co.nz/OO-essentials.html, Hadley Wickham’s Advanced\nR (Wickham, 2014).","code":""},{"path":"tlverse.html","id":"object-oriented-programming-python-and-r","chapter":"1 About the tlverse","heading":"Object Oriented Programming: Python and R","text":"OO concepts (classes inheritance) baked Python first\npublished version (version 0.9 1991). contrast, R gets OO “approach”\npredecessor, S, first released 1976. first 15 years, S\nsupport classes, , suddenly, S got two OO frameworks bolted \nrapid succession: informal classes S3 1991, formal classes \nS4 1998. process continues, new OO frameworks periodically\nreleased, try improve lackluster OO support R, reference\nclasses (R5, 2010) R6 (2014). , R6 behaves like Python\nclasses (also like OOP focused languages like C++ Java), including\nmethod definitions part class definitions, allowing objects \nmodified reference.","code":""},{"path":"setup.html","id":"setup","chapter":"2 Software Setup","heading":"2 Software Setup","text":"","code":""},{"path":"setup.html","id":"setting-up-r-and-rstudio","chapter":"2 Software Setup","heading":"2.1 Setting up R and RStudio","text":"R RStudio separate downloads installations. R \nunderlying statistical computing environment. RStudio graphical integrated\ndevelopment environment (IDE) makes using R much easier \ninteractive. need install R install RStudio.","code":""},{"path":"setup.html","id":"windows","chapter":"2 Software Setup","heading":"2.1.1 Windows","text":"","code":""},{"path":"setup.html","id":"if-you-already-have-r-and-rstudio-installed","chapter":"2 Software Setup","heading":"If you already have R and RStudio installed:","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears console indicates version R \nrunning. Alternatively, can type sessionInfo(), also display\nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install . \ncan check \ninformation remove old versions system \nwish .","code":""},{"path":"setup.html","id":"if-you-dont-have-r-and-rstudio-installed","chapter":"2 Software Setup","heading":"If you don’t have R and RStudio installed:","text":"Download R \nCRAN website.Run .exe file just downloadedGo RStudio download pageUnder Installers select RStudio x.yy.zzz - Windows\nXP/Vista/7/8 (x, y, z represent version numbers)Double click file install itOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"setup.html","id":"macos-mac-os-x","chapter":"2 Software Setup","heading":"2.1.2 macOS / Mac OS X","text":"","code":""},{"path":"setup.html","id":"if-you-already-have-r-and-rstudio-installed-1","chapter":"2 Software Setup","heading":"If you already have R and RStudio installed:","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears terminal indicates version R running.\nAlternatively, can type sessionInfo(), also display \nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install .","code":""},{"path":"setup.html","id":"if-you-dont-have-r-and-rstudio-installed-1","chapter":"2 Software Setup","heading":"If you don’t have R and RStudio installed:","text":"Download R \nCRAN website.Select .pkg file latest R versionDouble click downloaded file install RIt also good idea install XQuartz (needed\npackages)Go RStudio download\npageUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)\n(x, y, z represent version numbers)Double click file install RStudioOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"setup.html","id":"linux","chapter":"2 Software Setup","heading":"2.1.3 Linux","text":"Follow instructions distribution\nCRAN, provide information\nget recent version R common distributions. \ndistributions, use package manager (e.g., Debian/Ubuntu run\nsudo apt-get install r-base, Fedora sudo yum install R), \ndon’t recommend approach versions provided \nusually date. case, make sure recent version\nR.Go RStudio download\npageUnder Installers select version matches distribution, \ninstall preferred method (e.g., Debian/Ubuntu sudo dpkg -  rstudio-x.yy.zzz-amd64.deb terminal).’s installed, open RStudio make sure works don’t get \nerror messages.setup instructions adapted written Data Carpentry: R\nData Analysis Visualization Ecological\nData.","code":""},{"path":"setup.html","id":"installtlverse","chapter":"2 Software Setup","heading":"2.2 Install tlverse","text":"tlverse ecosystem packages currently hosted \nhttps://github.com/tlverse, yet CRAN. \ncan use usethis package install :tlverse depends large number packages also hosted\nGitHub. , may see following error:just means R tried install many packages GitHub \nshort window. fix , need tell R use GitHub \nuser (’ll need GitHub user account). Follow two steps:Type usethis::browse_github_pat() R console, direct\nGitHub’s page create New Personal Access Token (PAT).Type usethis::browse_github_pat() R console, direct\nGitHub’s page create New Personal Access Token (PAT).Create PAT simply clicking “Generate token” bottom page.Create PAT simply clicking “Generate token” bottom page.Copy PAT, long string lowercase letters numbers.Copy PAT, long string lowercase letters numbers.Type usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio.\n.Renviron file pop-calling\nusethis::edit_r_environ(); try inputting\nSys.setenv(GITHUB_PAT = \"yourPAT\"), replacing PAT inside \nquotes. error, skip step 8.\nType usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio..Renviron file pop-calling\nusethis::edit_r_environ(); try inputting\nSys.setenv(GITHUB_PAT = \"yourPAT\"), replacing PAT inside \nquotes. error, skip step 8..Renviron file, type GITHUB_PAT= paste PAT \nequals symbol space..Renviron file, type GITHUB_PAT= paste PAT \nequals symbol space..Renviron file, press enter key ensure .Renviron\nends new line..Renviron file, press enter key ensure .Renviron\nends new line.Save .Renviron file. example shows syntax \nlook.Save .Renviron file. example shows syntax \nlook.Restart R. can restart R via drop-menu RStudio’s “Session”\ntab, located top RStudio interface. \nrestart R changes take effect!following steps, able successfully install \npackage threw error .","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"tlverse/tlverse\")Error: HTTP error 403.\n  API rate limit exceeded for 71.204.135.82. (But here's the good news:\n  Authenticated requests get a higher rate limit. Check out the documentation\n  for more details.)\n\n  Rate limit remaining: 0/60\n  Rate limit reset at: 2019-03-04 19:39:05 UTC\n\n  To increase your GitHub API rate limit\n  - Use `usethis::browse_github_pat()` to create a Personal Access Token.\n  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`.\nGITHUB_PAT=yourPAT"},{"path":"example-datasets.html","id":"example-datasets","chapter":"3 Example Datasets","heading":"3 Example Datasets","text":"","code":""},{"path":"example-datasets.html","id":"wash","chapter":"3 Example Datasets","heading":"3.1 WASH Benefits Bangladesh Study","text":"example data come study effect water quality, sanitation,\nhand washing, nutritional interventions child development rural\nBangladesh (WASH Benefits Bangladesh), cluster randomized controlled trial\n(Tofail et al., 2018). study enrolled pregnant women first second\ntrimester rural villages Gazipur, Kishoreganj, Mymensingh, \nTangail districts central Bangladesh, average eight women per\ncluster. Groups eight geographically adjacent clusters block randomized,\nusing random number generator, six intervention groups (\nreceived weekly visits community health promoter first 6 months\nevery 2 weeks next 18 months) double-sized control group (\nintervention health promoter visit). six intervention groups :chlorinated drinking water;improved sanitation;handwashing soap;combined water, sanitation, handwashing;improved nutrition counseling provision lipid-based nutrient\nsupplements; andcombined water, sanitation, handwashing, nutrition.handbook, concentrate child growth (size age) outcome \ninterest. reference, trial registered ClinicalTrials.gov \nregistration number NCT01590095.instructional purposes, start treating data independent \nidentically distributed (..d.) random draws large target population. \naccount clustering data (within sampled geographic units),\n, avoid details handbook sake clarity \nillustration. Modifications TL methodology biased samples, repeated\nmeasures, related complications, readily available.28 variables measured, single variable set \noutcome interest. outcome, \\(Y\\), weight--height Z-score\n(whz dat); treatment interest, \\(\\), randomized treatment\ngroup (tr dat); adjustment set (potential baseline confounders),\n\\(W\\), consists simply everything else. results observed data\nstructure \\(n\\) ..d. copies \\(O_i = (W_i, A_i, Y_i)\\), \\(= 1, \\ldots, n\\).Using skimr package, can\nquickly summarize variables measured WASH Benefits data set:(#tab:skim_washb_data)Data summaryVariable type: characterVariable type: numericA convenient summary relevant variables appears , complete \nsparkline visualizations describing marginal characteristics \ncovariate. Note asset variables reflect socioeconomic status \nstudy participants. Notice also uniform distribution treatment groups\n(twice many controls) – , course, design.","code":"\nlibrary(readr)\n# read in data via readr::read_csv\ndat <- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  )\n)"},{"path":"roadmap.html","id":"roadmap","chapter":"4 Learning from Data: A Roadmap","heading":"4 Learning from Data: A Roadmap","text":"Learning ObjectivesTranslate scientific questions statistical questions.Define statistical model based knowledge scientific experiment\nstudy generated data.Identify causal parameter function observed data distribution.Explain following statistical causal assumptions alongside \nimplications: independent identically distributed (..d.), consistency,\nunmeasured confounding, interference, positivity.","code":""},{"path":"roadmap.html","id":"introduction-1","chapter":"4 Learning from Data: A Roadmap","heading":"Introduction","text":"roadmap statistical learning concerned process translating\nreal-world scientific questions mathematical formalisms\nnecessary formulating relevant statistical inference problems.\ninvolves viewing data random variable (complete \nunderlying probability distribution), incorporating scientific knowledge \nchoice statistical model, selecting statistical target parameter \nrepresents answer scientific question interest, developing\nefficient estimators statistical estimand.","code":""},{"path":"roadmap.html","id":"roadmap-steps","chapter":"4 Learning from Data: A Roadmap","heading":"4.1 The Roadmap","text":"roadmap six-stage process:Define data random variable probability distribution, \\(O \\sim P_0\\)Specify statistical model \\(\\M\\) realistically, \\(P_0 \\\\M\\)Translate scientific question interest statistical target\nparameter \\(\\Psi\\) establish target populationChoose estimator \\(\\hat{\\Psi}\\) \\(\\Psi\\) realistic \\(\\M\\)Construct measure uncertainty estimate \\(\\hat{\\Psi}(P_n)\\)Make substantive conclusion","code":""},{"path":"roadmap.html","id":"data-a-random-variable-with-a-probability-distribution-o-sim-p_0","chapter":"4 Learning from Data: A Roadmap","heading":"(1) Data: A random variable with a probability distribution, \\(O \\sim P_0\\)","text":"dataset confronted collection results \nscientific (natural) experiment. can view data random variable;\n, experiment repeated, expect see \ndifferent realization data generated underlying law governing\nexperiment question. particular, experiment repeated many\ntimes, underlying probability distribution generating data, \\(P_0\\), \nrevealed. observed data single unit, \\(O\\), may thought \ndrawn probability distribution \\(P_0\\). often, \\(n\\)\nindependent identically distributed (..d.) observations random\nvariable \\(O\\) dataset. , observed data collection \\(O_1, \\ldots, O_n\\), subscripts denote individual observational units.\ndata ..d., certainly common case applied\ndata analysis. number techniques handling non-..d. data,\nincluding establishing conditional independence, conditional \nvariable (e.g., subject ID repeated measures data) ..d. assumption\nholds, incorporating inferential corrections repeated clustered\nobservations, name .","code":""},{"path":"roadmap.html","id":"the-empirical-probability-measure-p_n","chapter":"4 Learning from Data: A Roadmap","heading":"The empirical probability measure, \\(P_n\\)","text":"\\(n\\) ..d. observations hand, can define empirical probability\nmeasure, \\(P_n\\). empirical probability measure approximation \ntrue probability measure, \\(P_0\\), allowing us learn observed data.\nexample, can define empirical probability measure set \nvariables, say \\(W\\), proportion observations belong \\(W\\).\n,\n\\[\\begin{equation*}\n  P_n(W) = \\frac{1}{n}\\sum_{=1}^{n} \\(O_i \\W)\n\\end{equation*}\\]order understand scope learning particular dataset, next\nneed ask “know process led data’s\ngeneration?” brings us Step 2.","code":""},{"path":"roadmap.html","id":"defining-the-statistical-model-m-such-that-p_0-in-m","chapter":"4 Learning from Data: A Roadmap","heading":"(2) Defining the statistical model \\(\\M\\) such that \\(P_0 \\in \\M\\)","text":"statistical model \\(\\M\\) set possible probability distributions\ndescribe process observed data generated,\nappropriately constrained background scientific knowledge. Often, \\(\\M\\) \nnecessarily large (.e., non-parametric), reflecting fact \nstatistical knowledge \\(P_0\\) limited.\\(P_0\\) described finite number parameters, statistical\nmodel referred parametric. assumption made, example,\nproposition \\(O\\) Normal distribution mean \\(\\mu\\) \nvariance \\(\\sigma^2\\). generally, parametric model may defined \\[\\begin{equation*}\n  \\M(\\theta) = \\{P_{\\theta} : \\theta \\\\R^d \\},\n\\end{equation*}\\]\ndescribes constrained statistical model consisting distributions\n\\(P_{\\theta}\\) indexed finite, \\(d\\)-dimensional parameter\n\\(\\theta\\).assumption \\(P_0\\) specific, parametric form made quite\ncommonly. Unfortunately, even case assumptions \nsupported domain knowledge data-generating process. practice\noversimplification current, traditional, culture statistical\ndata analysis typically complicates entirely thwarts attempt reliably\nanswer scientific question hand. , ask? Consider much\nknowledge one must know (beyond shadow doubt) \ndata-generating distribution underlying given dataset , fact, governed \njust two parameters, case ubiquitously relied upon Normal\ndistribution. Similarly, main terms Cox proportional hazards, logistic\nregression, linear models imply highly constrained statistical model, \nassumptions unwarranted bias \nresult (except treatment randomized). philosophy used justify\nparametric assumptions rooted misinterpretations often-quoted\nsaying George Box, “models wrong useful,” \nirresponsibly used encourage data analyst make arbitrary modeling\nchoices. However, one makes unfounded assumptions, likely\n\\(\\M\\) contain \\(P_0\\), case statistical model said \nmisspecified. Statistical model misspecification introduces bias leads\nmisleading, unreliable results inference.result unwarranted assumptions oversimplifications practice \nstatistical data science starkly disparate answers \nscientific problem emerge. Practically, owed application \ndistinct statistical techniques differing modeling decisions \nassumptions made (communicated well) different data analysts. Even \nnascent days statistical data analysis, recognized “far\nbetter [develop] approximate answer right question…exact\nanswer wrong question, can always made precise”\n(Tukey, 1962), though traditional statistics failed heed advice \nnumber decades (Donoho, 2017). roadmap avoids bias defining\nstatistical model representation true data-generating\ndistribution underlying observed data. ultimate goal formulate \nstatistical estimation problem precisely (constraints imposed \navailable scientific knowledge), one can tailor estimation\nprocedure motivating scientific problem.crucial domain scientist(s) absolute clarity \nactually known process/experiment generated data, \ncommunicated data scientists much detail possible. \nknowledge rarely ground truth , instead comes form \nscientific conventions, accepted hypotheses, operational assumptions.\n\ndata scientist’s responsibility translate domain knowledge\nstatistical knowledge \\(P_0\\), define statistical model\n\\(\\M\\) respects known \\(P_0\\) makes \nrestrictions. manner, can ensure \\(P_0\\) contained \\(\\M\\),\nrefer generally defining realistic statistical model \\(\\M\\).Defining \\(\\M\\) realistically requires shift paradigm statistical\nproblem solving. Instead considering methods/software one familiar\ntrying solve problems toolbox, one must obtain \ndeep understanding experiment scientific question first \nformulate plan learning data way respects . \nrequires statisticians solid methodological theoretical\nfoundations, good communication skills, several meetings domain\nexperts typically required review details study, possibly refine\nquestion interest, translate technical details, interpret \nfindings way statistically correct agreeable \nnon-statistician domain experts. Unfortunately, communication \nstatisticians non-statistician researchers often fraught \nmisinterpretation. expected, expertise, \nproper communication underlying science motivating study can\nhelp ensure appropriate context given statistical data\nanalysis. roadmap provides principled mechanism learning data\nrealistically, learned data represents reliable \nreproducible approximation answer scientific question interest.\nroadmap provides rigorous method translating scientific knowledge\nquestions statistical framework can used learn data,\ninvaluable tool guide communication statisticians \nnon-statistician domain scientists. brings us next step \nroadmap, “trying learn data?”","code":""},{"path":"roadmap.html","id":"the-statistical-target-parameter-psi-and-statistical-estimand-psi_0","chapter":"4 Learning from Data: A Roadmap","heading":"(3) The statistical target parameter \\(\\Psi\\) and statistical estimand \\(\\psi_0\\)","text":"statistical target parameter, \\(\\Psi\\), defined mapping \nstatistical model, \\(\\M\\), parameter space. Usually, parameter space \nreal number (necessarily ), case can formally define \ntarget parameter mapping \\(\\Psi: \\M \\rightarrow \\R\\). statistical\nestimand may seen representation quantity wish learn\ndata, answer well-specified – often causal – question \ninterest particular target population. contrast ordinary\nstatistical estimands, causal estimands require extra set assumptions \nallow identification observed data. Based causal models\n(Hernán Robins, 2022; Pearl, 2009), identification assumptions \nuntestable must justified combination knowledge \nsystem study process experiment conducted. \nassumptions described greater detail following section causal\ntarget parameters.simple example, consider dataset containing observations survival\ntime every adult, question interest “’s \nprobability adult lives longer five years?” ,\\[\\begin{equation*}\n  \\psi_0 = \\Psi(P_0) = \\E_{P_0}(O > 5) = \\int_5^{\\infty} dP_0(o).\n\\end{equation*}\\]answer question statistical estimand,\n\\(\\Psi(P_0)=\\psi_0\\), quantity wish learn data. \ndiscussed , back--forth communication domain scientists \nstatisticians often required define \\(\\M\\) realistically, finalize\n\\(\\Psi\\) target population question supported \ndata. instance, say interested learning average effect \nheadache medication treating migraines adults learn one\nhigh blood pressure can receive medication. next meeting \ndomain scientists, might suggest target population modified \nadults without high blood pressure ask question involving dynamic\ntreatment within \\(\\Psi\\) adults high blood pressure never\nconsidered individuals receive treatment. defined \\(O\\),\n\\(\\M\\) realistically \\(\\Psi\\), formally defined statistical\nestimation problem. Next comes Step 4: “learn data \napproximate answer question interest?”","code":""},{"path":"roadmap.html","id":"the-estimator-hatpsi-and-estimate-psi_n","chapter":"4 Learning from Data: A Roadmap","heading":"(4) The estimator \\(\\hat{\\Psi}\\) and estimate \\(\\psi_n\\)","text":"obtain good approximation statistical estimand, need estimator\n\\(\\hat{\\Psi}\\), priori-specified algorithm defined mapping set\nset possible empirical distributions \\(P_n\\) (live \nnon-parametric statistical model \\(\\M_{NP}\\)) parameter space \ntarget parameter interest: \\(\\hat{\\Psi} : \\M_{NP} \\rightarrow \\R\\). \nwords, \\(\\hat{\\Psi}\\) function takes input observed data, \nrealization \\(P_n\\), outputs value parameter space. \nestimator may seen operator maps observed data’s corresponding\nempirical distribution value parameter space, numerical output\nproduced function estimate, \\(\\hat{\\Psi}(P_n)=\\psi_n\\). Thus,\n\\(\\psi_n\\) element parameter space informed empirical\nprobability distribution \\(P_n\\) observed data \\(O_1, \\ldots, O_n\\). \nplug realization \\(P_n\\) (based sample size \\(n\\) random\nvariable \\(O\\), get back estimate \\(\\psi_n\\) true parameter value\n\\(\\psi_0\\).\n\nmotivated step 2, imperative consider realistic\nstatistical models estimation. Therefore, flexible estimators allow \nparts data-generating process unrestricted necessary.\nSemiparametric theory empirical process theory provide framework \nconstructing, benchmarking, understanding behavior estimators \ndepend flexible estimation strategies realistic statistical models. \ngeneral, desirable properties estimator regular\nasymptotically linear (RAL) efficient, thereby admitting Normal limit\ndistribution minimal variance. Substitution/plug-RAL estimators \nalso advantageous: guaranteed remain within bounds \\(\\M\\) ,\nrelative estimators plug-, improved bias variance \nfinite samples. -depth discussion theory properties \navailable literature (e.g., Kennedy, 2016; van der Laan Rose, 2011). review key concepts following step.order quantify uncertainty estimate target parameter,\npart process conducting statistical inference, understanding \nsampling distribution estimator necessary. brings us Step 5:\n“confident statistical answer scientific\nquestion?”","code":""},{"path":"roadmap.html","id":"a-measure-of-uncertainty-for-the-estimate-psi_n","chapter":"4 Learning from Data: A Roadmap","heading":"(5) A measure of uncertainty for the estimate \\(\\psi_n\\)","text":"Since estimator \\(\\hat{\\Psi}\\) function empirical distribution\n\\(P_n\\), estimator random variable sampling distribution.\nTherefore, repeat experiment drawing \\(n\\) observations, \nevery time end different realization estimate. hypothetical\ndistribution estimates sampling distribution estimator.primary goal construction estimators able derive \nasymptotic sampling distribution theoretical analysis involving\nempirical process theory. regard, important property \nestimators focus asymptotic linearity. particular,\nasymptotic linearity states difference estimator \ntarget parameter (.e., truth) can represented, asymptotically, \naverage ..d. random variables plus asymptotically negligible remainder\nterm:\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) - \\Psi(P_0) = \\frac{1}{n} \\sum_{=1}^n IC(P_0)(O_i) +\n    o_p(n^{-1/2}),\n\\end{equation*}\\]\ninfluence curve (IC) function observed data \\(O\\) \nfunction defined underlying data-generating distribution \\(P_0\\).\nBased asymptotic approximation, Central Limit Theorem can used \nshow\\[\\begin{equation*}\n  \\sqrt{n} \\left(\\hat{\\Psi}(P_n) - \\Psi(P_0)\\right) \\sim N(0, \\sigma^2_{IC}),\n\\end{equation*}\\]\n\\(\\sigma^2_{IC}\\) variance \\(IC(P_0)(O)\\). Given estimate \n\\(\\sigma^2_{IC}\\), possible construct classic, asymptotically\naccurate Wald-type confidence intervals (CIs) hypothesis tests. \nexample, standard \\((1 - \\alpha)\\) CI takes form\\[\\begin{equation*}\n  \\psi_n \\pm Z \\frac{\\hat{\\sigma}_{IC}}{\\sqrt{n}} \\ ,\n\\end{equation*}\\]\n\\(Z\\) \\((1 - \\alpha / 2)^\\text{th}\\) quantile standard Normal\ndistribution. Following convention, often interested constructing\n95% two-tailed CIs, corresponding probability mass \\(\\alpha/2 = 0.025\\) \ntail limit distribution; thus, take \\(Z \\approx 1.96\\) \nquantile.Steps (1)–(5) roadmap define statistical analysis plan, \ncan done data revealed. last step roadmap involves\ninterpreting results obtained step (4) (5) therefore requires \ndata analyzed; however, additional analysis may take place \npart step (6) can pre-specified well. final step roadmap\naddresses question, “interpretation robustness \nstudy’s findings, conclusions can drawn ?”","code":""},{"path":"roadmap.html","id":"make-substantive-conclusion","chapter":"4 Learning from Data: A Roadmap","heading":"(6) Make substantive conclusion","text":"Making substantive conclusion involves interpreting study findings. \nalso provides opportunity ask follow-questions might addressed\nlater /discuss issues can inform future studies. Statistical\nestimands \\(\\psi_0\\) can statistical (noncausal) causal interpretations.\noften interest can provided. target population \nclearly mentioned interpretation, regardless whether ’s purely\nstatistical causal interpretation, curtail extrapolation results.major distinction statistical versus causal interpretations \nlatter relies untestable -called “identifiability” assumptions. \nfollowing section, review assumptions one--one. , focus \ninterpretation robustness study findings respect .\nSpecifically, causal target parameters estimated observed data\nwithout additional identifiability assumptions, validity \nresult’s causal interpretation hinges holding data. \nassumptions hold, larger causal gap, difference\nstatistical estimand causal estimand. perfect\nrandomized control trial loss follow-, causal gap zero\nstatistical causal estimands equivalent. Dı́az van der Laan (2013),\nnon-parametric sensitivity analysis assessing impact hypothesized\ncausal gaps estimates inference proposed. Gruber et al. (2023) \nGruber et al. (2022), example implementations methods proposed\nDı́az van der Laan (2013); particular, difference adjusted \nunadjusted effect estimates used define range possible causal gaps\nrelative difference. question interest causal, \nmodel-free sensitivity analysis (possibly complement sensitivity\nanalyses) recommended assess robustness study findings.","code":""},{"path":"roadmap.html","id":"roadmap-summary","chapter":"4 Learning from Data: A Roadmap","heading":"4.2 Summary of the Roadmap","text":"Data collected across \\(n\\) ..d. units, \\(O_1, \\ldots, O_n\\), may viewed \ncollection random variables arising underlying probability\ndistribution \\(\\P_0\\). expressed denoting collection data \ngenerated \\(O_1, \\ldots, O_n \\sim P_0\\). Domain knowledge \nexperiment generated data (e.g., treatment randomized, \ntreatment decision loss follow-depended subset covariates,\ntime ordering variables added data) translated \nstatistician / data scientist define statistical model \\(\\M\\), \npostulated space candidate probability distributions supposed \ncontain \\(P_0\\). particular, roadmap emphasizes critical role \ndefining \\(\\M\\) \\(P_0\\) guaranteed encapsulated , \\(P_0 \\\\M\\). limiting \\(\\M\\) based domain knowledge experiment (.e.,\nreality) — opposed constraining unrealistically (e.g., assuming \nrestrictive functional form, like main terms linear/logistic model, describes\n\\(P_0\\)) — can ensured \\(P_0 \\\\M\\), refer defining\nrealistic statistical model. Often, knowledge can used constrain\n\\(\\M\\) limited, \\(\\M\\) must large define \\(P_0 \\\\M\\); hence, realistic statistical models often termed semi- \nnon-parametric, since large indexed finite-dimensional\nset parameters. Necessarily, statistical query must begin , “\ntrying learn data?”, question whose answer captured \nstatistical target parameter, \\(\\Psi\\), function defined true\ndata-generating distribution \\(P_0\\), maps \\(\\M\\) statistical\nestimand, \\(\\psi_0\\). stage, statistical estimation problem \nformally defined, allowing use statistical theory guide \nconstruction estimators, algorithms approximate answer \nquestion interest learning data. Desirable properties \nestimator unbiased, efficient, plug-, robust finite\nsamples. question interest causal, model-free sensitivity\nanalysis recommended assess robustness study’s findings \nvarious hypothesized causal gaps.","code":""},{"path":"roadmap.html","id":"causal","chapter":"4 Learning from Data: A Roadmap","heading":"4.3 Causal Target Parameters","text":"many cases, interested problems ask questions regarding \ncausal effect intervention, whether assigned treatment (e.g., \nprescribed drug) “naturally occurring” exposure (e.g., pollution \nnearby factory), future outcome interest. causal effects may \ndefined summaries population interest (e.g., population mean \nparticular outcome) contrasting interventions (e.g., comparing treated\nuntreated condition). example, causal effect defined \nmean difference disease outcome two causal contrasts,\ncounterfactual cases study population set uniformly\nexperience low pollution levels pollutant, \npopulation set uniformly experience high levels pollutant.different ways operationalizing theoretical experiments \ngenerate counterfactual data necessary describing causal contrasts\ninterest. simply assume counterfactual outcomes exist \ntheory treatment contrasts interest (Imbens Rubin, 2015; Neyman, 1938; Rubin, 2005), may encoded -called “science\ntables”. Alternatively, consider interventions structural causal\nmodels (SCMs) (Pearl, 1995, 2009), may represented\ndirected acyclic graphs (DAGs). frameworks allow known \nhypothesized set relationships variables system study \nencoded mathematically formalized.","code":""},{"path":"roadmap.html","id":"the-causal-model","chapter":"4 Learning from Data: A Roadmap","heading":"The Causal Model","text":"Throughout, focus use DAGs SCMs description \ncausal parameters. Estimators statistical parameters correspond, \nstandard untestable identifiability assumptions, causal\nparameters introduced . DAGs particularly useful tool \nvisually expressing know causal relations among variables \nsystem study. Ignoring exogenous \\(U\\) terms (explained ), \nassume following ordering variables compose observed data\n\\(O\\). demonstrate construction DAG using DAGitty\n(Textor et al., 2011):DAGs like provide convenient means express \ncausal relations variables, causal relations can \nequivalently represented SCM:\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= f_A(W, U_A) \\\\\n  Y &= f_Y(W, , U_Y),\n\\end{align*}\\]\n\\(f\\)’s unspecified deterministic functions generate \ncorresponding random variables function variable’s “parents” (.e.,\nupstream nodes arrows given random variable) DAG, \nunobserved, exogenous error terms (.e., \\(U\\)’s). SCM may thought \nrepresentation algorithm produces data, \\(O\\), population\ninterest. Much statistics data science devoted discovering\nproperties system equations (e.g., estimation functional form\n\\(f_Y\\) governing outcome variable \\(Y\\)).first hypothetical experiment consider assigning exposure \nentire population observing outcome, withholding exposure \npopulation observing outcome. corresponds comparison\noutcome distribution population two distinct interventions:\\(\\) set \\(1\\) individuals, \\(\\) set \\(0\\) individuals.interventions may thought operations imply changes\nstructural equations system study. case \\(= 1\\), \n\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 1 \\\\\n  Y(1) &= f_Y(W, 1, U_Y) \\ ,\n\\end{align*}\\]\n, case \\(=0\\),\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 0 \\\\\n  Y(0) &= f_Y(W, 0, U_Y) \\ .\n\\end{align*}\\]equations, \\(\\) longer function \\(W\\) intervention\nsystem set \\(\\) deterministically\none values \\(1\\) \\(0\\) consistent intervention performed. \nnew symbols \\(Y(1)\\) \\(Y(0)\\) indicate values outcome variable \ntake population interest generated removing \ncontribution \\(\\) \\(f_Y\\) instead setting \\(\\) values \\(1\\) \\(0\\),\nrespectively. variables \\(Y(1)\\) \\(Y(0)\\) often called counterfactuals\n(since arise interventions run contrary fact) , \nframeworks, called potential outcomes \\(Y\\)\n[Neyman (1938); rubin2005causal; imbens2015causal]. difference \ncounterfactual means outcome two interventions defines \nwell known causal parameter often called “average treatment\neffect” (ATE) denoted\\[\\begin{equation}\n  ATE = \\E_X[Y(1) - Y(0)],\n  \\tag{4.1}\n\\end{equation}\\]\n\\(\\E_X(\\cdot)\\) expectation taken theoretical (unobservable)\nfull data (.e., \\(X = (W, Y(1), Y(0))\\)) distribution \\(P_X\\). Note full\ndata structure \\(X\\) , definition, unobservable since one can never\nobserve \\(Y(1)\\) \\(Y(0)\\) observational unit.can define much complicated interventions SCMs, interventions\nbased upon dynamic rules (assign particular interventions based \nfunction covariates \\(W\\)), stochastic rules (can even account \nnatural value \\(\\) observed absence intervention), much\n. results different target causal parameter entails different\nidentifiability assumptions discussed .","code":"\nlibrary(dagitty)\nlibrary(ggdag)\n\n# make DAG by specifying dependence structure\ndag <- dagitty(\n  \"dag {\n    W -> A\n    W -> Y\n    A -> Y\n    W -> A -> Y\n  }\"\n)\nexposures(dag) <- c(\"A\")\noutcomes(dag) <- c(\"Y\")\ntidy_dag <- tidy_dagitty(dag)\n\n# visualize DAG\nggdag(tidy_dag) +\n  theme_dag()"},{"path":"roadmap.html","id":"identifiability","chapter":"4 Learning from Data: A Roadmap","heading":"Identifiability","text":"Since can never simultaneously observe \\(Y(0)\\), counterfactual outcome\n\\(=0\\), \\(Y(1)\\), counterfactual outcome \\(=1\\), \nestimate difference \\(Y(1) - Y(0)\\) (individual treatment effect), \nappears Equation (4.1) (inside expectation \\(\\E_X(\\cdot)\\) \ndefines ATE). called Fundamental Problem Causal Inference\n(Holland, 1986). Thus, one primary activities causal\ninference identify assumptions necessary express causal\nquantities interest functions data-generating distribution \nobserved data. , must make assumptions quantities\nmay estimated observed data \\(O \\sim P_0\\) corresponding\ndata-generating distribution \\(P_0\\). Fortunately, given causal model\nspecified SCM , can, handful untestable assumptions,\nestimate ATE observational data. assumptions may summarized \nfollows.Definition 4.1  (Consistency) outcome unit \\(\\) \\(Y_i()\\) whenever \\(A_i = \\), may thought \n“versions treatment” “side effects treatment.”Definition 4.2  (Interference) outcome unit \\(\\), \\(Y_i\\), affected exposure unit \\(j\\),\n\\(A_j\\), \\(\\neq j\\).Definition 4.3  (Unmeasured Confounding) \\(\\perp Y() \\mid W\\) \\(\\\\mathcal{}\\), states \npotential outcomes \\((Y() : \\\\mathcal{})\\) arise independently \nexposure status \\(\\), conditional observed covariates \\(W\\). \nanalog randomization assumption data arising natural\nexperiments, ensuring effect \\(\\) \\(Y\\) can disentangled \n\\(W\\) \\(Y\\), even though \\(W\\) affects .Definition 4.4  (Positivity/Overlap) observed units, across strata defined \\(W\\), must bounded\nprobability receiving treatment – , \\(\\epsilon < \\P(= \\mid W) < 1 - \\epsilon\\) \\(\\) \\(W\\) \\(\\epsilon > 0\\))  .Technically speaking, latter two assumptions necessary\nworking within SCM framework, first two implied properties\nSCM ..d. data (’re really curious, see commentary \nPearl (2010) extended discussion). introduce four\nidentification assumptions often considered together, \nfour necessary working within potential outcomes framework\n(Imbens Rubin, 2015; Rubin, 2005).assumptions, ATE may re-written function \\(P_0\\), \ndistribution observed data:\\[\\begin{align}\n  \\psi_{\\text{ATE}} &= \\E_0[Y(1) - Y(0)] \\\\ \\nonumber\n                    &= \\E_0 [\\E_0[Y \\mid = 1, W] -\n                       \\E_0[Y \\mid = 0, W]] \\ .\n  \\tag{4.2}\n\\end{align}\\]\nwords, ATE mean difference predicted outcome values \nsubject, contrast treatment conditions (\\(= 0\\) versus \\(= 1\\)), population (averaged observations). Thus, parameter\ntheoretical complete (“full”) data distribution can represented \nestimand observed data distribution. Significantly, nothing\nrepresentation Equation (4.2) requires\nparameteric assumptions; thus, regression functions right hand side\nmay estimated without restrictive assumptions underlying\nfunctional forms. different parameters, potentially\ndifferent identifiability assumptions resulting estimands can \nfunctions different components \\(P_0\\). discuss several complex\nestimands subsequent chapters.","code":""},{"path":"origami.html","id":"origami","chapter":"5 Cross-validation","heading":"5 Cross-validation","text":"Ivana MalenicaBased origami R package\nJeremy Coyle, Nima Hejazi, Ivana Malenica Rachael Phillips.","code":""},{"path":"origami.html","id":"learning-objectives","chapter":"5 Cross-validation","heading":"Learning Objectives","text":"end chapter able :Differentiate training, validation test sets.Differentiate training, validation test sets.Understand concept loss function, risk cross-validation.Understand concept loss function, risk cross-validation.Select loss function appropriate functional parameter \nestimated.Select loss function appropriate functional parameter \nestimated.Understand contrast different cross-validation schemes ..d. data.Understand contrast different cross-validation schemes ..d. data.Understand contrast different cross-validation schemes time dependent\ndata.Understand contrast different cross-validation schemes time dependent\ndata.Setup proper fold structure, build custom fold-based function, \ncross-validate proposed function using origami R package.Setup proper fold structure, build custom fold-based function, \ncross-validate proposed function using origami R package.Setup proper cross-validation structure use Super Learner\nusing origami R package.Setup proper cross-validation structure use Super Learner\nusing origami R package.","code":""},{"path":"origami.html","id":"introduction-2","chapter":"5 Cross-validation","heading":"5.1 Introduction","text":"Following Roadmap Targeted Learning, start elaborate\nestimation step current chapter. order generate estimate\ntarget parameter, need decide evaluate quality \nestimation procedure’s performance. performance, error, algorithm\n(estimator) corresponds generalizability independent datasets arising\ndata-generating process. Assessment performance \nalgorithm extremely important — provides quantitative measure \nwell algorithm performs, guides choice selecting among set\n(“library”) algorithms. order assess performance \nalgorithm, library , introduce concept loss\nfunction, defines risk expected prediction error.\ngoal estimate true performance (risk) estimator. \nnext chapter, elaborate estimate performance library \nalgorithms order choose best-performing one. following, \npropose method using observed data cross-validation\nprocedures implemented origami package (Coyle et al., n.d.; Coyle Hejazi, 2018).","code":""},{"path":"origami.html","id":"background","chapter":"5 Cross-validation","heading":"5.2 Background","text":"Ideally, data-rich scenario (.e., one unlimited observations), \nsplit dataset three parts:training set,validation set, andthe test (holdout) set.training set used fit algorithm(s) interest; evaluate \nperformance fit(s) validation set, can used estimate\nprediction error (e.g., algorithm tuning selection). final error \nselected algorithm obtained using test (holdout) set, \nkept entirely separate algorithms never encounter \nobservations final model evaluation step. One might wonder, \ntraining data readily available, use training error evaluate \nproposed algorithm’s performance? Unfortunately, training error biased\nestimate fitted algorithm’s generalizability, since uses data\nfitting evaluation.Since data often scarce, separating dataset training, validation \ntest sets can prove limiting, account decreasing available data\nuse training much. absence large dataset \ndesignated test set, must resort methods estimate algorithm’s\ntrue performance efficient sample re-use. Re-sampling methods, like \nbootstrap, involve repeatedly sampling training set fitting \nalgorithms samples. often computationally intensive, re-sampling\nmethods particularly useful evaluating algorithm selecting among\nset . addition, provide insight variability \nrobustness fitted algorithm, relative fitting algorithm \ntraining data.","code":""},{"path":"origami.html","id":"introducing-cross-validation","chapter":"5 Cross-validation","heading":"5.2.1 Introducing: cross-validation","text":"chapter, focus cross-validation — essential tool \nevaluating algorithm extends sample data target\npopulation arose. Cross-validation seen widespread\napplication facets modern statistics, perhaps notably \nstatistical machine learning. cross-validation procedure proven \noptimal algorithm selection large samples, .e. asymptotically.\nparticular,cross-validated algorithm estimates true risk \nestimate applied independent sample joint distribution \npredictors outcome.used model selection, cross-validation powerful optimality\nproperties. asymptotic optimality results state cross-validated\nselector performs (terms risk) asymptotically well optimal oracle\nselector — hypothetical procedure free access true, unknown\ndata-generating distribution. details theoretical results, \nsuggest consulting van der Laan Dudoit (2003), van der Laan et al. (2004), Dudoit van der Laan (2005)\nvan der Vaart et al. (2006).origami package provides suite tools cross-validation. \nfollowing, describe different types cross-validation schemes readily\navailable origami, introduce general structure origami\npackage, demonstrate use procedures various applied\nsettings.","code":""},{"path":"origami.html","id":"estimation-roadmap-how-does-it-all-fit-together","chapter":"5 Cross-validation","heading":"5.3 Estimation Roadmap: How does it all fit together?","text":"Similarly defined Roadmap Targeted Learning, \ncan define Estimation Roadmap guide estimation process. \nparticular, unified loss-based estimation framework (Dudoit van der Laan, 2005; van der Laan et al., 2004, 2007; van der Laan Dudoit, 2003; van der Vaart et al., 2006),\nrelies cross-validation estimator construction,\nselection, performance assessment, consists three main steps:Loss function:\nDefine target parameter minimizer expected loss (risk) \nfull data loss function chosen represent desired performance measure. \nfull data, refer complete data including missingness process, \nexample. Map full data loss function observed data loss function,\nexpected value leading estimator risk.Loss function:\nDefine target parameter minimizer expected loss (risk) \nfull data loss function chosen represent desired performance measure. \nfull data, refer complete data including missingness process, \nexample. Map full data loss function observed data loss function,\nexpected value leading estimator risk.Algorithms:\nConstruct finite collection candidate estimators parameter \ninterest.Algorithms:\nConstruct finite collection candidate estimators parameter \ninterest.Cross-validation scheme:\nApply appropriate cross-validation, use cross-validated risk \norder select best performing estimator among candidates. Assess\noverall performance resulting estimator.Cross-validation scheme:\nApply appropriate cross-validation, use cross-validated risk \norder select best performing estimator among candidates. Assess\noverall performance resulting estimator.","code":""},{"path":"origami.html","id":"example-cross-validation-and-prediction","chapter":"5 Cross-validation","heading":"5.4 Example: Cross-validation and Prediction","text":"introduced Estimation Roadmap, can precisely\ndefine objective using prediction example. Let observed data \ndefined \\(O = (W, Y)\\), unit specific data structure can written \n\\(O_i = (W_i, Y_i)\\), \\(= 1, \\ldots, n\\). denote \\(Y_i\\) \noutcome/dependent variable interest, \\(W_i\\) \\(p\\)-dimensional set \ncovariate (predictor) variables. assume \\(n\\) units independent, \nconditionally independent, identically distributed. Let \\(\\psi_0(W)\\) denote\ntarget parameter interest, quantity wish estimate (estimand).\nexample, interested estimating conditional expectation \noutcome given covariates, \\(\\psi_0(W) = \\E(Y \\mid W)\\). Following \nEstimation Roadmap, choose appropriate loss function, \\(L\\),\n\\(\\psi_0(W) = \\text{argmin}_{\\psi} \\E_0[L(O, \\psi(W))]\\). Note \n\\(\\psi_0(W)\\), true target parameter, minimizer risk (expected\nvalue chosen loss function). appropriate loss function \nconditional expectation continuous outcome mean squared error,\nexample. can define \\(L\\) \\(L(O, \\psi(W)) = (Y_i -\\psi(W_i)^2\\). Note\ncan many different algorithms estimate estimand (many\ndifferent \\(\\psi\\)s). know well candidate estimators \n\\(\\psi_0(W)\\) ? pick best-performing candidate estimator \nassess overall performance, use cross-validation. Observations \ntraining set used fit (train) estimator, validation\nset used assess risk (validate) .Next, introduce notation flexible enough represent cross-validation\nscheme. particular, define split vector, \\(B_n = (B_n(): = 1, \\ldots, n) \\\\{0,1\\}^n\\).\n\nrealization \\(B_n\\) defines random split data training \nvalidation subsets \n\\[B_n() = 0, \\ \\ \\text{sample training set}\\]\n\\[B_n() = 1, \\ \\ \\text{sample validation set.}\\]\ncan define \\(P_{n, B_n}^0\\) \\(P_{n, B_n}^1\\) empirical\ndistributions training validation sets, respectively. , \\(n_0 = \\sum_i (1 - B_n())\\) \\(n_1 = \\sum_i B_n()\\) denote number samples \ntraining validation sets, respectively. particular distribution\nsplit vector \\(B_n\\) defines type cross-validation scheme, tailored\nproblem dataset hand.","code":""},{"path":"origami.html","id":"cross-validation-schemes-in-origami","chapter":"5 Cross-validation","heading":"5.5 Cross-validation schemes in origami","text":"variety different partitioning schemes exist, tailored salient\ndetails problem interest, including data size, prevalence \noutcome, dependence structure (units across time). \nfollowing, describe different cross-validation schemes available \norigami package, go demonstrate use practical data\nanalysis examples.","code":""},{"path":"origami.html","id":"wash-benefits-study-example","chapter":"5 Cross-validation","heading":"WASH Benefits Study Example","text":"order illustrate different cross-validation schemes, using \nWASH Benefits example dataset (detailed information can found \nChapter 3). particular, interested predicting\nweight--height Z-score (whz) using available covariate data. \nillustration, start treating data independent identically\ndistributed (..d.) random draws unknown distribution \\(P_0\\). \nsee cross-validation scheme , subset data \n\\(n=30\\). Note row represents ..d. sampled unit, indexed row\nnumber.look first 30 data.","code":"\nlibrary(data.table)\nlibrary(origami)\nlibrary(knitr)\nlibrary(dplyr)\n\n# load data set and take a peek\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)"},{"path":"origami.html","id":"cross-validation-for-i.i.d.-data","chapter":"5 Cross-validation","heading":"5.5.1 Cross-validation for i.i.d. data","text":"","code":""},{"path":"origami.html","id":"re-substitution","chapter":"5 Cross-validation","heading":"5.5.1.1 Re-substitution","text":"re-substitution method perhaps simplest strategy estimating \nrisk fitted algorithm. \ncross-validation scheme, observed data units used \ntraining validation set.illustrate usage re-substitution method origami ,\nusing function folds_resubstitution. order set \nfolds_resubstitution, need specify total number sampled\nunits want allocate training validation sets; remember\nrow dataset unique ..d. sampled unit. Also, notice \nstructure origami output:v: cross-validation foldtraining_set: indices samples training setvalidation_set: indices samples training set.structure origami output, list fold(s), holds across \ncross-validation schemes presented chapter. , show fold\ngenerated re-substitution method:","code":"folds <- folds_resubstitution(nrow(washb_data))\nfolds\n[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\n$validation_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"holdout","chapter":"5 Cross-validation","heading":"5.5.1.2 Holdout","text":"holdout method, validation set approach, consists randomly\ndividing available data training validation (holdout) sets. \nmodel fitted (.e., “trained”) using observations training\nset subsequently evaluated (.e., “validated”) using observations \nvalidation set. Typically, dataset split \\(60:40\\), \\(70:30\\), \\(80:20\\)\neven \\(90:10\\) training--validation splits.holdout method intuitive computationally inexpensive; however, \ncarry disadvantage: repeat process randomly\nsplitting data training validation sets, get \ndifferent cross-validated estimates empirical risk. particular, \nempirical mean loss function (.e., empirical risk) evaluated \nvalidation set(s) highly variable, depending samples \nincluded training validation splits. Overall, cross-validated\nempirical risk holdout method variable, since includes\nvariability random split well — desirable. \nclassification problems (binary categorical outcome variable), \nadditional disadvantage: possible training validation\nsets end uneven distributions two () outcome classes,\nleading better training poor validation, vice-versa — though may\ncorrected incorporating stratification cross-validation process.\nFinally, note using data training evaluating\nperformance proposed algorithm, introduce bias.","code":""},{"path":"origami.html","id":"leave-one-out","chapter":"5 Cross-validation","heading":"5.5.1.3 Leave-one-out","text":"leave-one-cross-validation scheme closely related holdout\nmethod, also involves splitting dataset training validation\nsets; however, instead partitioning dataset sets similar size, \nsingle observation used validation set. , vast majority\nsampled units employed fitting (training) candidate\nlearning algorithm. Since single sampled unit (example \\(O_1 = (W_1, Y_1)\\)) left fitting process, leave-one-cross-validation can\nresult less biased estimate risk. Typically, \nleave-one-approach overestimate risk much holdout\nmethod . hand, since estimate risk based single\nsampled unit, usually highly variable estimate.can repeat process spiting dataset training validation\nsets sampled units chance act validation\nset. Continuing example , subsequent iteration leave-one-\ncross-validation scheme may use \\(O_2 = (W_2, Y_2)\\) validation set (,\n, \\(O_1 = (W_1, Y_1)\\) played role), remaining \\(n-1\\) sampled\nunits included training set. Repeating approach \\(n\\) times\nresults \\(n\\) risk estimates, example, \\(MSE_1, MSE_2, \\ldots, MSE_n\\) (note\nmean squared error (MSE) estimates unit \\(\\) \nvalidation set). estimate true risk average \\(n\\)\nleave-one-risk estimates. leave-one-cross-validation scheme\nresults less biased (albeit, variable) estimate risk \nholdout method, can computationally expensive implement \\(n\\)\nlarge.illustrate usage leave-one-cross-validation scheme \norigami , using folds_loo(n) function. order set \nfolds_loo(n), similarly case re-substitution method, need\ntotal number sampled units cross-validation procedure\noperate. show first two folds generated leave-one-\ncross-validation .","code":"folds <- folds_loo(nrow(washb_data))\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n[26] 27 28 29 30\n\n$validation_set\n[1] 1\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n[26] 27 28 29 30\n\n$validation_set\n[1] 2\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"v-fold","chapter":"5 Cross-validation","heading":"5.5.1.4 \\(V\\)-fold","text":"alternative leave-one-scheme \\(V\\)-fold cross-validation. \ncross-validation scheme randomly divides dataset \\(V\\) splits \nequal (approximately equal) size. split \\(v=1,\\ldots,V\\), \n\\(v\\)th fold defined \\(v\\)th split (defines validation set \nfold \\(v\\)) complement \\(v\\)th split (defines training set\nfold \\(v\\)). algorithms fit \\(V\\) times separately, \n\\(V\\) training sets. risk fold’s fitted algorithms evaluated\nvia predictions obtained validation set. cross-validated risk \nfitted algorithm, example MSE, average risk across folds.\n\\(V\\)-fold cross-validation, observations\nused training validation stages, preventing candidate\nlearning algorithm overfitting subset data (e.g., given\ntraining set).dataset \\(n\\) sampled units, \\(V\\)-fold cross-validation \\(v=n\\)\nmerely reduces leave-one-. Similarly, set \\(n=1\\), can get \nholdout method’s estimate candidate learning algorithm’s performance.\nBeyond computational advantages, \\(V\\)-fold cross-validation often yields \naccurate estimates true, underlying risk. rooted differing\nbias-variance trade-offs associated two cross-validation schemes:\nleave-one-scheme may less biased, much greater variance\n(since single unit included validation set). difference\nbecomes obvious \\(n\\) becomes much greater \\(v\\). \\(V\\)-fold\ncross-validation scheme, end averaging risk estimates across \\(v\\)\nvalidation folds, typically less correlated risk estimates\nleave-one-fits. Owing fact mean many highly\ncorrelated quantities higher variance, leave-one-estimates risk\nhigher variance corresponding estimates based \\(V\\)-fold\ncross-validation.Now, let’s see \\(V\\)-fold cross-validation origami action! next\nchapter, turn studying Super Learner algorithm — algorithm\ncapable selecting “best” algorithm among large library candidate\nlearning algorithms – ’d like fit evaluate performance .\nSuper Learner algorithm relies \\(V\\)-fold cross-validation default\ncross-validation scheme. order set \\(V\\)-fold cross-validation, need\ncall origami’s folds_vfold(n, V) function. two required arguments \nfolds_vfold(n, V) total number sample units cross-validated\nnumber folds wish .example, \\(V=2\\), get two folds, approximately \\(n/2\\)\nsampled units training validation sets.","code":"folds <- folds_vfold(nrow(washb_data), V = 2)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  2  3  4  6  7  8 11 12 14 15 19 22 23 24 28\n\n$validation_set\n [1]  1  5  9 10 13 16 17 18 20 21 25 26 27 29 30\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  5  9 10 13 16 17 18 20 21 25 26 27 29 30\n\n$validation_set\n [1]  2  3  4  6  7  8 11 12 14 15 19 22 23 24 28\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"monte-carlo","chapter":"5 Cross-validation","heading":"5.5.1.5 Monte Carlo","text":"Monte Carlo cross-validation scheme, randomly select fraction \ndata, without replacement, form training set, assigning \nremainder sampled units validation set. way, dataset\nrandomly divided two independent splits: training set \\(n_0 = n \\cdot (1 - p)\\) observations validation set \\(n_1 = n \\cdot p\\)\nobservations. repeating procedure many times, Monte Carlo\ncross-validation scheme generates, random, many training validation\npartitions dataset.Since partitions independent across folds, observational unit\ncan appear validation set multiple times; note stark\ndifference Monte Carlo \\(V\\)-fold cross-validation schemes. \ngiven sampling fraction \\(p\\), Monte Carlo cross-validation scheme \noptimal repeated infinitely many times — course, \ncomputationally feasible. Monte Carlo cross-validation, possible \nexplore many partitions dataset \\(V\\)-fold cross-validation,\nresulting (possibly) less variable estimates risk (across partitions),\nthough comes cost increase bias (splits \ncorrelated). Monte Carlo cross-validation generates many splits \noverlaps sampled units, splits (thus computational time)\nnecessary achieve level performance (terms unbiasedness)\n\\(V\\)-fold cross-validation scheme achieves \\(V\\) splits.illustrate usage Monte Carlo cross-validation scheme \norigami , using folds_montecarlo(n, V, pvalidation) function. \norder set folds_montecarlo(n, V, pvalidation), need following,total number observations wish cross-validate;number folds; andthe proportion observations placed validation set.example, setting \\(V=2\\) \\(pvalidation = 0.2\\), obtain two folds, \napproximately \\(6\\) sampled units validation set fold.","code":"folds <- folds_montecarlo(nrow(washb_data), V = 2, pvalidation = 0.2)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1] 19 27 16 29 23 12  1  3 18 11  5  7  8  6  9 22 10 25 20 28 15  2 24 26\n\n$validation_set\n[1]  4 13 14 17 21 30\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1] 19 15 28 25 29 11 20 17 14  4  9 12 30  8 27 18 16 10 13  6 24  3 26  1\n\n$validation_set\n[1]  2  5  7 21 22 23\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"bootstrap","chapter":"5 Cross-validation","heading":"5.5.1.6 Bootstrap","text":"Like Monte Carlo cross-validation scheme, bootstrap cross-validation\nscheme also consists randomly selecting sampled units, replacement,\ntraining set; rest sampled units allocated \nvalidation set. process repeated multiple times, generating (\nrandom) new training validation partitions dataset time. \ncontrast Monte Carlo cross-validation scheme, total number sampled\nunits training validation sets (.e., sizes two partitions)\nacross folds held constant. Also, name suggests, sampling \nperformed replacement (bootstrap (Davison Hinkley, 1997)), hence\nexact observational units may included multiple training sets.\nproportion observational units validation sets random\nvariable, expectation \\(\\sim 0.368\\).\nillustrate usage bootstrap cross-validation scheme origami\n, using folds_bootstrap(n, V) function. order set \nfolds_bootstrap(n, V), need specify following arguments:total number observations wish cross-validate; andthe number folds.example, setting \\(V=2\\), obtain two folds, different numbers \nsampled units validation sets across folds.","code":"folds <- folds_bootstrap(nrow(washb_data), V = 2)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  2  5 30  1 29 16 10 11  8 25 28  2 11  2 16 28 15 28  1 27  9 19 20 30 18\n[26] 11 13  2 18 12\n\n$validation_set\n [1]  3  4  6  7 14 17 21 22 23 24 26\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1] 12 16 10 29 22 15 27  9 27 16 12 28 10 28 26  1 14  6 23 14 21 16  5 20  8\n[26] 23 25  8 27  5\n\n$validation_set\n [1]  2  3  4  7 11 13 17 18 19 24 30\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"cross-validation-for-time-series-data","chapter":"5 Cross-validation","heading":"5.5.2 Cross-validation for Time-series Data","text":"origami package also supports numerous cross-validation schemes \ntime-series data, single multiple time-series\narbitrary time network dependence.","code":""},{"path":"origami.html","id":"airpassenger-data-example","chapter":"5 Cross-validation","heading":"AirPassenger Data Example","text":"order illustrate different cross-validation schemes time-series, \nusing AirPassenger data; widely used, freely available\ndataset. AirPassenger dataset, included R, provides monthly totals \ninternational airline passengers years 1949 1960.Goal: want forecast number airline passengers time \\(h\\)\nhorizon using historical data 1949 1960.","code":"\nlibrary(ggfortify)\n\ndata(AirPassengers)\nAP <- AirPassengers\n\nautoplot(AP) +\n  labs(\n    x = \"Date\",\n    y = \"Passenger numbers (1000's)\",\n    title = \"Air Passengers from 1949 to 1961\"\n  )\n\nt <- length(AP)"},{"path":"origami.html","id":"rolling-origin","chapter":"5 Cross-validation","heading":"5.5.2.1 Rolling origin","text":"rolling origin cross-validation scheme lends “online” learning\nalgorithms, large streams data fit continually\n(respecting time), fit learning algorithm (constantly)\nupdated data accrues. general, rolling origin scheme defines \ninitial training set, , iteration, size training set\ngrows batch \\(m\\) observations, size validation set remains\nconstant, might gap training validation times size\n\\(h\\) (lag window), new folds added time \\(t\\) reached \nvalidation set. time points included training set always lag behind\nbehind validation set.illustrate rolling origin cross-validation, show example\nyields three folds. , first window size fifteen time points, \nfirst train candidate learning algorithm. evaluate \nperformance ten time points gap (\\(h\\)) five time points \ntraining validation sets.following, train learning algorithm longer stream data, 25\ntime points, including original fifteen initially started.\n, evaluate performance (temporal) distance ten time points\nahead.\nFIGURE 5.1: Rolling origin CV\nillustrate usage rolling origin cross-validation scheme \norigami , using folds_rolling_origin(n, first_window, validation_size, gap, batch) function. order set \nfolds_rolling_origin(n, first_window, validation_size, gap, batch), need\nfollowing,total number time points wish cross-validate (n);size first training set (first_window);size validation set (validation_size);gap training validation set(gap); andthe size training set update per iteration cross-validation (batch).time-series \\(t=144\\) time points. Setting first_window \\(50\\),\nvalidation_size 10, gap 5, batch 20 yields four time-series\nfolds; show first two .","code":"folds <- folds_rolling_origin(\n  n = t,\n  first_window = 50, validation_size = 10, gap = 5, batch = 20\n)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n$validation_set\n [1] 56 57 58 59 60 61 62 63 64 65\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\n$validation_set\n [1] 76 77 78 79 80 81 82 83 84 85\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"rolling-window","chapter":"5 Cross-validation","heading":"5.5.2.2 Rolling window","text":"Rather adding time points training set \niteration cross-validation (rolling origin scheme), rolling\nwindow cross-validation scheme “rolls” training sample forward time \n\\(m\\) units (time). strategy can useful, example, settings \nparametric learning algorithms, often sensitive moment (e.g.,\nmean, variance) parameter drift, challenging explicitly\naccount model construction step. rolling window scheme also\ncomputationally efficient, possibly warranted rolling origin\nworking streaming data analysis training data large\nconvenient access. contrast \nrolling origin scheme, sampled units training set always \niteration rolling window scheme.illustration depicts rolling window cross-validation using three\ntime-series folds. first window size 15 time points, first\ntrain candidate learning algorithm. previous illustration, \nevaluate performance 10 time points, gap size 5 time points\ntraining validation sets. However, next fold, train\nlearning algorithm time points away origin (, 10\ntime points). Note size training set new fold \nfirst fold (include 15 time points). setup keeps \ntraining sets comparable time (across folds), unlike rolling\norigin cross-validation scheme. evaluate performance \ncandidate learning algorithm 10 time points future.\nFIGURE 5.2: Rolling window CV\ndemonstrate usage rolling window cross-validation scheme \norigami , using folds_rolling_window(n, window_size, validation_size, gap, batch) function. order set \nfolds_rolling_window(n, window_size, validation_size, gap, batch), need \nspecify following arguments:total number time points wish cross-validate (n);size training sets (window_size);size validation set (validation_size);gap training validation set (gap); andthe size training set update per iteration cross-validation (batch).Setting window_size \\(50\\), validation_size 10, gap 5 \nbatch 20, also get 4 time-series folds; show first two .","code":"folds <- folds_rolling_window(\n  n = t,\n  window_size = 50, validation_size = 10, gap = 5, batch = 20\n)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n$validation_set\n [1] 56 57 58 59 60 61 62 63 64 65\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\n$validation_set\n [1] 76 77 78 79 80 81 82 83 84 85\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"rolling-origin-with-v-fold","chapter":"5 Cross-validation","heading":"5.5.2.3 Rolling origin with \\(V\\)-fold","text":"variant rolling origin cross-validation scheme, accounting sample\ndependence, rolling-origin-\\(V\\)-fold cross-validation scheme. contrast\ncanonical rolling origin scheme, hybrid scheme, sampled units\ntraining validation sets , scheme\naccomplishes incorporating \\(V\\)-fold cross-validation within time-series\nsetup. , learning algorithm’s predictions evaluated future\ntime points time-series observational units excluded training\nstep, accommodating potential dependence across time also across\nobservational units. use rolling-origin-\\(V\\)-fold cross-validation scheme\norigami, can invoke folds_vfold_rolling_origin_pooled(n, t, id, time, V, first_window, validation_size, gap, batch) function. figure\n, show \\(V=2\\) folds, alongside two time-series (rolling origin)\ncross-validation folds.\nFIGURE 5.3: Rolling origin V-fold CV\n","code":""},{"path":"origami.html","id":"rolling-window-with-v-fold","chapter":"5 Cross-validation","heading":"5.5.2.4 Rolling window with \\(V\\)-fold","text":"Just like scheme described , rolling window approach, like \nrolling origin approach, can extended support multiple time-series \narbitrary sample-level dependence incorporating \\(V\\)-fold splitting\ncomponent. rolling-window-\\(V\\)-fold cross-validation scheme can used\norigami via folds_vfold_rolling_window_pooled(n, t, id, time, V, window_size, validation_size, gap, batch) function. figure displays\n\\(V=2\\) folds two time-series (rolling window) cross-validation folds.\nFIGURE 5.4: Rolling window V-fold CV\n","code":""},{"path":"origami.html","id":"general-workflow-of-origami","chapter":"5 Cross-validation","heading":"5.6 General workflow of origami","text":"dive details, let’s take moment review \nbasic functionality origami R package. main workhorse function \norigami cross_validate(). start , user must define fold\nstructure function operates fold (cv_fun(), \norigami’s parlance, usually dictates candidate learning algorithm \ntrained predictions validated).passed cross_validate(), workhorse function iteratively apply\nspecified function (.e., cv_fun()) fold, combining \nfold-specific results meaningful way. see action later\nsections — now, provide specific details step \nprocess .","code":""},{"path":"origami.html","id":"define-folds","chapter":"5 Cross-validation","heading":"5.6.1 (1) Define folds","text":"folds object passed cross_validate list folds; list\nobjects generated using make_folds() helper function. fold\nconsists list \"training\" index vector, \"validation\" index\nvector, \"fold_index\" (order overall list folds). \nmake_folds() function supports variety cross-validation schemes,\ndescribed preceding section. make_folds() function can also ensure\nbalance across levels given variable (strata_ids arguments),\ncan also keep observations independent unit together (via\ncluster_ids argument).","code":""},{"path":"origami.html","id":"define-the-fold-function","chapter":"5 Cross-validation","heading":"5.6.2 (2) Define the fold function","text":"cv_fun argument cross_validate() custom function performs\noperation fold (, usually specifies training \ncandidate learning algorithm evaluation given training/validation\nsplit, .e., single fold). first argument function \nfold, specifies indices units given training/validation\nsplit (note first argument automatically passed cv_fun()\ncross_validate(), queries folds object make_folds() \n). Additional arguments can passed cv_fun() \n... argument cross_validate(). Within function, convenience\nfunctions training(), validation() fold_index() can used return\nvarious components fold object. training() \nvalidation() functions passed object particular class, \nindex object sensible way. instance, input object \nvector, helper functions index vector directly, input\nobject data.frame matrix, functions automatically index\nrows. allows user easily partition data training \nvalidation sets. fold function must return named list results\ncontaining whatever fold-specific outputs desired.","code":""},{"path":"origami.html","id":"apply-cross_validate","chapter":"5 Cross-validation","heading":"5.6.3 (3) Apply cross_validate()","text":"defining folds, cross_validate() function can used map \ncv_fun() across folds; internally, uses either lapply() \nfuture_lapply() (parallelized variant ). way,\ncross_validate() can easily parallelized specifying parallelization\nscheme (.e., plan future parallelization framework \nR (Bengtsson, 2021)). \napplication cross_validate() generates list results, matching \ncustomized list specified relevant cv_fun(). noted , \ncall cv_fun() returns list results, different named\nslots type result wish store. main cross_validate()\nloop generates list individual, fold-specific lists results (\nlist lists “meta-list”). Internally, “meta-list” cleaned \n(concatenation) single slot per type result specified \ncv_fun() returned (list results fold).\ndefault, combine_results() helper function used combine \nindividual, fold-specific lists results sensible manner. results\ncombined determined automatically examining data types \nresults first fold. can modified specifying list \narguments .combine_control argument.","code":""},{"path":"origami.html","id":"cross-validation-in-action","chapter":"5 Cross-validation","heading":"5.7 Cross-validation in action","text":"’ve waited long enough. Now, let’s see origami action! next\nchapter, learn use cross-validation Super Learner\nalgorithm, can utilize power cross-validation build optimal\nensembles algorithms — going far beyond application cross-validation\nsingle statistical learning method.","code":""},{"path":"origami.html","id":"cross-validation-with-linear-regression","chapter":"5 Cross-validation","heading":"5.7.1 Cross-validation with linear regression","text":"First, let’s load relevant R packages, set seed (reproducibility),\nload WASH Benefits example dataset. illustrative\npurposes, ’ll examine application cross-validation simple linear\nregression origami, focusing predicting weight--height Z-score\n(whz) using available covariates dataset. mentioned\n, assume dataset contains independent identically\ndistributed units, ignoring clustering structure imposed trial\ndesign. sake illustration, work subset \ndata, removing observational units missing covariate data \nanalysis-ready dataset. prior chapter, discussed deal \nmissingness.’s look data:Let’s remind covariates used prediction step:Next, let’s fit simple main-terms linear regression model \nanalysis-ready dataset. , goal predict weight--height\nZ-score (\"whz\", assigned variable outcome) using \navailable covariate data. Let’s try :can assess quality model fit dataset comparing \nlinear model’s predictions weight--height Z-score \nobservations dataset. well-known, standard,\nmean squared error (MSE). can extract summary measure lm\nmodel object like soThe MSE estimate 0.8657, , examination , merely \nmean squared residuals model fit. important problem arises\nassess learning algorithm’s quality way — , \ntrained linear regression model complete analysis-ready\ndataset assessed performance (MSE) dataset, \ndata used model training validation. Unfortunately, \nsimple estimate MSE overly optimistic. ? linear regression\nmodel trained dataset used evaluation, unlike reusing\nproblems homework assignment course examination. course, \ngenerally interested well algorithm explains variation \nobserved dataset; rather, interested well explanations\nprovided learning algorithm generalize target population \nparticular sample drawn. using data available us \ntraining learning algorithm, left unable honestly evaluate \nwell algorithm fits (, thus, explains) variation level \ntarget population.\n\nresolve issue, cross-validation allows particular procedure (e.g.,\nlinear regression) implemented training validation splits \ndataset, evaluating well procedure fits holdout (validation)\nset. evaluation learning algorithm’s quality data unseen \ntraining phase provides honest evaluation algorithm’s\ngeneralization error.can easily incorporate cross-validation linear regression procedure\nusing origami. First, let’s define new function perform linear regression\nspecific partition dataset (.e., fold):cv_lm() function quite simple: merely splits available data \ndistinct training validation sets (using eponymous functions provided \norigami), fits linear model training set, evaluates quality\ntrained linear regression model validation set. simple\nexample origami considers cv_fun() — functions applying\nparticular routine input dataset cross-validated manner.defined function, can simply generate set partitions using\norigami’s make_folds() function apply cv_lm() function \nresultant folds object using cross_validate(). , replicate \nre-substitution estimate error — “hand” — using \nfunctions make_folds() cv_lm().(nearly) matches estimate error obtained .can honestly evaluate error \\(V\\)-fold cross-validation, \npartitions dataset \\(V\\) subsets, fitting algorithm \\(V - 1\\) \nsubsets (training) evaluating subset held \nfitting (validation). repeated holdout subset\ntakes turn used validation. can easily apply cv_lm()\nfunction way using origami’s cross_validate() (note default\nfunction performs \\(10\\)-fold cross-validation):performed \\(V\\)-fold cross-validation 10 folds (default), \nquickly notice previous\nestimate model error (re-substitution) bit optimistic. honest\nestimate linear regression model’s error larger!","code":"\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# load data set and take a peek\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\n\n# remove missing data with drop_na(), then pick just the first 500 rows\nwashb_data <- washb_data %>%\n  drop_na() %>%\n  slice(1:500)\n\n# specify the outcome and covariates as character vectors\noutcome <- \"whz\"\ncovars <- colnames(washb_data)[-which(names(washb_data) == outcome)]covars\n [1] \"tr\"             \"fracode\"        \"month\"          \"aged\"          \n [5] \"sex\"            \"momage\"         \"momedu\"         \"momheight\"     \n [9] \"hfiacat\"        \"Nlt18\"          \"Ncomp\"          \"watmin\"        \n[13] \"elec\"           \"floor\"          \"walls\"          \"roof\"          \n[17] \"asset_wardrobe\" \"asset_table\"    \"asset_chair\"    \"asset_khat\"    \n[21] \"asset_chouki\"   \"asset_tv\"       \"asset_refrig\"   \"asset_bike\"    \n[25] \"asset_moto\"     \"asset_sewmach\"  \"asset_mobile\"  lm_mod <- lm(whz ~ ., data = washb_data)\nsummary(lm_mod)\n\nCall:\nlm(formula = whz ~ ., data = washb_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8890 -0.6799 -0.0169  0.6595  3.1005 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                     -1.89006    1.72022   -1.10   0.2725   \ntrHandwashing                   -0.25276    0.17032   -1.48   0.1385   \ntrNutrition                     -0.09695    0.15696   -0.62   0.5371   \ntrNutrition + WSH               -0.09587    0.16528   -0.58   0.5622   \ntrSanitation                    -0.27702    0.15846   -1.75   0.0811 . \ntrWSH                           -0.02846    0.15967   -0.18   0.8586   \ntrWater                         -0.07148    0.15813   -0.45   0.6515   \nfracodeN05160                    0.62355    0.30719    2.03   0.0430 * \nfracodeN05265                    0.38762    0.31011    1.25   0.2120   \nfracodeN05359                    0.10187    0.31329    0.33   0.7452   \nfracodeN06229                    0.30933    0.29766    1.04   0.2993   \nfracodeN06453                    0.08066    0.30006    0.27   0.7882   \nfracodeN06458                    0.43707    0.29970    1.46   0.1454   \nfracodeN06473                    0.45406    0.30912    1.47   0.1426   \nfracodeN06479                    0.60994    0.31463    1.94   0.0532 . \nfracodeN06489                    0.25923    0.31901    0.81   0.4169   \nfracodeN06500                    0.07539    0.35794    0.21   0.8333   \nfracodeN06502                    0.36748    0.30504    1.20   0.2290   \nfracodeN06505                    0.20038    0.31560    0.63   0.5258   \nfracodeN06516                    0.55455    0.29807    1.86   0.0635 . \nfracodeN06524                    0.49429    0.31423    1.57   0.1164   \nfracodeN06528                    0.75966    0.31060    2.45   0.0148 * \nfracodeN06531                    0.36856    0.30155    1.22   0.2223   \nfracodeN06862                    0.56932    0.29293    1.94   0.0526 . \nfracodeN08002                    0.36779    0.26846    1.37   0.1714   \nmonth                            0.17161    0.10865    1.58   0.1149   \naged                            -0.00336    0.00112   -3.00   0.0029 **\nsexmale                          0.12352    0.09203    1.34   0.1802   \nmomage                          -0.01379    0.00973   -1.42   0.1570   \nmomeduPrimary (1-5y)            -0.13214    0.15225   -0.87   0.3859   \nmomeduSecondary (>5y)            0.12632    0.16041    0.79   0.4314   \nmomheight                        0.00512    0.00919    0.56   0.5776   \nhfiacatMildly Food Insecure      0.05804    0.19341    0.30   0.7643   \nhfiacatModerately Food Insecure -0.01362    0.12887   -0.11   0.9159   \nhfiacatSeverely Food Insecure   -0.13447    0.25418   -0.53   0.5970   \nNlt18                           -0.02557    0.04060   -0.63   0.5291   \nNcomp                            0.00179    0.00762    0.23   0.8145   \nwatmin                           0.01347    0.00861    1.57   0.1182   \nelec                             0.08906    0.10700    0.83   0.4057   \nfloor                           -0.17763    0.17734   -1.00   0.3171   \nwalls                           -0.03001    0.21445   -0.14   0.8888   \nroof                            -0.03716    0.49214   -0.08   0.9399   \nasset_wardrobe                  -0.05754    0.13736   -0.42   0.6755   \nasset_table                     -0.22079    0.12276   -1.80   0.0728 . \nasset_chair                      0.28012    0.13750    2.04   0.0422 * \nasset_khat                       0.02306    0.11766    0.20   0.8447   \nasset_chouki                    -0.13943    0.14084   -0.99   0.3227   \nasset_tv                         0.17723    0.12972    1.37   0.1726   \nasset_refrig                     0.12613    0.23162    0.54   0.5863   \nasset_bike                      -0.02568    0.10083   -0.25   0.7990   \nasset_moto                      -0.32094    0.19944   -1.61   0.1083   \nasset_sewmach                    0.05090    0.17795    0.29   0.7750   \nasset_mobile                     0.01420    0.14972    0.09   0.9245   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.984 on 447 degrees of freedom\nMultiple R-squared:  0.129, Adjusted R-squared:  0.0277 \nF-statistic: 1.27 on 52 and 447 DF,  p-value: 0.104(err <- mean(resid(lm_mod)^2))\n[1] 0.8657\ncv_lm <- function(fold, data, reg_form) {\n  # get name and index of outcome variable from regression formula\n  out_var <- as.character(unlist(str_split(reg_form, \" \"))[1])\n  out_var_ind <- as.numeric(which(colnames(data) == out_var))\n\n  # split up data into training and validation sets\n  train_data <- training(data)\n  valid_data <- validation(data)\n\n  # fit linear model on training set and predict on validation set\n  mod <- lm(as.formula(reg_form), data = train_data)\n  preds <- predict(mod, newdata = valid_data)\n  valid_data <- as.data.frame(valid_data)\n\n  # capture results to be returned as output\n  out <- list(\n    coef = data.frame(t(coef(mod))),\n    SE = (preds - valid_data[, out_var_ind])^2\n  )\n  return(out)\n}# re-substitution estimate\nresub <- make_folds(washb_data, fold_fun = folds_resubstitution)[[1]]\nresub_results <- cv_lm(fold = resub, data = washb_data, reg_form = \"whz ~ .\")\nmean(resub_results$SE, na.rm = TRUE)\n[1] 0.8657# cross-validated estimate\nfolds <- make_folds(washb_data)\ncvlm_results <- cross_validate(\n  cv_fun = cv_lm, folds = folds, data = washb_data, reg_form = \"whz ~ .\",\n  use_future = FALSE\n)\nmean(cvlm_results$SE, na.rm = TRUE)\n[1] 1.35"},{"path":"origami.html","id":"cross-validation-with-random-forests","chapter":"5 Cross-validation","heading":"5.7.2 Cross-validation with random forests","text":"examine origami , let’s return example analysis using \nWASH Benefits dataset. , write new cv_fun() function. \nexample, use Breiman’s random forest algorithm (Breiman, 2001),\nimplemented randomForest() function (randomForest package):cv_rf() function, cross-validates training evaluation \nrandomForest algorithm, used previous cv_lm() function template.\nnow, individual cv_fun()s must written hand; however, future\nreleases package, wrapper may made available support\nauto-generating cv_funs use origami., use cross_validate() apply custom cv_rf() function \nfolds object generated make_folds():Using \\(V\\)-fold cross-validation 10 folds, obtain honest estimate \nprediction error random forest. one example \norigami’s cross_validate() procedure can generalized arbitrary\nestimation techniques, long appropriate cv_fun() function \navailable.","code":"\n# make sure to load the package!\nlibrary(randomForest)\n\ncv_rf <- function(fold, data, reg_form) {\n  # get name and index of outcome variable from regression formula\n  out_var <- as.character(unlist(str_split(reg_form, \" \"))[1])\n  out_var_ind <- as.numeric(which(colnames(data) == out_var))\n\n  # define training and validation sets based on input object of class \"folds\"\n  train_data <- training(data)\n  valid_data <- validation(data)\n\n  # fit Random Forest regression on training set and predict on holdout set\n  mod <- randomForest(formula = as.formula(reg_form), data = train_data)\n  preds <- predict(mod, newdata = valid_data)\n  valid_data <- as.data.frame(valid_data)\n\n  # define output object to be returned as list (for flexibility)\n  out <- list(\n    coef = data.frame(mod$coefs),\n    SE = ((preds - valid_data[, out_var_ind])^2)\n  )\n  return(out)\n}# now, let's cross-validate...\nfolds <- make_folds(washb_data)\ncvrf_results <- cross_validate(\n  cv_fun = cv_rf, folds = folds,\n  data = washb_data, reg_form = \"whz ~ .\",\n  use_future = FALSE\n)\nmean(cvrf_results$SE)\n[1] 1.027"},{"path":"origami.html","id":"cross-validation-with-arima","chapter":"5 Cross-validation","heading":"5.7.3 Cross-validation with ARIMA","text":"Cross-validation can also used selection forecasting models \nsettings time-series data. , partitioning scheme mirrors \napplication forecasting model: ’ll train learning algorithm past\nobservations (either available recent, time, subset), use\nfitted model predict next (, time) observations. \ndemonstrate , return AirPassengers dataset, monthly\ntime-series passenger air traffic thousands travelers.Suppose want pick two forecasting models different ARIMA\n(AutoRegressive Integrated Moving Average) model configurations. can choose\namong models evaluating forecasting performance. First, set \nappropriate cross-validation scheme use time-series data. , \npick rolling origin cross-validation scheme described .default, folds_rolling_origin increase size training set \none time point training fold. followed default option, \n85 folds train! Luckily, can pass batch option \nfolds_rolling_origin, telling increase size training set \n10 points iteration (don’t many training folds).\nSince want forecast immediately following time point, gap\nargument remains default zero.applying cross_validate() cv_forecasts() custom function, \nfind ARIMA model AR (autoregressive) component seems \nbetter fit dataset.","code":"data(AirPassengers)\nprint(AirPassengers)\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432folds <- make_folds(AirPassengers,\n  fold_fun = folds_rolling_origin,\n  first_window = 36, validation_size = 24, batch = 10\n)\n\n# How many folds where generated?\nlength(folds)\n[1] 9\n\n# Examine the first 2 folds.\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n$validation_set\n [1] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n\n$validation_set\n [1] 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\nattr(,\"class\")\n[1] \"fold\"# make sure to load the package!\nlibrary(forecast)\n\n# function to calculate cross-validated squared error\ncv_forecasts <- function(fold, data) {\n  # Get training and validation data\n  train_data <- training(data)\n  valid_data <- validation(data)\n  valid_size <- length(valid_data)\n\n  train_ts <- ts(log10(train_data), frequency = 12)\n\n  # First arima model\n  arima_fit <- arima(train_ts, c(0, 1, 1),\n    seasonal = list(\n      order = c(0, 1, 1),\n      period = 12\n    )\n  )\n  raw_arima_pred <- predict(arima_fit, n.ahead = valid_size)\n  arima_pred <- 10^raw_arima_pred$pred\n  arima_MSE <- mean((arima_pred - valid_data)^2)\n\n  # Second arima model\n  arima_fit2 <- arima(train_ts, c(5, 1, 1),\n    seasonal = list(\n      order = c(0, 1, 1),\n      period = 12\n    )\n  )\n  raw_arima_pred2 <- predict(arima_fit2, n.ahead = valid_size)\n  arima_pred2 <- 10^raw_arima_pred2$pred\n  arima_MSE2 <- mean((arima_pred2 - valid_data)^2)\n\n  out <- list(mse = data.frame(\n    fold = fold_index(),\n    arima = arima_MSE, arima2 = arima_MSE2\n  ))\n  return(out)\n}\n\nmses <- cross_validate(\n  cv_fun = cv_forecasts, folds = folds, data = AirPassengers,\n  use_future = FALSE\n)\nmses$mse\n  fold   arima arima2\n1    1   68.21  137.3\n2    2  319.68  313.2\n3    3  578.35  713.4\n4    4  428.69  505.3\n5    5  407.33  371.3\n6    6  281.82  251.0\n7    7  827.56  910.1\n8    8 2099.59 2213.1\n9    9  398.37  293.4\ncolMeans(mses$mse[, c(\"arima\", \"arima2\")])\n arima arima2 \n 601.1  634.2 "},{"path":"origami.html","id":"exercises","chapter":"5 Cross-validation","heading":"5.8 Exercises","text":"","code":""},{"path":"origami.html","id":"review-of-key-concepts","chapter":"5 Cross-validation","heading":"5.8.1 Review of Key Concepts","text":"Compare contrast \\(V\\)-fold cross-validation re-substitution\ncross-validation. differences two methods?\nsimilar? Describe scenario use one \n.Compare contrast \\(V\\)-fold cross-validation re-substitution\ncross-validation. differences two methods?\nsimilar? Describe scenario use one \n.advantages disadvantages \\(V\\)-fold cross-validation\nrelative :\nholdout cross-validation?\nleave-one-cross-validation?\nadvantages disadvantages \\(V\\)-fold cross-validation\nrelative :holdout cross-validation?leave-one-cross-validation?\\(V\\)-fold cross-validation inappropriate use time-series data?\\(V\\)-fold cross-validation inappropriate use time-series data?use rolling window rolling origin cross-validation \nnon-stationary time-series? ?\n\n### Ideas ActionWould use rolling window rolling origin cross-validation \nnon-stationary time-series? ?\n\n### Ideas ActionLet \\(Y\\) binary variable \\(P(Y=1 \\mid W) = 0.01\\), , rare\noutcome. kind cross-validation scheme used type\noutcome? can origami package?Let \\(Y\\) binary variable \\(P(Y=1 \\mid W) = 0.01\\), , rare\noutcome. kind cross-validation scheme used type\noutcome? can origami package?Consider WASH Benefits example dataset discussed chapter. can\nincorporate cluster-level information cross-validation scheme? \ncan implement strategy origami package?Consider WASH Benefits example dataset discussed chapter. can\nincorporate cluster-level information cross-validation scheme? \ncan implement strategy origami package?","code":""},{"path":"origami.html","id":"advanced-topics","chapter":"5 Cross-validation","heading":"5.8.2 Advanced Topics","text":"Think dataset spatial dependence structure, \ndegree dependence known groups formed dependence\nstructure clear spillover effects. kind \ncross-validation scheme appropriate case?Think dataset spatial dependence structure, \ndegree dependence known groups formed dependence\nstructure clear spillover effects. kind \ncross-validation scheme appropriate case?Continuing previous problem, kind procedure, \ncross-validation scheme, can use spatial dependence \nclearly defined assumptions made preceding problem?Continuing previous problem, kind procedure, \ncross-validation scheme, can use spatial dependence \nclearly defined assumptions made preceding problem?Consider classification problem large number predictors \nbinary outcome. friendly neighborhood statistician proposes \nfollowing analysis:\nFirst, screen predictors, isolating covariates \nstrongly correlated (binary) outcome labels.\nNext, train learning algorithm using subset covariates\nhighly correlated outcome.\nFinally, use cross-validation estimate tuning parameters \nperformance learning algorithm.\napplication cross-validation correct? ?Consider classification problem large number predictors \nbinary outcome. friendly neighborhood statistician proposes \nfollowing analysis:First, screen predictors, isolating covariates \nstrongly correlated (binary) outcome labels.Next, train learning algorithm using subset covariates\nhighly correlated outcome.Finally, use cross-validation estimate tuning parameters \nperformance learning algorithm.application cross-validation correct? ?","code":""},{"path":"sl3.html","id":"sl3","chapter":"6 Super Learning","heading":"6 Super Learning","text":"Rachael PhillipsBased sl3 R package Jeremy\nCoyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Oleg Sofrygin.","code":""},{"path":"sl3.html","id":"learning-objectives-1","chapter":"6 Super Learning","heading":"Learning Objectives","text":"end chapter able :Select performance metric optimized true prediction\nfunction, define true prediction prediction interest \noptimizer performance metric.Select performance metric optimized true prediction\nfunction, define true prediction prediction interest \noptimizer performance metric.Assemble diverse set (“library”) learners considered super\nlearner. particular, able :\nCustomize learner modifying tuning parameters.\nCreate variations base learner different tuning\nparameter specifications.\nCouple screener(s) learner(s) create learners consider \ncovariates reduced, screener-selected subset .\nAssemble diverse set (“library”) learners considered super\nlearner. particular, able :Customize learner modifying tuning parameters.Create variations base learner different tuning\nparameter specifications.Couple screener(s) learner(s) create learners consider \ncovariates reduced, screener-selected subset .Specify meta-learner optimizes objective function interest.Specify meta-learner optimizes objective function interest.Justify library meta-learner terms prediction problem\nhand, intended use analysis real world, statistical model,\nsample size, number covariates, outcome prevalence discrete\noutcomes.Justify library meta-learner terms prediction problem\nhand, intended use analysis real world, statistical model,\nsample size, number covariates, outcome prevalence discrete\noutcomes.Interpret fit super learner table cross-validated risk\nestimates super learner coefficients.Interpret fit super learner table cross-validated risk\nestimates super learner coefficients.","code":""},{"path":"sl3.html","id":"introduction-3","chapter":"6 Super Learning","heading":"6.1 Introduction","text":"common task data analysis prediction, using observed data \nlearn function takes input data covariates/predictors outputs \npredicted value. Occasionally, scientific question interest lends \ncausal effect estimation. Even scenarios, prediction \nforefront, prediction tasks embedded procedure. instance, \ntargeted minimum loss-based estimation (TMLE) average treatment effect,\npredictive modeling necessary estimating outcome regressions propensity scores.various strategies can employed model relationships \ndata, refer interchangeably “estimators”, “algorithms”, \n“learners”. data, algorithms can pick complex relationships\namong variables necessary adequately model . data, parametric\nregression learners might fit data reasonably well. generally\nimpossible know advance approach best given data\nset prediction problem.Super Learner (SL) solves issue selecting algorithm, can\nconsider many - simplest parametric regressions \ncomplex machine learning algorithms (e.g., neural nets, support vector machines,\netc). Additionally, proven perform well possible\n(good unknown oracle) large samples, given learners specified\n(Dudoit van der Laan, 2005; van der Laan et al., 2004; van der Laan Dudoit, 2003; van der Vaart et al., 2006).\nSL represents entirely pre-specified, flexible, theoretically\ngrounded approach predictive modeling. shown adaptive \nrobust variety applications, even small samples. Detailed\ndescriptions outlining SL procedure widely available (Naimi Balzer, 2018; Polley van der Laan, 2010). Practical considerations specifying SL, including\nspecify rich diverse library learners, choose performance\nmetric SL, specify cross-validation (CV) scheme, described \npre-print article (Phillips et al., 2023). , focus introducing sl3, \nstandard tlverse software package SL.","code":""},{"path":"sl3.html","id":"super-learning-with-sl3","chapter":"6 Super Learning","heading":"Super Learning with sl3:","text":"","code":""},{"path":"sl3.html","id":"how-to-fit-the-super-learner","chapter":"6 Super Learning","heading":"6.2 How to Fit the Super Learner","text":"section, core functionality fitting SL sl3 \nillustrated. sections follow, additional sl3 functionality \npresented.Fitting SL sl3 consists following three steps:Define prediction task make_sl3_Task.Instantiate SL Lrnr_sl.Fit SL task train.","code":""},{"path":"sl3.html","id":"running-example-with-wash-benefits-dataset","chapter":"6 Super Learning","heading":"Running example with WASH Benefits dataset","text":"use WASH Benefits Bangladesh study example guide \noverview sl3. study, interested predicting child\ndevelopment outcome, weight--height z-score, covariates/predictors,\nincluding socio-economic status variables, gestational age, maternal\nfeatures. information dataset described “Meet \nData” chapter \ntlverse handbook.","code":""},{"path":"sl3.html","id":"preliminaries","chapter":"6 Super Learning","heading":"Preliminaries","text":"First, need load data relevant packages R session.","code":""},{"path":"sl3.html","id":"load-the-data","chapter":"6 Super Learning","heading":"Load the data","text":"use fread function data.table R package load WASH\nBenefits example dataset:Next, take peek first rows dataset:","code":"\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\nhead(washb_data)"},{"path":"sl3.html","id":"install-sl3-software-as-needed","chapter":"6 Super Learning","heading":"Install sl3 software (as needed)","text":"install package, recommend first clearing R workspace \nrestarting R session. RStudio, can achieved clicking \ntab “Session” “Clear Workspace”, clicking “Session” \n“Restart R”.can install sl3 using function install_github provided \ndevtools R package. using development (“devel”) version sl3\nmaterials, show install version .R package installed, recommend restarting R session .","code":"\nlibrary(devtools)\ninstall_github(\"tlverse/sl3@devel\")"},{"path":"sl3.html","id":"load-sl3-software","chapter":"6 Super Learning","heading":"Load sl3 software","text":"sl3 installed, can load like R package:","code":"\nlibrary(sl3)"},{"path":"sl3.html","id":"define-the-prediction-task-with-make_sl3_task","chapter":"6 Super Learning","heading":"1. Define the prediction task with make_sl3_Task","text":"sl3_Task object defines prediction task interest. Recall \ntask illustrative example use WASH Benefits Bangladesh\nexample dataset learn function covariates predicting\nweight--height Z-score whz.sl3_Task keeps track roles variables play prediction\nproblem. Additional information relevant prediction task (\nobservational-level weights, offset, id, CV folds) can also specified \nmake_sl3_Task. default CV fold structure sl3 V-fold CV (VFCV)\nV=10 folds. id specified task, clustered V=10 VFCV\nscheme considered; outcome type binary categorical, \nstratified V=10 VFCV scheme considered. Different CV schemes can \nspecified inputting origami folds object, generated \nmake_folds function origami R package. information,\nrefer previous Chapter cross-validation, consult documentation\norigami’s make_folds function (e.g., RStudio, \nloading origami R package inputting “?make_folds” \nConsole). details sl3_Task, refer documentation (e.g., \ninputting “?sl3_Task” R).Tip: type task$ press tab key (press tab twice \nRStudio), can view active public fields, well methods \ncan accessed task$ object. $ like key access\nmany internals object. next section, see can use $\ndig SL fit objects well - obtain predictions SL fit \ncandidate learners, examine SL fit candidates, summarize SL\nfit.","code":"# create the task (i.e., use washb_data to predict outcome using covariates)\ntask <- make_sl3_Task(\n  data = washb_data,\n  outcome = \"whz\",\n  covariates = c(\"tr\", \"fracode\", \"month\", \"aged\", \"sex\", \"momage\", \"momedu\", \n                 \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\", \"elec\", \n                 \"floor\", \"walls\", \"roof\", \"asset_wardrobe\", \"asset_table\", \n                 \"asset_chair\", \"asset_khat\", \"asset_chouki\", \"asset_tv\", \n                 \"asset_refrig\", \"asset_bike\", \"asset_moto\", \"asset_sewmach\", \n                 \"asset_mobile\")\n)\n\n# let's examine the task\ntask\nAn sl3 Task with 4695 obs and these nodes:\n$covariates\n [1] \"tr\"              \"fracode\"         \"month\"           \"aged\"           \n [5] \"sex\"             \"momage\"          \"momedu\"          \"momheight\"      \n [9] \"hfiacat\"         \"Nlt18\"           \"Ncomp\"           \"watmin\"         \n[13] \"elec\"            \"floor\"           \"walls\"           \"roof\"           \n[17] \"asset_wardrobe\"  \"asset_table\"     \"asset_chair\"     \"asset_khat\"     \n[21] \"asset_chouki\"    \"asset_tv\"        \"asset_refrig\"    \"asset_bike\"     \n[25] \"asset_moto\"      \"asset_sewmach\"   \"asset_mobile\"    \"delta_momage\"   \n[29] \"delta_momheight\"\n\n$outcome\n[1] \"whz\"\n\n$id\nNULL\n\n$weights\nNULL\n\n$offset\nNULL\n\n$time\nNULL"},{"path":"sl3.html","id":"instantiate-the-super-learner-with-lrnr_sl","chapter":"6 Super Learning","heading":"2. Instantiate the Super Learner with Lrnr_sl","text":"order create Lrnr_sl need specify, minimum, set \nlearners SL consider candidates. set algorithms \nalso commonly referred “library”. might also specify \nmeta-learner, algorithm ensembles learners (note part \noptional since already defaults set sl3). See “Practical\nconsiderations specifying super learner” step--step guidelines \ntailoring SL specification (including library meta-learner(s)) optimizes prediction task hand (Phillips et al., 2023).Learners properties indicate features support. may use\nsl3_list_properties() function get list properties supported\nleast one learner:Since whz continuous outcome, can identify learners support\noutcome type sl3_list_learners():Now idea learners, let’s instantiate .\ninstantiate Lrnr_glm Lrnr_mean, main terms generalized\nlinear model (GLM) mean model, respectively.learners created , just used default tuning\nparameters. can also customize learner’s tuning parameters incorporate\ndiversity different settings, consider learner different\ntuning parameter specifications., consider base learner, Lrnr_glmnet (.e., GLMs\nelastic net regression), create two different candidates :\nL2-penalized/ridge regression L1-penalized/lasso regression.setting alpha Lrnr_glmnet , customized learner’s tuning\nparameter. instantiate Lrnr_hal9001 show multiple tuning\nparameters (specifically, max_degreeand num_knots) can modified \ntime.Let’s also instantiate learners enforce relationships \nlinear monotonic, diversifies set candidates \ninclude nonparametric learners.Let’s also include generalized additive model (GAM) Bayesian GLM \ndiversify pool consider candidates SL.Now ’ve instantiated set learners, need put together \nSL can consider candidates. sl3, creating \n-called Stack learners. Stack created way \ncreated learners. Stack learner ; \ninterface learners. makes stack special \nconsiders multiple learners : can train simultaneously, \npredictions can combined /compared.can see names learners stack long. \ndefault naming learner sl3 clunky: learner,\nevery tuning parameter sl3 contained name. next section,\n“Naming\nLearners”,\nshow different ways user name learners wish.Now instantiated set learners stacked together, \nready instantiate SL. use default meta-learner, \nnon-negative least squares (NNLS) regression (Lrnr_nnls) continuous\noutcomes. illustrative purposes, still go ahead specify \nfollowing.","code":"sl3_list_properties()\n [1] \"binomial\"      \"categorical\"   \"continuous\"    \"cv\"           \n [5] \"density\"       \"h2o\"           \"ids\"           \"importance\"   \n [9] \"offset\"        \"preprocessing\" \"sampling\"      \"screener\"     \n[13] \"timeseries\"    \"weights\"       \"wrapper\"      sl3_list_learners(properties = \"continuous\")\n [1] \"Lrnr_arima\"                     \"Lrnr_bartMachine\"              \n [3] \"Lrnr_bayesglm\"                  \"Lrnr_bilstm\"                   \n [5] \"Lrnr_bound\"                     \"Lrnr_caret\"                    \n [7] \"Lrnr_cv_selector\"               \"Lrnr_dbarts\"                   \n [9] \"Lrnr_earth\"                     \"Lrnr_expSmooth\"                \n[11] \"Lrnr_ga\"                        \"Lrnr_gam\"                      \n[13] \"Lrnr_gbm\"                       \"Lrnr_glm\"                      \n[15] \"Lrnr_glm_fast\"                  \"Lrnr_glm_semiparametric\"       \n[17] \"Lrnr_glmnet\"                    \"Lrnr_glmtree\"                  \n[19] \"Lrnr_grf\"                       \"Lrnr_gru_keras\"                \n[21] \"Lrnr_gts\"                       \"Lrnr_h2o_glm\"                  \n[23] \"Lrnr_h2o_grid\"                  \"Lrnr_hal9001\"                  \n[25] \"Lrnr_HarmonicReg\"               \"Lrnr_hts\"                      \n[27] \"Lrnr_lightgbm\"                  \"Lrnr_lstm_keras\"               \n[29] \"Lrnr_mean\"                      \"Lrnr_multiple_ts\"              \n[31] \"Lrnr_nnet\"                      \"Lrnr_nnls\"                     \n[33] \"Lrnr_optim\"                     \"Lrnr_pkg_SuperLearner\"         \n[35] \"Lrnr_pkg_SuperLearner_method\"   \"Lrnr_pkg_SuperLearner_screener\"\n[37] \"Lrnr_polspline\"                 \"Lrnr_randomForest\"             \n[39] \"Lrnr_ranger\"                    \"Lrnr_rpart\"                    \n[41] \"Lrnr_rugarch\"                   \"Lrnr_screener_correlation\"     \n[43] \"Lrnr_solnp\"                     \"Lrnr_stratified\"               \n[45] \"Lrnr_svm\"                       \"Lrnr_tsDyn\"                    \n[47] \"Lrnr_xgboost\"                  \nlrn_glm <- Lrnr_glm$new()\nlrn_mean <- Lrnr_mean$new()\n# penalized regressions:\nlrn_ridge <- Lrnr_glmnet$new(alpha = 0)\nlrn_lasso <- Lrnr_glmnet$new(alpha = 1)\n# spline regressions:\nlrn_polspline <- Lrnr_polspline$new()\nlrn_earth <- Lrnr_earth$new()\n\n# fast highly adaptive lasso (HAL) implementation\nlrn_hal <- Lrnr_hal9001$new(max_degree = 2, num_knots = c(3,2), nfolds = 5)\n\n# tree-based methods\nlrn_ranger <- Lrnr_ranger$new()\nlrn_xgb <- Lrnr_xgboost$new()\nlrn_gam <- Lrnr_gam$new()\nlrn_bayesglm <- Lrnr_bayesglm$new()stack <- Stack$new(\n  lrn_glm, lrn_mean, lrn_ridge, lrn_lasso, lrn_polspline, lrn_earth, lrn_hal, \n  lrn_ranger, lrn_xgb, lrn_gam, lrn_bayesglm\n)\nstack\n [1] \"Lrnr_glm_TRUE\"                          \n [2] \"Lrnr_mean\"                              \n [3] \"Lrnr_glmnet_NULL_deviance_10_0_100_TRUE\"\n [4] \"Lrnr_glmnet_NULL_deviance_10_1_100_TRUE\"\n [5] \"Lrnr_polspline\"                         \n [6] \"Lrnr_earth_2_3_backward_0_1_0_0\"        \n [7] \"Lrnr_hal9001_2_1_c(3, 2)_5\"             \n [8] \"Lrnr_ranger_500_TRUE_none_1\"            \n [9] \"Lrnr_xgboost_20_1\"                      \n[10] \"Lrnr_gam_NULL_NULL_GCV.Cp\"              \n[11] \"Lrnr_bayesglm_TRUE\"                     \nsl <- Lrnr_sl$new(learners = stack, metalearner = Lrnr_nnls$new())"},{"path":"sl3.html","id":"fit-the-super-learner-to-the-prediction-task-with-train","chapter":"6 Super Learning","heading":"3. Fit the Super Learner to the prediction task with train","text":"last step fitting SL prediction task call train \nsupply task. call train, set random number generator\nresults reproducible, also time .took 292.6 seconds\n(4.9 minutes) fit SL.","code":"start_time <- proc.time() # start time\n\nset.seed(4197)\nsl_fit <- sl$train(task = task)\n\nruntime_sl_fit <- proc.time() - start_time # end time - start time = run time\nruntime_sl_fit\n   user  system elapsed \n295.640   6.606 292.619 "},{"path":"sl3.html","id":"summary","chapter":"6 Super Learning","heading":"Summary","text":"section, core functionality fitting SL sl3 \nillustrated. consists following three steps:Define prediction task make_sl3_Task.Instantiate SL Lrnr_sl.Fit SL task train.example demonstrative purposes . See Phillips et al. (2023) \nstep--step guidelines constructing SL well-specified \nprediction task hand.","code":""},{"path":"sl3.html","id":"additional-sl3-topics","chapter":"6 Super Learning","heading":"Additional sl3 Topics:","text":"","code":""},{"path":"sl3.html","id":"obtaining-predictions","chapter":"6 Super Learning","heading":"6.3 Obtaining Predictions","text":"","code":""},{"path":"sl3.html","id":"super-learner-and-candidate-learner-predictions","chapter":"6 Super Learning","heading":"6.3.1 Super learner and candidate learner predictions","text":"draw fitted SL object , sl_fit, obtain \nSL’s predicted whz value subject.can also obtain predicted values candidate learner SL. \nobtain predictions GLM learner.Note predicted values SL correspond -called “full fits”\ncandidate learners, candidates fit entire\nanalytic dataset, .e., data supplied data make_sl3_Task.\nFigure 2 Phillips et al. (2023) provides visual overview SL fitting\nprocedure.visualize observed values whz predicted whz values \nSL, GLM mean.\nFIGURE 6.1: Observed predicted values weight--height z-score (whz)\n","code":"sl_preds <- sl_fit$predict(task = task)\nhead(sl_preds)\n[1] -0.5719 -0.8717 -0.6881 -0.7342 -0.6308 -0.6596glm_preds <- sl_fit$learner_fits$Lrnr_glm_TRUE$predict(task = task)\nhead(glm_preds)\n[1] -0.7262 -0.9361 -0.7085 -0.6492 -0.7013 -0.8462# we can also access the candidate learner full fits directly and obtain\n# the same \"full fit\" candidate predictions from there \n# (we split this into two lines to avoid overflow)\nstack_full_fits <- sl_fit$fit_object$full_fit$learner_fits$Stack$learner_fits\nglm_preds_full_fit <- stack_full_fits$Lrnr_glm_TRUE$predict(task)\n\n# check that they are identical\nidentical(glm_preds, glm_preds_full_fit)\n[1] TRUE\n# table of observed and predicted outcome values and arrange by observed values\ndf_plot <- data.table(\n  Obs = washb_data[[\"whz\"]], SL_Pred = sl_preds, GLM_Pred = glm_preds,\n  Mean_Pred = sl_fit$learner_fits$Lrnr_mean$predict(task)\n)\ndf_plot <- df_plot[order(df_plot$Obs), ] \nhead(df_plot)\n# melt the table so we can plot observed and predicted values\ndf_plot$id <- seq(1:nrow(df_plot))\ndf_plot_melted <- melt(\n  df_plot, id.vars = \"id\",\n  measure.vars = c(\"Obs\", \"SL_Pred\", \"GLM_Pred\", \"Mean_Pred\")\n)\n\nlibrary(ggplot2)\nggplot(df_plot_melted, aes(id, value, color = variable)) + \n  geom_point(size = 0.1) + \n  labs(x = \"Subjects (ordered by increasing whz)\", \n       y = \"whz\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank(),\n        axis.text.x = element_blank(), axis.ticks.x = element_blank()) + \n  guides(color = guide_legend(override.aes = list(size = 1)))"},{"path":"sl3.html","id":"cross-validated-predictions","chapter":"6 Super Learning","heading":"6.3.2 Cross-validated predictions","text":"can also obtain cross-validated (CV) predictions candidate\nlearners. can different ways.","code":"# one way to obtain the CV predictions for the candidate learners\ncv_preds_option1 <- sl_fit$fit_object$cv_fit$predict_fold(\n  task = task, fold_number = \"validation\"\n)\n# another way to obtain the CV predictions for the candidate learners\ncv_preds_option2 <- sl_fit$fit_object$cv_fit$predict(task = task)\n\n# we can check that they are identical\nidentical(cv_preds_option1, cv_preds_option2)\n[1] TRUE\nhead(cv_preds_option1)"},{"path":"sl3.html","id":"predict_fold","chapter":"6 Super Learning","heading":"predict_fold","text":"first option get CV predictions, cv_preds_option1, used \npredict_fold function obtain validation set predictions across folds.\nfunction exists learner fits cross-validated sl3,\nlike Lrnr_sl. addition supplying fold_number = \"validation\"\npredict_fold, can set fold_number = \"full\" obtain predictions \nlearners fit entire analytic dataset (.e., data supplied \nmake_sl3_Task). instance, show glm_preds calculated\ncan also obtained setting fold_number = \"full\".can also supply specific integer 1 number CV folds\nfold_number argument predict_fold; example \nfunctionality shown next part.","code":"full_fit_preds <- sl_fit$fit_object$cv_fit$predict_fold(\n  task = task, fold_number = \"full\"\n)\nglm_full_fit_preds <- full_fit_preds$Lrnr_glm_TRUE\n\n# check that they are identical\nidentical(glm_preds, glm_full_fit_preds)\n[1] TRUE"},{"path":"sl3.html","id":"cross-validated-predictions-by-hand","chapter":"6 Super Learning","heading":"Cross-validated predictions by hand","text":"can get CV predictions “hand”, tapping folds, \nusing fitted candidate learners (trained training\nset fold) predict validation set outcomes (seen \ntraining).","code":"##### CV predictions \"by hand\" #####\n# for each fold, i, we obtain validation set predictions:\ncv_preds_list <- lapply(seq_along(task$folds), function(i){\n  \n  # get validation dataset for fold i:\n  v_data <- task$data[task$folds[[i]]$validation_set, ]\n  \n  # get observed outcomes in fold i's validation dataset:\n  v_outcomes <- v_data[[\"whz\"]]\n\n  # make task (for prediction) using fold i's validation dataset as data, \n  # and keeping all else the same:\n  v_task <- make_sl3_Task(covariates = task$nodes$covariates, data = v_data)\n  \n  # get predicted outcomes for fold i's validation dataset, using candidates \n  # trained to fold i's training dataset\n  v_preds <- sl_fit$fit_object$cv_fit$predict_fold(\n    task = v_task, fold_number = i\n  )\n  # note: v_preds is a matrix of candidate learner predictions, where the \n  # number of rows is the number of observations in fold i's validation dataset \n  # and the number of columns is the number of candidate learners (excluding \n  # any that might have failed)\n  \n  # an identical way to get v_preds, which is used when we calculate the \n  # cv risk by hand in a later part of this chapter:\n  # v_preds <- sl_fit$fit_object$cv_fit$fit_object$fold_fits[[i]]$predict(\n  #   task = v_task\n  # )\n  \n  # we will also return the row indices for fold i's validation set, so we \n  # can later reorder the CV predictions and make sure they are equal to what \n  # we obtained above\n  return(list(\"v_preds\" = v_preds, \"v_index\" = task$folds[[i]]$validation_set))\n})\n\n# extract the validation set predictions across all folds\ncv_preds_byhand <- do.call(rbind, lapply(cv_preds_list, \"[[\", \"v_preds\"))\n\n# extract the indices of validation set observations across all folds\n# then reorder cv_preds_byhand to correspond to the ordering in the data\nrow_index_in_data <- unlist(lapply(cv_preds_list, \"[[\", \"v_index\"))\ncv_preds_byhand_ordered <- cv_preds_byhand[order(row_index_in_data), ]\n# now we can check that they are identical\nidentical(cv_preds_option1, cv_preds_byhand_ordered)\n[1] TRUE"},{"path":"sl3.html","id":"predictions-with-new-data","chapter":"6 Super Learning","heading":"6.3.3 Predictions with new data","text":"wanted obtain predicted values new data need \ncreate new sl3_Task new data. Also, covariates new\nsl3_Task must identical covariates sl3_Task training.\nexample, let’s assume new covariate data washb_data_new \nwant use fitted SL obtain predicted weight--height\nz-score values.","code":"\n# we do not evaluate this code chunk, as `washb_data_new` does not exist\nprediction_task <- make_sl3_Task(\n  data = washb_data_new, # assuming we have some new data for predictions\n  covariates = c(\"tr\", \"fracode\", \"month\", \"aged\", \"sex\", \"momage\", \"momedu\", \n                 \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\", \"elec\", \n                 \"floor\", \"walls\", \"roof\", \"asset_wardrobe\", \"asset_table\", \n                 \"asset_chair\", \"asset_khat\", \"asset_chouki\", \"asset_tv\", \n                 \"asset_refrig\", \"asset_bike\", \"asset_moto\", \"asset_sewmach\", \n                 \"asset_mobile\")\n)\nsl_preds_new_task <- sl_fit$predict(task = prediction_task)"},{"path":"sl3.html","id":"counterfactual-predictions","chapter":"6 Super Learning","heading":"6.3.4 Counterfactual predictions","text":"Counterfactual predictions predicted values intervention \ninterest. Recall can obtain predicted values new data \ncreating sl3_Task new data whose covariates match set\nconsidered training. example draws WASH Benefits\nBangladesh study, suppose like obtain predictions every\nsubject’s weight--height z-score (whz) outcome intervention \ntreatment (tr) sets nutrition, water, sanitation, \nhandwashing regime.First need create copy dataset, can intervene \ntr copied dataset, create new sl3_Task using copied data \ncovariates training task, finally obtain predictions\nfitted SL (named sl_fit previous section).Note type intervention, every subject receives \nintervention, referred “static”. Interventions vary depending \ncharacteristics subject referred “dynamic”. instance,\nmight consider intervention sets treatment desired\n(nutrition, water, sanitation, handwashing) regime subject \nrefridgerator, nutrition-omitted (water, sanitation, handwashing)\nregime otherwise.","code":"### 1. Copy data\ntr_intervention_data <- data.table::copy(washb_data) \n\n### 2. Define intervention in copied dataset\ntr_intervention <- rep(\"Nutrition + WSH\", nrow(washb_data))\n# NOTE: When we intervene on a categorical variable (such as \"tr\"), we need to \n#       define the intervention as a categorical variable (ie a factor).\n#       Also, even though not all levels of the factor will be represented in \n#       the intervention, we still need this factor to reflect all of the \n#       levels that are present in the observed data\ntr_levels <- levels(washb_data[[\"tr\"]])\ntr_levels\n[1] \"Control\"         \"Handwashing\"     \"Nutrition\"       \"Nutrition + WSH\"\n[5] \"Sanitation\"      \"WSH\"             \"Water\"          \ntr_intervention <- factor(tr_intervention, levels = tr_levels)\ntr_intervention_data[,\"tr\" := tr_intervention, ]\n\n### 3. Create a new sl3_Task\n# note that we do not need to specify the outcome in this new task since we are \n# only using it to obtain predictions\ntr_intervention_task <- make_sl3_Task(\n  data = tr_intervention_data, \n  covariates = c(\"tr\", \"fracode\", \"month\", \"aged\", \"sex\", \"momage\", \"momedu\", \n                 \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\", \"elec\", \n                 \"floor\", \"walls\", \"roof\", \"asset_wardrobe\", \"asset_table\", \n                 \"asset_chair\", \"asset_khat\", \"asset_chouki\", \"asset_tv\", \n                 \"asset_refrig\", \"asset_bike\", \"asset_moto\", \"asset_sewmach\", \n                 \"asset_mobile\")\n)\n### 4. Get predicted values under intervention of interest\n# SL predictions of what \"whz\" would have been had everyone received \"tr\" \n# equal to \"Nutrition + WSH\"\ncounterfactual_pred <- sl_fit$predict(tr_intervention_task)\ndynamic_tr_intervention_data <- data.table::copy(washb_data) \n\ndynamic_tr_intervention <- ifelse(\n  washb_data[[\"asset_refrig\"]] == 1, \"Nutrition + WSH\", \"WSH\"\n)\ndynamic_tr_intervention <- factor(dynamic_tr_intervention, levels = tr_levels)\ndynamic_tr_intervention_data[,\"tr\" := dynamic_tr_intervention, ]\n\ndynamic_tr_intervention_task <- make_sl3_Task(\n  data = dynamic_tr_intervention_data, \n  covariates = c(\"tr\", \"fracode\", \"month\", \"aged\", \"sex\", \"momage\", \"momedu\", \n                 \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\", \"elec\", \n                 \"floor\", \"walls\", \"roof\", \"asset_wardrobe\", \"asset_table\", \n                 \"asset_chair\", \"asset_khat\", \"asset_chouki\", \"asset_tv\", \n                 \"asset_refrig\", \"asset_bike\", \"asset_moto\", \"asset_sewmach\", \n                 \"asset_mobile\")\n)\n### 4. Get predicted values under intervention of interest\n# SL predictions of what \"whz\" would have been had every subject received \"tr\" \n# equal to \"Nutrition + WSH\" if they had a fridge and \"WSH\" if they didn't have \n# a fridge\ncounterfactual_pred <- sl_fit$predict(dynamic_tr_intervention_task)"},{"path":"sl3.html","id":"summarizing-super-learner-fits","chapter":"6 Super Learning","heading":"6.4 Summarizing Super Learner Fits","text":"","code":""},{"path":"sl3.html","id":"super-learner-coefficients-fitted-meta-learner-summary","chapter":"6 Super Learning","heading":"6.4.1 Super Learner coefficients / fitted meta-learner summary","text":"can see meta-learner created function learners \nways. illustrative example, considered default, NNLS meta-learner\ncontinuous outcomes. meta-learners simply learn weighted\ncombination, can examine coefficients.can also examine coefficients directly accessing meta-learner’s\nfit object.Direct access meta-learner fit object also handy \ncomplex meta-learners (e.g., non-parametric meta-learners) defined\nsimple set main terms regression coefficients.","code":"round(sl_fit$coefficients, 3)\n                          Lrnr_glm_TRUE                               Lrnr_mean \n                                  0.000                                   0.000 \nLrnr_glmnet_NULL_deviance_10_0_100_TRUE Lrnr_glmnet_NULL_deviance_10_1_100_TRUE \n                                  0.096                                   0.000 \n                         Lrnr_polspline         Lrnr_earth_2_3_backward_0_1_0_0 \n                                  0.168                                   0.399 \n             Lrnr_hal9001_2_1_c(3, 2)_5             Lrnr_ranger_500_TRUE_none_1 \n                                  0.000                                   0.337 \n                      Lrnr_xgboost_20_1               Lrnr_gam_NULL_NULL_GCV.Cp \n                                  0.000                                   0.000 \n                     Lrnr_bayesglm_TRUE \n                                  0.000 metalrnr_fit <- sl_fit$fit_object$cv_meta_fit$fit_object\nround(metalrnr_fit$coefficients, 3)\n                          Lrnr_glm_TRUE                               Lrnr_mean \n                                  0.000                                   0.000 \nLrnr_glmnet_NULL_deviance_10_0_100_TRUE Lrnr_glmnet_NULL_deviance_10_1_100_TRUE \n                                  0.096                                   0.000 \n                         Lrnr_polspline         Lrnr_earth_2_3_backward_0_1_0_0 \n                                  0.168                                   0.399 \n             Lrnr_hal9001_2_1_c(3, 2)_5             Lrnr_ranger_500_TRUE_none_1 \n                                  0.000                                   0.337 \n                      Lrnr_xgboost_20_1               Lrnr_gam_NULL_NULL_GCV.Cp \n                                  0.000                                   0.000 \n                     Lrnr_bayesglm_TRUE \n                                  0.000 "},{"path":"sl3.html","id":"cross-validated-predictive-performance","chapter":"6 Super Learning","heading":"6.4.2 Cross-validated predictive performance","text":"can obtain table cross-validated (CV) predictive performance, .e.,\nCV risk, learner included SL. , use \nsquared error loss evaluation function, equates mean\nsquared error (MSE) metric summarize predictive performance. \nreason use MSE valid metric estimating \nconditional mean, ’re learning prediction function \nWASH Benefits example. information selecting appropriate\nperformance metric, see Phillips et al. (2023).","code":"\ncv_risk_table <- sl_fit$cv_risk(eval_fun = loss_squared_error)\ncv_risk_table[,c(1:3)]"},{"path":"sl3.html","id":"cross-validated-predictive-performance-by-hand","chapter":"6 Super Learning","heading":"Cross-validated predictive performance by hand","text":"Similar got CV predictions “hand”, can also calculate CV\nperformance/risk way exposes procedure. Specifically, done\ntapping folds, using fitted candidate learners\n(trained training set fold) predict validation set\noutcomes (seen training) measure predictive\nperformance (.e., risk). candidate learner’s fold-specific risk \naveraged across folds obtain CV risk. function cv_risk \ninternally show hand , can \nhelpful understanding CV risk calculated.","code":"##### CV risk \"by hand\" #####\n# for each fold, i, we obtain predictive performance/risk for each candidate:\ncv_risks_list <- lapply(seq_along(task$folds), function(i){\n  \n  # get validation dataset for fold i:\n  v_data <- task$data[task$folds[[i]]$validation_set, ]\n  \n  # get observed outcomes in fold i's validation dataset:\n  v_outcomes <- v_data[[\"whz\"]]\n\n  # make task (for prediction) using fold i's validation dataset as data, \n  # and keeping all else the same:\n  v_task <- make_sl3_Task(covariates = task$nodes$covariates, data = v_data)\n  \n  # get predicted outcomes for fold i's validation dataset, using candidates \n  # trained to fold i's training dataset\n  v_preds <- sl_fit$fit_object$cv_fit$fit_object$fold_fits[[i]]$predict(v_task)\n  # note: v_preds is a matrix of candidate learner predictions, where the \n  # number of rows is the number of observations in fold i's validation dataset \n  # and the number of columns is the number of candidate learners (excluding \n  # any that might have failed)\n  \n  # calculate predictive performance for fold i for each candidate\n  eval_function <- loss_squared_error # valid for estimation of conditional mean\n  v_losses <- apply(v_preds, 2, eval_function, v_outcomes)\n  cv_risks <- colMeans(v_losses)\n  return(cv_risks)\n})\n# average the predictive performance across all folds for each candidate\ncv_risks_byhand <- colMeans(do.call(rbind, cv_risks_list))\ncv_risk_table_byhand <- data.table(\n  learner = names(cv_risks_byhand), MSE = cv_risks_byhand\n)\n# check that the CV risks are identical when calculated by hand and function\n# (ignoring small differences by rounding to the fourth decimal place)\nidentical(\n  round(cv_risk_table_byhand$MSE,4), round(as.numeric(cv_risk_table$MSE),4)\n)\n[1] TRUE"},{"path":"sl3.html","id":"cross-validated-super-learner","chapter":"6 Super Learning","heading":"6.4.3 Cross-validated Super Learner","text":"can see CV risk table SL listed. \nCV risk SL unless cross-validate \ninclude candidate another SL; latter shown next\nsubsection.\n, show obtain CV risk estimate SL using function\ncv_sl. Like called sl$train, set random number\ngenerator results reproducible, also time .took 3051.4 seconds (50.9 minutes) fit CV SL.CV risk SL \n0.0234, lower\ncandidates’ CV risks.can see SL fits varied across folds coefficients \nSL fold.","code":"\nstart_time <- proc.time()\n\nset.seed(569)\ncv_sl_fit <- cv_sl(lrnr_sl = sl_fit, task = task, eval_fun = loss_squared_error)\n\nruntime_cv_sl_fit <- proc.time() - start_time\nruntime_cv_sl_fit   user  system elapsed \n 2792.6   159.6  3051.4 \ncv_sl_fit$cv_risk[,c(1:3)]\nround(cv_sl_fit$coef, 3)"},{"path":"sl3.html","id":"revere-cross-validated-predictive-performance-of-super-learner","chapter":"6 Super Learning","heading":"6.4.4 Revere-cross-validated predictive performance of Super Learner","text":"can also use -called “revere”, obtain partial CV risk SL,\nSL candidate learner fits cross-validated meta-learner fit\n. takes essentially extra time calculate revere-CV\nperformance/risk estimate SL, since already CV fits \ncandidates. isn’t say revere-CV SL performance can replace \nobtained actual CV SL. Revere can used quickly examine \napproximate lower bound SL’s CV risk meta-learner simple\nmodel, like NNLS. can output revere-based CV risk estimate setting\nget_sl_revere_risk = TRUE cv_risk.","code":"\ncv_risk_w_sl_revere <- sl_fit$cv_risk(\n  eval_fun = loss_squared_error, get_sl_revere_risk = TRUE\n)\ncv_risk_w_sl_revere[,c(1:3)]"},{"path":"sl3.html","id":"revere-cross-validated-predictive-performance-of-super-learner-by-hand","chapter":"6 Super Learning","heading":"Revere-cross-validated predictive performance of Super Learner by hand","text":"show calculate revere-CV predictive performance/risk SL \nhand , might helpful understanding revere can \nused obtain partial CV performance/risk estimate SL.reason fully cross-validated risk estimate \ncv_meta_fit object (trained meta-learner), previously\nfit entire matrix CV predictions every fold (.e., \nmeta-level dataset; see Figure 2 Phillips et al. (2023) detail). \nrevere-based risks true CV risk. meta-learner simple\nregression function, instead flexible learner (e.g., random\nforest) used meta-learner, revere-CV risk estimate \nresulting SL worse approximation CV risk estimate. \nflexible learners likely overfit. simple\nparametric regressions used meta-learner, like considered \nSL (NNLS Lrnr_nnls), like default meta-learners \nsl3, revere-CV risk quick way examine approximation \nCV risk estimate SL can thought ballpark lower bound\n. idea holds example; , simple NNLS\nmeta-learner revere risk estimate SL (1.0033)\nclose , slightly lower , CV risk estimate SL\n(1.0067).","code":"##### revere-based risk \"by hand\" #####\n# for each fold, i, we obtain predictive performance/risk for the SL\nsl_revere_risk_list <- lapply(seq_along(task$folds), function(i){\n  # get validation dataset for fold i:\n  v_data <- task$data[task$folds[[i]]$validation_set, ]\n\n  # get observed outcomes in fold i's validation dataset:\n  v_outcomes <- v_data[[\"whz\"]]\n\n  # make task (for prediction) using fold i's validation dataset as data,\n  # and keeping all else the same:\n  v_task <- make_sl3_Task(\n    covariates = task$nodes$covariates, data = v_data\n  )\n\n  # get predicted outcomes for fold i's validation dataset, using candidates\n  # trained to fold i's training dataset\n  v_preds <- sl_fit$fit_object$cv_fit$fit_object$fold_fits[[i]]$predict(v_task)\n\n  # make a metalevel task (for prediction with sl):\n  v_meta_task <- make_sl3_Task(\n    covariates = sl_fit$fit_object$cv_meta_task$nodes$covariates,\n    data = v_preds\n  )\n\n  # get predicted outcomes for fold i's metalevel dataset, using the fitted\n  # metalearner, cv_meta_fit\n  sl_revere_v_preds <- sl_fit$fit_object$cv_meta_fit$predict(task=v_meta_task)\n  # note: cv_meta_fit was trained on the metalevel dataset, which contains the\n  # candidates' cv predictions and validation dataset outcomes across ALL folds,\n  # so cv_meta_fit has already seen fold i's validation dataset outcomes.\n\n  # calculate predictive performance for fold i for the SL\n  eval_function <- loss_squared_error # valid for estimation of conditional mean\n  # note: by evaluating the predictive performance of the SL using outcomes\n  # that were already seen by the metalearner, this is not a cross-validated\n  # measure of predictive performance for the SL.\n  sl_revere_v_loss <- eval_function(\n    pred = sl_revere_v_preds, observed = v_outcomes\n  )\n  sl_revere_v_risk <- mean(sl_revere_v_loss)\n  return(sl_revere_v_risk)\n})\n# average the predictive performance across all folds for the SL\nsl_revere_risk_byhand <- mean(unlist(sl_revere_risk_list))\nsl_revere_risk_byhand\n[1] 1.003\n\n# check that our calculation by hand equals what is output in cv_risk_table_revere\nsl_revere_risk <- as.numeric(cv_risk_w_sl_revere[learner==\"SuperLearner\",\"MSE\"])\nsl_revere_risk\n[1] 1.003"},{"path":"sl3.html","id":"discrete-super-learner","chapter":"6 Super Learning","heading":"6.5 Discrete Super Learner","text":"glossary (Table 1) entry discrete SL (dSL) Phillips et al. (2023),\ndSL “SL uses winner-take-meta-learner called\ncross-validated selector. dSL therefore identical candidate\nbest cross-validated performance; predictions \ncandidate’s predictions”. cross-validated selector \nLrnr_cv_selector sl3 (see Lrnr_cv_selector documentation \ndetail) dSL instantiated sl3 using Lrnr_cv_selector \nmeta-learner Lrnr_sl.Just like , use learner’s train method fit \nprediction task.Following subsection “Summarizing Super Learner\nFits”\n, can see Lrnr_cv_selector meta-learner created function \ncandidates.can also examine CV risk candidates alongside coefficients:multivariate adaptive splines regression candidate (Lrnr_earth) \nlowest CV risk. Indeed, winner-take-meta-learner Lrnr_cv_selector\ngave weight one others zero weight; resulting dSL \ndefined weighted combination, .e., dSL_fit identical \nfull fit Lrnr_earth. verify dSL_fit’s predictions identical\nLrnr_earth’s .","code":"\ncv_selector <- Lrnr_cv_selector$new(eval_function = loss_squared_error)\ndSL <- Lrnr_sl$new(learners = stack, metalearner = cv_selector)\nset.seed(4197)\ndSL_fit <- dSL$train(task)round(dSL_fit$coefficients, 3)\n                          Lrnr_glm_TRUE                               Lrnr_mean \n                                      0                                       0 \nLrnr_glmnet_NULL_deviance_10_0_100_TRUE Lrnr_glmnet_NULL_deviance_10_1_100_TRUE \n                                      0                                       0 \n                         Lrnr_polspline         Lrnr_earth_2_3_backward_0_1_0_0 \n                                      0                                       1 \n             Lrnr_hal9001_2_1_c(3, 2)_5             Lrnr_ranger_500_TRUE_none_1 \n                                      0                                       0 \n                      Lrnr_xgboost_20_1               Lrnr_gam_NULL_NULL_GCV.Cp \n                                      0                                       0 \n                     Lrnr_bayesglm_TRUE \n                                      0 \ndSL_cv_risk_table <- dSL_fit$cv_risk(eval_fun = loss_squared_error)\ndSL_cv_risk_table[,c(1:3)]dSL_pred <- dSL_fit$predict(task)\nearth_pred <- dSL_fit$learner_fits$Lrnr_earth_2_3_backward_0_1_0_0$predict(task)\nidentical(dSL_pred, earth_pred)\n[1] TRUE"},{"path":"sl3.html","id":"including-ensemble-super-learners-as-candidates-in-discrete-super-learner","chapter":"6 Super Learning","heading":"6.5.1 Including ensemble Super Learner(s) as candidate(s) in discrete Super Learner","text":"recommend using CV evaluate predictive performance SL. \nshowed cv_sl . also seen \ninclude learner candidate SL (sl3 terms, include \nlearner Stack passed Lrnr_sl learners), able \nexamine CV risk. Also, use dSL, candidate achieved \nlowest CV risk defines resulting SL. therefore can use dSL automate\nprocedure obtaining final SL represents candidate \nbest cross-validated predictive performance. ensemble SL (eSL) \ncandidate learners considered dSL candidates, eSL’s CV\nperformance can compared learners \nconstructed, final SL candidate achieved lowest CV\nrisk. glossary (Table 1) entry eSL Phillips et al. (2023), \neSL “SL uses parametric non-parametric algorithm \nmeta-learner. Therefore, eSL defined combination multiple\ncandidates; predictions defined combination multiple candidates’\npredictions.” following, show include eSL, multiple\neSLs, candidates dSL.Recall SL object, sl, defined section 2:sl eSL since used NNLS meta-learner. rename sl \neSL_metaNNLS clarify eSL uses NNLS \nmeta-learner. Note candidate learners eSL passed\nlearners argument, .e., stack.consider eSL_metaNNLS additional candidate stack, can\ncreate new stack includes original candidate learners eSL.instantiate dSL considers candidates eSL_metaNNLS \nindividual learners eSL_metaNNLS constructed, define\nnew Lrnr_sl considers stack_with_eSL candidates \nLrnr_cv_selector meta-learner.include eSL candidate dSL, allows eSL’s CV\nperformance compared learners \nconstructed. similar calling CV SL, cv_sl, . difference\nincluding eSL candidate dSL calling cv_sl \nformer automates procedure final SL learner \nachieved best CV predictive performance, .e., lowest CV risk. eSL\noutperforms candidate, dSL end selecting \nresulting SL eSL. mentioned Phillips et al. (2023), “another advantage\napproach multiple eSLs use flexible meta-learner\nmethods (e.g., non-parametric machine learning algorithms like HAL) can \nevaluated simultaneously.”, show multiple eSLs can included candidates dSL:included candidates dSL:eSL , eSL_metaNNLS;learners considered candidates (1);eSL considered candidate learners (1) convex\ncombination-constrained NNLS meta-learner;eSL considered candidate learners (1) lasso\nmeta-learner, using lrn_lasso instantiated section 2;eSL considered candidate learners (1) \nmultivariate adaptive regression splines (earth) meta-learner, using\nlrn_earth instantiated section 2;eSL considered candidate learners (1) \nranger meta-learner, using lrn_ranger instantiated section 2; andan eSL considered candidate learners (1) \nHAL meta-learner, using lrn_hal instantiated section 2.Running many eSLs dSL currently computationally intensive\nsl3, akin running cross-validated SL eSL. Parallel\nprogramming (reviewed ) recommended training learners \ncomputationally intensive, like dSL defined . , parallel\nprocessing scheme defined calling dSL$train(task) order\nspeed run time.","code":"\n# in the section 2 we defined Lrnr_sl as\n# sl <- Lrnr_sl$new(learners = stack, metalearner = Lrnr_nnls$new())\n# let's rename it to clarify that this is an eSL that uses NNLS as meta-learner\neSL_metaNNLS <- sl\nstack_with_eSL <- Stack$new(stack, eSL_metaNNLS)\ncv_selector <- Lrnr_cv_selector$new(eval_function = loss_squared_error)\ndSL <- Lrnr_sl$new(learners = stack_with_eSL, metalearner = cv_selector)\n# instantiate more eSLs\neSL_metaNNLSconvex <- Lrnr_sl$new(\n  learners = stack, metalearner = Lrnr_nnls$new(convex = TRUE)\n)\neSL_metaLasso <- Lrnr_sl$new(learners = stack, metalearner = lrn_lasso)\neSL_metaEarth <- Lrnr_sl$new(learners = stack, metalearner = lrn_earth)\neSL_metaRanger <- Lrnr_sl$new(learners = stack, metalearner = lrn_ranger)\neSL_metaHAL <- Lrnr_sl$new(learners = stack, metalearner = lrn_hal)\n# adding the eSLs to the stack that defined them\nstack_with_eSLs <- Stack$new(\n  stack, eSL_metaNNLS, eSL_metaNNLSconvex, eSL_metaLasso, eSL_metaEarth, \n  eSL_metaRanger, eSL_metaHAL\n)\n# specify dSL\ndSL <- Lrnr_sl$new(learners = stack_with_eSLs, metalearner = cv_selector)"},{"path":"sl3.html","id":"parallel-processing","chapter":"6 Super Learning","heading":"6.6 Parallel Processing","text":"’s straightforward take advantage sl3’s built-parallel processing\nsupport, draws future R\npackage, \nprovides lightweight, unified Future API sequential parallel\nprocessing R expressions via futures. future package\ndocumentation: “package implements sequential, multicore, multisession, cluster futures. , R expressions can evaluated local\nmachine, parallel set local machines, distributed mix local\nremote machines. Extensions package implement additional backends\nprocessing futures via compute cluster schedulers, etc. \nunified API, need modify code order switch \nsequential local machine , say, distributed processing remote\ncompute cluster. Another strength package global variables \nfunctions automatically identified exported needed, making \nstraightforward tweak existing code make use futures.”use future sl3, can simply choose futures plan(), shown\n.","code":"# let's load the future package and set n-1 cores for parallel processing\nlibrary(future)\nncores <- availableCores()-1\nncores\nsystem \n     1 \nplan(multicore, workers = ncores)\n# now, let's re-train sl in parallel for demonstrative purposes\n# we will also set a stopwatch so we can see how long this takes\nstart_time <- proc.time()\n\nset.seed(4197)\nsl_fit_parallel <- sl$train(task)\n\nruntime_sl_fit_parallel <- proc.time() - start_time\nruntime_sl_fit_parallel\n   user  system elapsed \n284.359   5.969 281.609 "},{"path":"sl3.html","id":"default-data-pre-processing","chapter":"6 Super Learning","heading":"6.7 Default Data Pre-processing","text":"sl3 required analytic dataset (.e., dataset\nconsisting observations outcome covariates) contain \nmissing values, contain character factor covariates.\nsubsection, review default functionality sl3 takes care\ninternally; specifically, data pre-processing occurs \nmake_sl3_Task called.Users can also perform pre-processing creating sl3_Task\n(needed) bypass default functionality discussed following.\nSee Phillips et al. (2023), section “Preliminaries: Analytic dataset pre-processing”\ninformation general guidelines follow pre-processing \nanalytic dataset, including considerations pre-processing high\ndimensional settings.Recall sl3_Task object defines prediction task interest. \ntask illustrative example use WASH Benefits\nBangladesh data learn function covariates predicting\nweight--height Z-score whz. details sl3_Task, refer \ndocumentation (e.g., inputting “?sl3_Task” R). instantiate \ntask order examine pre-processing washb_data.","code":"# create the task (i.e., use washb_data to predict outcome using covariates)\ntask <- make_sl3_Task(\n  data = washb_data,\n  outcome = \"whz\",\n  covariates = c(\"tr\", \"fracode\", \"month\", \"aged\", \"sex\", \"momage\", \"momedu\", \n                 \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\", \"elec\", \n                 \"floor\", \"walls\", \"roof\", \"asset_wardrobe\", \"asset_table\", \n                 \"asset_chair\", \"asset_khat\", \"asset_chouki\", \"asset_tv\", \n                 \"asset_refrig\", \"asset_bike\", \"asset_moto\", \"asset_sewmach\", \n                 \"asset_mobile\")\n)\nWarning in process_data(data, nodes, column_names = column_names, flag = flag,\n: Imputing missing values and adding missingness indicators for the following\ncovariates with missing values: momage, momheight. See documentation of the\nprocess_data function for details."},{"path":"sl3.html","id":"imputation-and-missingness-indicators","chapter":"6 Super Learning","heading":"6.7.1 Imputation and missingness indicators","text":"Notice warning appeared created task . (muted \nwarning created task previous section). warning states\nmissing covariate data detected imputed. covariate column\nmissing values, sl3 uses median impute missing continuous\ncovariates, mode impute discrete (binary categorical) covariates.Also, covariate missing values, additional column indicating\nwhether value imputed incorporated. -called “missingness\nindicator” covariates can helpful, pattern covariate missingness\nmight informative predicting outcome.Users free handle missingness covariate data creating\nsl3 task. case, recommend inclusion \nmissingness indicator covariate. Let’s examine greater detail \ncompleteness. ’s also easier see ’s going examining \nexample.First, let’s examine missingness data:can see covariates momage momheight missing observations.\nLet’s check rows data missing values:called make_sl3_Task using washb_data missing covariate values,\nmomage momheight imputed respective medians (since \ncontinuous), missingness indicator (denoted prefix “delta_”) \nadded . See :Indeed, can see washb_task$data missing values. missingness\nindicators take value 0 observation original data\nvalue 1 observation original data.data supplied make_sl3_Task contains missing outcome values, \nerror thrown. Missing outcomes data can easily dropped \ntask created, setting drop_missing_outcome = TRUE. general, \nrecommend dropping missing outcomes data pre-processing, unless \nproblem interest purely prediction. complete case analyses\ngenerally biased; typically unrealistic assume missingness \ncompletely random therefore unsafe just drop observations \nmissing outcomes. instance, estimation estimands admit\nTargeted Minimum Loss-based Estimators (.e., pathwise differentiable estimands,\nincluding parameters arising causal inference violate\npositivity, reviewed following chapters), missingness \nreflected expression question interest (e.g., \naverage effect treatment Drug compared standard\ncare loss follow-) also incorporated estimation\nprocedure. , probability loss follow-prediction\nfunction approximated (e.g., SL) incorporated \nestimation target parameter inference / uncertainty\nquantification.","code":"# which columns have missing values, and how many observations are missing?\ncolSums(is.na(washb_data))\n           whz             tr        fracode          month           aged \n             0              0              0              0              0 \n           sex         momage         momedu      momheight        hfiacat \n             0             18              0             31              0 \n         Nlt18          Ncomp         watmin           elec          floor \n             0              0              0              0              0 \n         walls           roof asset_wardrobe    asset_table    asset_chair \n             0              0              0              0              0 \n    asset_khat   asset_chouki       asset_tv   asset_refrig     asset_bike \n             0              0              0              0              0 \n    asset_moto  asset_sewmach   asset_mobile \n             0              0              0 some_rows_with_missingness <- which(!complete.cases(washb_data))[31:33]\n# note: we chose 31:33 because missingness in momage & momheight is there\nwashb_data[some_rows_with_missingness, c(\"momage\", \"momheight\")]\n   momage momheight\n1:     NA     153.2\n2:     17        NA\n3:     23        NAtask$data[some_rows_with_missingness,\n          c(\"momage\", \"momheight\", \"delta_momage\", \"delta_momheight\")]\n   momage momheight delta_momage delta_momheight\n1:     23     153.2            0               1\n2:     17     150.6            1               0\n3:     23     150.6            1               0\ncolSums(is.na(task$data))\n             tr         fracode           month            aged             sex \n              0               0               0               0               0 \n         momage          momedu       momheight         hfiacat           Nlt18 \n              0               0               0               0               0 \n          Ncomp          watmin            elec           floor           walls \n              0               0               0               0               0 \n           roof  asset_wardrobe     asset_table     asset_chair      asset_khat \n              0               0               0               0               0 \n   asset_chouki        asset_tv    asset_refrig      asset_bike      asset_moto \n              0               0               0               0               0 \n  asset_sewmach    asset_mobile    delta_momage delta_momheight             whz \n              0               0               0               0               0 "},{"path":"sl3.html","id":"character-and-categorical-covariates","chapter":"6 Super Learning","heading":"6.7.2 Character and categorical covariates","text":"First character covariates converted factors. factor\ncovariates one-hot encoded, .e., levels factor become set \nbinary indicators. example, factor cats ’s one-hot encoding \nshown :second value cats “tabby” second row cats_onehot \nvalue 1 tabby. Every level cats except one represented \ncats_onehot table. first last cats “calico” first \nlast rows cats_onehot zero across columns, denote level\nappear explicitly table.learners sl3 trained object X task, sample \nX learners use CV. Let’s check first six rows task’s\nX object:can see character columns WASH Benefits dataset \nconverted factors factors (tr, momedu, hfiacat fracode)\none-hot encoded. can also see missingness indicators reviewed\nlast two columns task$X: delta_momage delta_momage.\nimputed momage momheight also task’s X object.","code":"cats <- c(\"calico\", \"tabby\", \"cow\", \"ragdoll\", \"mancoon\", \"dwarf\", \"calico\")\ncats <- factor(cats)\ncats_onehot <- factor_to_indicators(cats)\ncats_onehot\n     cow dwarf mancoon ragdoll tabby\n[1,]   0     0       0       0     0\n[2,]   0     0       0       0     1\n[3,]   1     0       0       0     0\n[4,]   0     0       0       1     0\n[5,]   0     0       1       0     0\n[6,]   0     1       0       0     0\n[7,]   0     0       0       0     0\nhead(task$X)"},{"path":"sl3.html","id":"learner-documentation","chapter":"6 Super Learning","heading":"6.8 Learner Documentation","text":"Documentation learners tuning parameters can found\nR session (e.g., see Lrnr_glmnet’s parameters, one type\n“?Lrnr_glmnet” RStudio’s R console) online sl3 Learners\nReference.\nlearners sl3 simply wrappers around existing functions \nsoftware packages R. example, sl3’s Lrnr_xgboost learner\nsl3 fitting XGBoost (eXtreme Gradient Boosting) algorithm. \ndescribed Lrnr_xgboost documentation, “learner provides fitting\nprocedures xgboost models, using xgboost package, via xgb.train”.\ngeneral, documentation sl3 learner refers reader \noriginal function package sl3 wrapped learner around. \nmind, sl3 learner documentation good first place look \nlearner, show us exactly package function learner\nbased . However, thorough investigation learner (\ndetailed explanation tuning parameters models data)\ntypically involves referencing original package. Continuing \nexample , means , information provided \nLrnr_xgboost documentation, learning Lrnr_xgboost uses \nxgboost package’s xgb.train function, deepest understanding \nXGBoost algorithm available sl3 come referencing xgboost\nR package xgb.train function.","code":""},{"path":"sl3.html","id":"naming-learners","chapter":"6 Super Learning","heading":"6.9 Naming Learners","text":"Recall Stack example long names., show different ways user name learners. first way\nname learner upon instantiation, shown :can specify name learner upon instantiating . ,\nnamed GLM learner “GLM”.Also, can specify names learners upon creation Stack:","code":"stack\n [1] \"Lrnr_glm_TRUE\"                          \n [2] \"Lrnr_mean\"                              \n [3] \"Lrnr_glmnet_NULL_deviance_10_0_100_TRUE\"\n [4] \"Lrnr_glmnet_NULL_deviance_10_1_100_TRUE\"\n [5] \"Lrnr_polspline\"                         \n [6] \"Lrnr_earth_2_3_backward_0_1_0_0\"        \n [7] \"Lrnr_hal9001_2_1_c(3, 2)_5\"             \n [8] \"Lrnr_ranger_500_TRUE_none_1\"            \n [9] \"Lrnr_xgboost_20_1\"                      \n[10] \"Lrnr_gam_NULL_NULL_GCV.Cp\"              \n[11] \"Lrnr_bayesglm_TRUE\"                     \nlrn_glm <- Lrnr_glm$new(name = \"GLM\")learners_pretty_names <- c(\n  \"GLM\" = lrn_glm, \"Mean\" = lrn_mean, \"Ridge\" = lrn_ridge, \n  \"Lasso\" = lrn_lasso, \"Polspline\" = lrn_polspline, \"Earth\" = lrn_earth, \n  \"HAL\" = lrn_hal, \"RF\" = lrn_ranger, \"XGBoost\" = lrn_xgb, \"GAM\" = lrn_gam, \n  \"BayesGLM\" = lrn_bayesglm\n)\nstack_pretty_names <- Stack$new(learners_pretty_names)\nstack_pretty_names\n [1] \"GLM\"       \"Mean\"      \"Ridge\"     \"Lasso\"     \"Polspline\" \"Earth\"    \n [7] \"HAL\"       \"RF\"        \"XGBoost\"   \"GAM\"       \"BayesGLM\" "},{"path":"sl3.html","id":"defining-learners-over-grid-of-tuning-parameters","chapter":"6 Super Learning","heading":"6.10 Defining Learners over Grid of Tuning Parameters","text":"Customized learners can created grid tuning parameters. \nhighly flexible learners require careful tuning, oftentimes\nhelpful consider different tuning parameter specifications. However,\ntime consuming, computational feasibility considered.\nAlso, effective sample size small, highly flexible learners\nlikely perform well since typically require lot data fit\nmodels. See Phillips et al. (2023) information effective sample size,\nstep--step guidelines tailoring SL specification perform well\nprediction task hand.show two ways customize learners grid tuning parameters. \nfirst, “--” approach requires user collaborator \nknowledge algorithm tuning parameters, can adequately\nspecify set tuning parameters . second approach \nrequire user specialized knowledge algorithm (although \nunderstanding still helpful); uses caret software automatically\nselect “optimal” set tuning parameters grid .","code":""},{"path":"sl3.html","id":"do-it-yourself-grid","chapter":"6 Super Learning","heading":"6.10.1 Do-it-yourself grid","text":", show can create several variations XGBoost learner,\nLrnr_xgboost, hand. example just demonstrative purposes; users\nconsult documentation, consider computational feasibility \nprediction task specify appropriate grid tuning parameters \ntask.example , considered every possible combination grid \ncreate nine XGBoost learners. wanted create custom names \nlearners well:","code":"grid_params <- list(\n  max_depth = c(3, 5, 8),\n  eta = c(0.001, 0.1, 0.3),\n  nrounds = 100\n)\ngrid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)\n\nxgb_learners <- apply(grid, MARGIN = 1, function(tuning_params) {\n  do.call(Lrnr_xgboost$new, as.list(tuning_params))\n})\nxgb_learners\n[[1]]\n[1] \"Lrnr_xgboost_100_1_3_0.001\"\n\n[[2]]\n[1] \"Lrnr_xgboost_100_1_5_0.001\"\n\n[[3]]\n[1] \"Lrnr_xgboost_100_1_8_0.001\"\n\n[[4]]\n[1] \"Lrnr_xgboost_100_1_3_0.1\"\n\n[[5]]\n[1] \"Lrnr_xgboost_100_1_5_0.1\"\n\n[[6]]\n[1] \"Lrnr_xgboost_100_1_8_0.1\"\n\n[[7]]\n[1] \"Lrnr_xgboost_100_1_3_0.3\"\n\n[[8]]\n[1] \"Lrnr_xgboost_100_1_5_0.3\"\n\n[[9]]\n[1] \"Lrnr_xgboost_100_1_8_0.3\"\nnames(xgb_learners) <- c(\n  \"XGBoost_depth3_eta.001\", \"XGBoost_depth5_eta.001\", \"XGBoost_depth8_eta.001\", \n  \"XGBoost_depth3_eta.1\", \"XGBoost_depth5_eta.1\", \"XGBoost_depth8_eta.1\", \n  \"XGBoost_depth3_eta.3\", \"XGBoost_depth5_eta.3\", \"XGBoost_depth8_eta.3\"\n)"},{"path":"sl3.html","id":"automatic-grid-and-selection-with-caret","chapter":"6 Super Learning","heading":"6.10.2 Automatic grid and selection with caret","text":"can use Lrnr_caret use caret software. described \nLrnr_caret documentation, Lrnr_caret “uses caret package’s train\nfunction automatically tune predictive model”. , instantiate \nneural network automatically tuned caret name \nlearner “NNET_autotune”.","code":"\nlrnr_nnet_autotune <- Lrnr_caret$new(method = \"nnet\", name = \"NNET_autotune\")"},{"path":"sl3.html","id":"learners-with-interactions-and-formula-interface","chapter":"6 Super Learning","heading":"6.11 Learners with Interactions and formula Interface","text":"described Phillips et al. (2023), ’s known/possible \ninteractions among covariates can include learners pick \nexplicitly (e.g., including library parametric regression learner\ninteractions specified formula) implicitly (e.g., including \nlibrary tree-based algorithms learn interactions empirically).One way define interaction terms among covariates sl3 \nformula. argument exists Lrnr_base, inherited every\nlearner sl3; even though formula explicitly appear \nlearner argument, via inheritance. implementation allows\nformula supplied learners, even without native formula\nsupport. , show specify GLM learner considers two-way\ninteractions among covariates.can see , general behavior formulain R applies \nsl3. See Details formula stats R package details \nsyntax (e.g,. RStudio, type “?formula” Console information\nappear Help tab).","code":"\nlrnr_glm_interaction <- Lrnr_glm$new(formula = \"~.^2\")"},{"path":"sl3.html","id":"covariate-screening","chapter":"6 Super Learning","heading":"6.12 Covariate Screening","text":"One characteristic rich library learners effective \nhandling covariates high dimension. many covariates \ndata relative effective sample size (see Figure 1 Flowchart \nPhillips et al. (2023)), candidate learners coupled range -called\n“screeners”. screener simply function returns subset \ncovariates. screener intended coupled candidate learner, \ndefine new candidate learner considers reduced set \nscreener-returned covariates covariates.stated Phillips et al. (2023), “covariate screening essential \ndimensionality data large, can practically useful \nSL machine learning application. Screening covariates considers\nassociations outcome must cross validated avoid biasing \nestimate algorithm’s predictive performance”. including\nscreener-learner couplings additional candidates SL library, \ncross validating screening covariates. Covariates retained CV\nfold may vary.“range screeners” set screeners exhibits varying\ndegrees dimension reduction incorporates different fitting procedures\n(e.g., lasso-based screeners retain covariates non-zero\ncoefficients, importance-based screeners retain top \\(j\\) \nimportant covariates according importance metric. current set \nscreeners available sl3 described part .see , define screener learner coupling sl3,\nneed create Pipeline. Pipeline set learners\nfit sequentially, fit one learner used define \ntask next learner.","code":""},{"path":"sl3.html","id":"variable-importance-based-screeners","chapter":"6 Super Learning","heading":"6.12.1 Variable importance-based screeners","text":"Variable importance-based screeners retain top \\(j\\) important covariates\naccording importance metric. screener provided \nLrnr_screener_importance sl3 parameter \\(j\\) (default five) \nprovided user via num_screen argument. user also gets \nchoose importance metric considered via learner argument. \nlearner importance method can used Lrnr_screener_importance;\ncurrently includes following:Let’s consider screening covariates based Lrnr_ranger variable importance\nranking selects top ten important covariates, according \nranger’s “impurity_corrected” importance. couple screener \nLrnr_glm define new learner (1) selects top ten important\ncovariates, according ranger’s “impurity_corrected” importance, \n(2) passes screener-selected covariates Lrnr_glm, Lrnr_glm\nfits model according reduced set covariates. mentioned ,\ncoupling establishes new learner requires defining Pipeline.\nPipeline sl3’s way going (1) (2).even define Pipeline entire Stack, every\nlearner fit screener-selected, reduced set ten covariates.","code":"sl3_list_learners(properties = \"importance\")\n[1] \"Lrnr_lightgbm\"     \"Lrnr_randomForest\" \"Lrnr_ranger\"      \n[4] \"Lrnr_xgboost\"     \nranger_with_importance <- Lrnr_ranger$new(importance = \"impurity_corrected\")\nRFscreen_top10 <- Lrnr_screener_importance$new(\n  learner = ranger_with_importance, num_screen = 10\n)\nRFscreen_top10_glm <- Pipeline$new(RFscreen_top10, lrn_glm)\nRFscreen_top10_stack <- Pipeline$new(RFscreen_top10, stack)"},{"path":"sl3.html","id":"coefficient-threshold-based-screeners","chapter":"6 Super Learning","heading":"6.12.2 Coefficient threshold-based screeners","text":"Lrnr_screener_coefs provides screening covariates based magnitude\nestimated coefficients (possibly regularized) GLM. \nthreshold (default = 1e-3) defines minimum absolute size \ncoefficients, thus covariates, kept. Also, max_retain argument\ncan optionally provided restrict number selected covariates \nmax_retain.Let’s consider screening covariates Lrnr_screener_coefs select \nvariables non-zero lasso regression coefficients. couple \nscreener Lrnr_glm define new learner (1) selects covariates\nnon-zero lasso regression coefficients, (2) passes \nscreener-selected covariates Lrnr_glm, Lrnr_glm fits model\naccording reduced set covariates. structure similar \n.even define Pipeline entire Stack, every\nlearner fit lasso screener-selected, reduced set covariates.","code":"\nlasso_screen <- Lrnr_screener_coefs$new(learner = lrn_lasso, threshold = 0)\nlasso_screen_glm <- Pipeline$new(lasso_screen, lrn_glm)\nlasso_screen_stack <- Pipeline$new(lasso_screen, stack)"},{"path":"sl3.html","id":"correlation-based-screeners","chapter":"6 Super Learning","heading":"6.12.3 Correlation-based screeners","text":"Lrnr_screener_correlation provides covariate screening procedures \nrunning test correlation (Pearson default), selecting (1) top\nranked variables (default), (2) variables p-value lower \nuser-specified threshold.Let’s consider screening covariates Lrnr_screener_coefs. \nillustrate set pipeline Stack, looks \nprevious examples. Pipeline single learner also looks \nprevious examples.","code":"\n# select top 10 most correlated covariates\ncorRank_screen <- Lrnr_screener_correlation$new(\n  type = \"rank\", num_screen = 10\n)\ncorRank_screen_stack <- Pipeline$new(corRank_screen, stack)\n\n# select covariates with correlation p-value below 0.05, and a minimum of 3\ncorP_screen <- Lrnr_screener_correlation$new(\n  type = \"threshold\", pvalue_threshold = 0.05, min_screen = 3\n)\ncorP_screen_stack <- Pipeline$new(corP_screen, stack)"},{"path":"sl3.html","id":"augmented-screeners","chapter":"6 Super Learning","heading":"6.12.4 Augmented screeners","text":"Augmented screeners special enforce certain covariates \nalways included. , screener removes “mandatory” covariate \nLrnr_screener_augment reincorporate learner(s) \nPipeline fit. example use screener included .\nassume aged momage covariates must kept learner\nfitting.Lrnr_screener_augment useful subject-matter experts feel strongly\ncertain covariate sets must included, even screening procedures.","code":"\nkeepme <- c(\"aged\", \"momage\")\n# using corRank_screen as an example, but any instantiated screener can be \n# supplied as screener.\ncorRank_screen_augmented <- Lrnr_screener_augment$new(\n  screener = corRank_screen, default_covariates = keepme\n)\ncorRank_screen_augmented_glm <- Pipeline$new(corRank_screen_augmented, lrn_glm)"},{"path":"sl3.html","id":"stack-with-range-of-screeners","chapter":"6 Super Learning","heading":"6.12.5 Stack with range of screeners","text":", mentioned ’d like consider range screeners \ndiversify library. show can create new Stack \nlearners stacks includes learners screening, learners coupled\nvarious screeners.screeners_stack inputted learners Lrnr_sl \ndefine SL considers candidates learners screening, \nlearners coupled various screeners.","code":"\nscreeners_stack <- Stack$new(stack, corP_screen_stack, corRank_screen_stack, \n                             lasso_screen_stack, RFscreen_top10_stack)"},{"path":"sl3.html","id":"advanced-sl3-functionality","chapter":"6 Super Learning","heading":"Advanced sl3 Functionality:","text":"","code":""},{"path":"sl3.html","id":"variable-importance-measures","chapter":"6 Super Learning","heading":"6.13 Variable Importance Measures","text":"Variable importance can interesting informative. can also \ncontradictory confusing. Nevertheless, collaborators tend like ,\ncreated function assess variable importance sl3. sl3\nimportance function returns table variables listed decreasing order\nimportance (.e., important listed first row).measure importance sl3 based ratio difference \npredictive performance SL fit removed permuted\ncovariate (covariate grouping), SL fit observed covariate (\ncovariate grouping), across . manner, larger \nratio/difference predictive performance, important covariate\n(covariate group) SL prediction.intuition measure calculates predictive risk (e.g.,\nMSE) losing one covariate (one group covariates), keeping\neverything else fixed, comparing predictive risk one \nanalytic dataset. ratio predictive risks one, difference \nzero, losing covariate (group) impact, thus \nimportant according measure. procedure repeated across \ncovariates/groups. stated , can remove covariate (\ncovariate group) refit SL without , just permute (faster) \nhope shuffling distort meaningful information present.\nidea permuting instead removing saves lot time, also\nincorporated randomForest variable importance measures. However, \npermutation approach risky. sl3 importance default remove\ncovariate refit. , use permute approach \nmuch faster.Let’s explore sl3 variable importance measurements sl_fit, \nSL fit WASH Benefits example dataset. define grouping\ncovariates consider importance evaluation based \nhousehold assets, collection variables reflects socio-economic\nstatus (SES) study’s participants.\nFIGURE 6.2: sl3 variable importance predicting weight--height z-score WASH Benefits example dataset\nAccording sl3 variable importance measures, assessed \nmean squared error (MSE) difference permutations covariate, \nfitted SL’s (sl_fit) important variables predicting weight--height\nz-score (whz) child age (aged) household assets (assets) \nreflect socio-economic status study’s subjects.","code":"\nassets <- c(\"asset_wardrobe\", \"asset_table\", \"asset_chair\", \"asset_khat\",\n            \"asset_chouki\", \"asset_tv\", \"asset_refrig\", \"asset_bike\", \n            \"asset_moto\", \"asset_sewmach\", \"asset_mobile\", \"Nlt18\", \"Ncomp\", \n            \"watmin\", \"elec\", \"floor\", \"walls\", \"roof\")\nset.seed(983)\nwashb_varimp <- importance(\n  fit = sl_fit, eval_fun = loss_squared_error, type = \"permute\", \n  covariate_groups = list(\"assets\" = assets)\n)\nwashb_varimp\n# plot variable importance\nimportance_plot(x = washb_varimp)"},{"path":"sl3.html","id":"conditional-density-estimation","chapter":"6 Super Learning","heading":"6.14 Conditional Density Estimation","text":"certain scenarios may useful estimate conditional density \ndependent variable, given predictors/covariates precede . context\ncausal inference, arises readily working \ncontinuous-valued treatments. Specifically, conditional density estimation (CDE)\nnecessary estimating treatment mechanism continuous-valued\ntreatment, often called generalized propensity score. Compared \nclassical propensity score (PS) binary treatments (conditional\nprobability receiving treatment given covariates), \\(\\mathbb{P}(= 1 \\mid W)\\), generalized PS conditional density treatment \\(\\), given\ncovariates \\(W\\), \\(\\mathbb{P}(\\mid W)\\).CDE often requires specialized approaches tied specific algorithmic\nimplementations. knowledge, general flexible algorithms CDE \nproposed sparsely literature. implemented two \napproaches sl3: semiparametric CDE approach makes certain\nassumptions constancy (higher) moments underlying\ndistribution, second approach exploits relationship \nconditional hazard density functions allow CDE via pooled hazard\nregression. approaches flexible allow use arbitrary\nregression functions machine learning algorithms estimation \nnuisance quantities (conditional mean conditional hazard,\nrespectively). elaborate two frameworks . Importantly, per\nDudoit van der Laan (2005) related works, loss function appropriate \ndensity estimation negative log-density loss \\(L(\\cdot) = -\\log(p_n(\\cdot))\\).","code":""},{"path":"sl3.html","id":"moment-restricted-location-scale","chapter":"6 Super Learning","heading":"6.14.1 Moment-restricted location-scale","text":"family semiparametric CDE approaches exploits general form \\(\\rho(Y - \\mu(X) / \\sigma(X))\\), \\(Y\\) dependent variable interest (e.g.,\ntreatment \\(\\) PS), \\(X\\) predictors (e.g., covariates \\(W\\) \nPS), \\(\\rho\\) specified marginal density function, \\(\\mu(X) = \\E(Y \\mid X)\\) \\(\\sigma(X) = \\E[(Y - \\mu(X))^2 \\mid X]\\) nuisance functions \ndependent variable may estimated flexibly. CDE procedures formulated\nwithin framework may characterized belonging conditional\nlocation-scale family, , \\(p_n(Y \\mid X) = \\rho((Y - \\mu_n(X)) / \\sigma_n(X))\\). CDE conditional location-scale families without\npotential disadvantages (e.g., restriction density’s functional form\nlead misspecification bias), strategy flexible \nallows arbitrary machine learning algorithms used estimating \nconditional mean \\(Y\\) given \\(X\\), \\(\\mu(X) = \\E(Y \\mid X)\\), conditional\nvariance \\(Y\\) given \\(X\\), \\(\\sigma(X) = \\E[(Y - \\mu(X))^2 \\mid X]\\).settings limited data, additional structure imposed \nassumption target density belongs location-scale family may prove\nadvantageous smoothing areas low support data. However, \npractice, impossible know whether assumption holds. \nprocedure novel contribution (unable \nlocate formal description literature); nevertheless, provide\ninformal algorithm sketch . algorithm considers access \\(n\\)\nindependent identically distributed (..d.) copies observed data\nrandom variable \\(O = (Y, X)\\), priori-specified kernel function \\(\\rho\\), \ncandidate regression procedure \\(f_{\\mu}\\) estimate \\(\\mu(X)\\), candidate\nregression procedure \\(f_{\\sigma}\\) estimate \\(\\sigma(X)\\).Estimate \\(\\mu(X) = \\E[Y \\mid X]\\), conditional mean \\(Y\\) given \\(X\\), \napplying regression estimator \\(f_{\\mu}\\), yielding \\(\\hat{\\mu}(X)\\).Estimate \\(\\sigma(X) = \\mathbb{V}[Y \\mid X]\\), conditional variance \\(Y\\)\ngiven \\(X\\), applying regression estimator \\(f_{\\sigma}\\), yielding\n\\(\\hat{\\sigma}^2(X)\\). Note step involves estimation \nconditional mean \\(\\E[(Y - \\hat{\\mu}(X))^2 \\mid X]\\).Estimate one-dimensional density \\((Y - \\hat{\\mu}(X))^2 / \\hat{\\sigma}^2(X)\\), using kernel smoothing obtain \\(\\hat{\\rho}(Y)\\).Construct estimated conditional density \\(p_n(Y \\mid X) = \\hat{\\rho}((Y - \\hat{\\mu}(X)) / \\hat{\\sigma}(X))\\).algorithm sketch encompasses two forms CDE approach, diverge\nsecond step . simplify approach, one may elect estimate\nconditional mean \\(\\mu(X)\\), leaving conditional variance \nassumed constant (.e., estimated simply marginal mean residuals\n\\(\\E[(Y - \\hat{\\mu}(X))^2]\\)). subclass CDE approaches homoscedastic\nerror based variance assumption made. conditional variance can\ninstead estimated conditional mean residuals \\((Y - \\hat{\\mu}(X))^2\\) given \\(X\\), \\(\\E[(Y - \\hat{\\mu}(X))^2 \\mid X]\\), \ncandidate algorithm \\(f_{\\sigma}\\) used evaluate expectation. \napproaches implemented sl3, learner\nLrnr_density_semiparametric. mean_learner argument specifies \\(f_{\\mu}\\)\noptional var_learner argument specifies \\(f_{\\sigma}\\). demonstrate\nCDE approach .","code":"# semiparametric density estimator with homoscedastic errors (HOSE)\nhose_hal_lrnr <- Lrnr_density_semiparametric$new(\n  mean_learner = Lrnr_hal9001$new()\n)\n# semiparametric density estimator with heteroscedastic errors (HESE)\nhese_rf_glm_lrnr <- Lrnr_density_semiparametric$new(\n  mean_learner = Lrnr_ranger$new()\n  var_learner = Lrnr_glm$new()\n)\n\n# SL for the conditional treatment density\nsl_dens_lrnr <- Lrnr_sl$new(\n  learners = list(hose_hal_lrnr, hese_rf_glm_lrnr),\n  metalearner = Lrnr_solnp_density$new()\n)"},{"path":"sl3.html","id":"pooled-hazard-regression","chapter":"6 Super Learning","heading":"6.14.2 Pooled hazard regression","text":"Another approach CDE available sl3, originally proposed \nDı́az van der Laan (2011), leverages relationship (conditional) hazard \ndensity functions. develop CDE framework, Dı́az van der Laan (2011) proposed\ndiscretizing continuous dependent variable \\(Y\\) support \\(\\mathcal{Y}\\)\nbased number bins \\(T\\) binning procedure (e.g., cutting\n\\(\\mathcal{Y}\\) \\(T\\) bins exactly length). tuning parameter\n\\(T\\) conceptually corresponds choice bandwidth classical kernel\ndensity estimation. Following discretization, unit represented \ncollection records, number records representing given unit\ndepends rank bin (along discretized support) \nunit falls.take example, instantiation procedure might divide support\n\\(Y\\) , say, \\(T = 4\\), bins equal length (note requires \\(T+1\\) cut\npoints): \\([\\alpha_1, \\alpha_2), [\\alpha_2, \\alpha_3), [\\alpha_3, \\alpha_4), [\\alpha_4, \\alpha_5]\\) (n.b., rightmost interval fully closed \nothers partially closed). Next, artificial, repeated measures\ndataset created unit represented \\(T\\)\nrecords. better see structure, consider individual unit \\(O_i = (Y_i, X_i)\\) whose \\(Y_i\\) value within \\([\\alpha_3, \\alpha_4)\\), third bin. \nunit represented three distinct records: \\(\\{Y_{ij}, X_{ij}\\}_{j=1}^3\\), \\(\\{\\{Y_{ij} = 0\\}_{j=1}^2\\), \\(Y_{i3} = 1\\}\\) three\nexact copies \\(X_i\\), \\(\\{X_{ij}\\}_{j=1}^3\\). representation terms \nmultiple records unit allows conditional hazard probability\n\\(Y_i\\) falling given bin along discretized support evaluated\nvia standard binary regression techniques.fact, proposal reformulates binary regression problem \ncorresponding set hazard regressions: \\(\\mathbb{P} (Y \\[\\alpha_{t-1}, \\alpha_t) \\mid X) = \\mathbb{P} (Y \\[\\alpha_{t-1}, \\alpha_t) \\mid Y \\geq \\alpha_{t-1}, X) \\times \\prod_{j = 1}^{t -1} \\{1 - \\mathbb{P} (Y \\[\\alpha_{j-1}, \\alpha_j) \\mid Y \\geq \\alpha_{j-1}, X) \\}\\). , probability\n\\(Y \\\\mathcal{Y}\\) falling bin \\([\\alpha_{t-1}, \\alpha_t)\\) may directly\nestimated via binary regression procedure, re-expressing corresponding\nlikelihood terms likelihood binary variable dataset \nrepeated measures structure. Finally, hazard estimates can mapped \ndensity estimates re-scaling hazard estimates bin sizes \\(\\lvert \\alpha_t - \\alpha_{t-1} \\rvert\\), , \\(p_{n, \\alpha}(Y \\mid X) = \\mathbb{P}(Y \\[\\alpha_{t-1}, \\alpha_t) \\mid X) / \\lvert \\alpha_t - \\alpha_{t-1} \\rvert\\), \\(\\alpha_{t-1} \\leq < \\alpha_t\\). provide \ninformal sketch algorithm .Apply procedure divide observed support \\(Y\\), \\(\\max(Y) - \\min(Y)\\),\n\\(T\\) bins: \\([\\alpha_1, \\alpha_2), \\ldots, [\\alpha_{t-1}, \\alpha_t), [\\alpha_t, \\alpha_{t+1}]\\).Expand observed data repeated measures data structure, expressing\nindividual observation set \\(T\\) records, recording \nobservation ID alongside record. single unit \\(\\), set \nrecords takes form \\(\\{Y_{ij}, X_{ij}\\}_{j=1}^{T_i}\\), \\(X_{ij}\\) \nconstant index set \\(\\mathcal{J}\\), \\(Y_{ij}\\) binary counting\nprocess jumps \\(0\\) \\(1\\) final index (bin \n\\(Y_i\\) falls), \\(T_i \\leq T\\) indicates bin along support \n\\(Y_i\\) falls.Estimate hazard probability, conditional \\(X\\), bin membership\n\\(\\mathbb{P}(Y_i \\[\\alpha_{t-1}, \\alpha_t) \\mid X)\\) using binary\nregression estimator appropriate machine learning algorithm.Rescale conditional hazard probability estimates conditional\ndensity scale dividing cumulative hazard width bin \n\\(X_i\\) falls, observation \\(= 1, \\ldots, n\\). support\nset partitioned bins equal size (approximately \\(n/T\\) samples \nbin), amounts rescaling constant. support set \npartitioned bins equal range, rescaling might vary across\nbins.key element proposal flexibility use binary regression\nprocedure appropriate machine learning algorithm estimate \\(\\mathbb{P}(Y \\[\\alpha_{t-1}, \\alpha_t) \\mid X)\\), facilitating incorporation \nflexibletechniques like ensemble learning (Breiman, 1996; van der Laan et al., 2007).\nextreme degree flexibility integrates perfectly underlying\ndesign principles sl3; however, yet implemented approach\nfull generality. version CDE approach, limits \noriginal proposal replacing use arbitrary binary regression \nhighly adaptive lasso (HAL) algorithm (Benkeser van der Laan, 2016) supported \nhaldensify package\n(Hejazi, Benkeser, et al., 2022) (HAL implementation haldensify provided \nhal9001 package (Coyle et al., 2022; Hejazi, Coyle, et al., 2020)). CDE algorithm uses haldensify incorporated\nlearner Lrnr_haldensify sl3, demonstrate .","code":"\n# learners used for conditional densities for (g_n)\nhaldensify_lrnr <- Lrnr_haldensify$new(\n  n_bins = c(5, 10)\n)"},{"path":"sl3.html","id":"exercises-1","chapter":"6 Super Learning","heading":"Exercises:","text":"","code":""},{"path":"sl3.html","id":"binary-outcome-prediction","chapter":"6 Super Learning","heading":"Binary outcome prediction","text":"Follow steps predict probability myocardial infarction\n(mi) using available covariate data. thank Dr. David Benkeser,\nAssistant Professor Biostatistics Bioinformatics Emory University, \nmaking Cardiovascular Health Study (CHS) publicly data available.Let’s take quick peek data:Create sl3 task, setting myocardial infarction mi outcome \nusing available covariate data.Make library seven relatively fast base learning algorithms (.e., \nconsider BART HAL). Customize tuning parameters one \nlearners. Incorporate least one screener-learner coupling.Make SL train task.Print SL fit results adding $cv_risk(loss_squared_error) \nfit object.","code":"\n# load the data set\nlibrary(readr)\ndb_data <- url(\n  paste0(\n    \"https://raw.githubusercontent.com/benkeser/sllecture/master/\",\n    \"chspred.csv\"\n  )\n)\nchspred <- read_csv(file = db_data, col_names = TRUE)\nhead(chspred)"},{"path":"sl3.html","id":"concluding-remarks","chapter":"6 Super Learning","heading":"6.15 Concluding Remarks","text":"Super Learner (SL) general approach can applied diversity \nestimation prediction problems can defined loss function.Super Learner (SL) general approach can applied diversity \nestimation prediction problems can defined loss function.straightforward plug estimator returned SL \ntarget parameter mapping.\nexample, suppose average treatment effect (ATE) \nbinary treatment intervention:\n\\(\\Psi_0 = E_{0,W}[E_0(Y|=1,W) - E_0(Y|=0,W)]\\).\nuse SL trained original data (let’s call\nsl_fit) predict outcome subjects \nintervention. need take average difference\ncounterfactual outcomes intervention interest.\nConsidering \\(\\Psi_0\\) , first need two \\(n\\)-length vectors \npredicted outcomes intervention. One vector represent\npredicted outcomes intervention sets subjects \nreceive \\(=1\\), \\(Y_i|A_i=1,W_i\\) \\(=1,\\ldots,n\\). vector\nrepresent predicted outcomes intervention sets\nsubjects receive \\(=0\\), \\(Y_i|A_i=0,W_i\\) \\(=1,\\ldots,n\\).\nobtaining vectors counterfactual predicted outcomes, \nneed average take difference order \n“plug-” SL estimator target parameter mapping.\nsl3 current ATE example, achieved \nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task));\nA1_task$data contain 1’s (level pertains \nreceiving treatment) treatment column data (keeping\nelse ), A0_task$data contain 0’s (\nlevel pertains receiving treatment) treatment\ncolumn data.\nstraightforward plug estimator returned SL \ntarget parameter mapping.example, suppose average treatment effect (ATE) \nbinary treatment intervention:\n\\(\\Psi_0 = E_{0,W}[E_0(Y|=1,W) - E_0(Y|=0,W)]\\).use SL trained original data (let’s call\nsl_fit) predict outcome subjects \nintervention. need take average difference\ncounterfactual outcomes intervention interest.Considering \\(\\Psi_0\\) , first need two \\(n\\)-length vectors \npredicted outcomes intervention. One vector represent\npredicted outcomes intervention sets subjects \nreceive \\(=1\\), \\(Y_i|A_i=1,W_i\\) \\(=1,\\ldots,n\\). vector\nrepresent predicted outcomes intervention sets\nsubjects receive \\(=0\\), \\(Y_i|A_i=0,W_i\\) \\(=1,\\ldots,n\\).obtaining vectors counterfactual predicted outcomes, \nneed average take difference order \n“plug-” SL estimator target parameter mapping.sl3 current ATE example, achieved \nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task));\nA1_task$data contain 1’s (level pertains \nreceiving treatment) treatment column data (keeping\nelse ), A0_task$data contain 0’s (\nlevel pertains receiving treatment) treatment\ncolumn data.’s worthwhile exercise obtain predicted counterfactual outcomes\ncreate counterfactual sl3 tasks. ’s biased, however, \nplug SL fit target parameter mapping, (e.g., calling result\nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task)) \nestimated ATE. end estimator ATE \noptimized estimation prediction function, ATE!’s worthwhile exercise obtain predicted counterfactual outcomes\ncreate counterfactual sl3 tasks. ’s biased, however, \nplug SL fit target parameter mapping, (e.g., calling result\nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task)) \nestimated ATE. end estimator ATE \noptimized estimation prediction function, ATE!Ultimately, want estimator best job approximating\nquestion interest. , care best job\npossible estimating \\(\\psi_0\\). SL essential step help us get\n: counterfactual predicted outcome estimates (like \nexplained ), SL-derived estimates (like propensity score)\nplay key role estimating \\(\\psi_0\\). However, SL end \nestimation procedure. Specifically, simply plugged SL estimates\ntarget parameter, asymptotically linear\nestimator target estimand; SL efficient substitution\nestimator admit statistical inference. matter?\nasymptotically linear estimator one converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference,\n.e., confidence intervals \\(p\\)-values, (see Chapters 4–6 \nvan der Laan Rose (2011)).\nSubstitution, plug-, estimators desirable respect\nlocal global constraints statistical model, \nbounds outcome, better finite-sample properties\n(see Chapter 6 van der Laan Rose (2011)).\nefficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient (see Chapter 6 van der Laan Rose (2011)).\ncanonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem (Chapter 5 van der Laan Rose (2011)).\nVarious canonical gradients shown chapters follow.\nPractitioners need know calculate canonical\ngradient explain properties desirable \nestimator possess (like substitution/plug-, admits valid inference,\nefficient, ability optimize finite sample performance).\nproperties motivate use TMLE, since TMLE satisfies .\n\nUltimately, want estimator best job approximating\nquestion interest. , care best job\npossible estimating \\(\\psi_0\\). SL essential step help us get\n: counterfactual predicted outcome estimates (like \nexplained ), SL-derived estimates (like propensity score)\nplay key role estimating \\(\\psi_0\\). However, SL end \nestimation procedure. Specifically, simply plugged SL estimates\ntarget parameter, asymptotically linear\nestimator target estimand; SL efficient substitution\nestimator admit statistical inference. matter?asymptotically linear estimator one converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference,\n.e., confidence intervals \\(p\\)-values, (see Chapters 4–6 \nvan der Laan Rose (2011)).asymptotically linear estimator one converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference,\n.e., confidence intervals \\(p\\)-values, (see Chapters 4–6 \nvan der Laan Rose (2011)).Substitution, plug-, estimators desirable respect\nlocal global constraints statistical model, \nbounds outcome, better finite-sample properties\n(see Chapter 6 van der Laan Rose (2011)).Substitution, plug-, estimators desirable respect\nlocal global constraints statistical model, \nbounds outcome, better finite-sample properties\n(see Chapter 6 van der Laan Rose (2011)).efficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient (see Chapter 6 van der Laan Rose (2011)).\ncanonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem (Chapter 5 van der Laan Rose (2011)).\nVarious canonical gradients shown chapters follow.\nPractitioners need know calculate canonical\ngradient explain properties desirable \nestimator possess (like substitution/plug-, admits valid inference,\nefficient, ability optimize finite sample performance).\nproperties motivate use TMLE, since TMLE satisfies .\nefficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient (see Chapter 6 van der Laan Rose (2011)).canonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem (Chapter 5 van der Laan Rose (2011)).\nVarious canonical gradients shown chapters follow.Practitioners need know calculate canonical\ngradient explain properties desirable \nestimator possess (like substitution/plug-, admits valid inference,\nefficient, ability optimize finite sample performance).\nproperties motivate use TMLE, since TMLE satisfies .TMLE general strategy succeeds constructing efficient \nasymptotically linear plug-estimators robust finite samples.TMLE general strategy succeeds constructing efficient \nasymptotically linear plug-estimators robust finite samples.SL fantastic pure prediction, obtaining initial\nestimates components likelihood (first step TMLE), \nneed second, targeting/updating/fluctuation, step desirable\nstatistical properties mentioned .SL fantastic pure prediction, obtaining initial\nestimates components likelihood (first step TMLE), \nneed second, targeting/updating/fluctuation, step desirable\nstatistical properties mentioned .chapters follow, focus various targeted maximum likelihood\nestimator targeted minimum loss-based estimator, referred \nTMLE.chapters follow, focus various targeted maximum likelihood\nestimator targeted minimum loss-based estimator, referred \nTMLE.","code":""},{"path":"sl3.html","id":"appendix","chapter":"6 Super Learning","heading":"6.16 Appendix","text":"","code":""},{"path":"sl3.html","id":"sl3ex1-sol","chapter":"6 Super Learning","heading":"6.16.1 Exercise 1 Solution","text":"potential solution sl3 Exercise 1 – Predicting Myocardial\nInfarction \nsl3.","code":"\ndb_data <- url(\n  \"https://raw.githubusercontent.com/benkeser/sllecture/master/chspred.csv\"\n)\nchspred <- read_csv(file = db_data, col_names = TRUE)\ndata.table::setDT(chspred)\n\n# make task\nchspred_task <- make_sl3_Task(\n  data = chspred,\n  covariates = colnames(chspred)[-1],\n  outcome = \"mi\"\n)\n\n# make learners\nglm_learner <- Lrnr_glm$new()\nlasso_learner <- Lrnr_glmnet$new(alpha = 1)\nridge_learner <- Lrnr_glmnet$new(alpha = 0)\nenet_learner <- Lrnr_glmnet$new(alpha = 0.5)\n# curated_glm_learner uses formula = \"mi ~ smoke + beta\"\ncurated_glm_learner <- Lrnr_glm_fast$new(covariates = c(\"smoke\", \"beta\"))\nmean_learner <- Lrnr_mean$new() # That is one mean learner!\nglm_fast_learner <- Lrnr_glm_fast$new()\nranger_learner <- Lrnr_ranger$new()\nsvm_learner <- Lrnr_svm$new()\nxgb_learner <- Lrnr_xgboost$new()\n\n# screening\nscreen_cor <- make_learner(Lrnr_screener_correlation)\nglm_pipeline <- make_learner(Pipeline, screen_cor, glm_learner)\n\n# stack learners together\nstack <- make_learner(\n  Stack,\n  glm_pipeline, glm_learner,\n  lasso_learner, ridge_learner, enet_learner,\n  curated_glm_learner, mean_learner, glm_fast_learner,\n  ranger_learner, svm_learner, xgb_learner\n)\n\n# make and train SL\nsl <- Lrnr_sl$new(\n  learners = stack\n)\nsl_fit <- sl$train(chspred_task)\nsl_fit$cv_risk(loss_squared_error)"},{"path":"tmle3.html","id":"tmle3","chapter":"7 The TMLE Framework","heading":"7 The TMLE Framework","text":"Jeremy CoyleBased tmle3 R package.","code":""},{"path":"tmle3.html","id":"learn-tmle","chapter":"7 The TMLE Framework","heading":"7.1 Learning Objectives","text":"end chapter, able toUnderstand use TMLE effect estimation.Use tmle3 estimate Average Treatment Effect (ATE).Understand use tmle3 “Specs” objects.Fit tmle3 custom set target parameters.Use delta method estimate transformations target parameters.","code":""},{"path":"tmle3.html","id":"tmle-intro","chapter":"7 The TMLE Framework","heading":"7.2 Introduction","text":"previous chapter sl3 learned estimate regression\nfunction like \\(\\mathbb{E}[Y \\mid X]\\) data. ’s important first step\nlearning data, can use predictive model estimate\nstatistical causal effects?Going back roadmap targeted learning, suppose ’d like \nestimate effect treatment variable \\(\\) outcome \\(Y\\). discussed,\none potential parameter characterizes effect Average Treatment\nEffect (ATE), defined \\(\\psi_0 = \\mathbb{E}_W[\\mathbb{E}[Y \\mid =1,W] - \\mathbb{E}[Y \\mid =0,W]]\\) interpreted difference mean outcome\ntreatment \\(=1\\) \\(=0\\), averaging distribution \ncovariates \\(W\\). ’ll illustrate several potential estimators \nparameter, motivate use TMLE (targeted maximum likelihood\nestimation; targeted minimum loss-based estimation) framework, using \nfollowing example data:small ticks right indicate mean outcomes (averaging \\(W\\))\n\\(=1\\) \\(=0\\) respectively, difference quantity ’d\nlike estimate.hope motivate application TMLE chapter, refer \ninterested reader two Targeted Learning books associated works \nfull technical details.","code":""},{"path":"tmle3.html","id":"substitution-est","chapter":"7 The TMLE Framework","heading":"7.3 Substitution Estimators","text":"can use sl3 fit Super Learner regression model estimate\noutcome regression function \\(\\mathbb{E}_0[Y \\mid ,W]\\), often refer\n\\(\\overline{Q}_0(,W)\\) whose estimate denote \\(\\overline{Q}_n(,W)\\).\nconstruct estimate ATE \\(\\psi_n\\), need “plug-” \nestimates \\(\\overline{Q}_n(,W)\\), evaluated two intervention contrasts,\ncorresponding ATE “plug-” formula:\n\\(\\psi_n = \\frac{1}{n}\\sum(\\overline{Q}_n(1,W)-\\overline{Q}_n(0,W))\\). kind\nestimator called plug-substitution estimator, since accurate\nestimates \\(\\psi_n\\) parameter \\(\\psi_0\\) may obtained substituting\nestimates \\(\\overline{Q}_n(,W)\\) relevant regression functions\n\\(\\overline{Q}_0(,W)\\) .Applying sl3 estimate outcome regression example, can see\nensemble machine learning predictions fit data quite well:solid lines indicate sl3 estimate regression function, \ndotted lines indicating tmle3 updates (described ).substitution estimators intuitive, naively using approach \nSuper Learner estimate \\(\\overline{Q}_0(,W)\\) several limitations. First,\nSuper Learner selecting learner weights minimize risk across entire\nregression function, instead “targeting” ATE parameter hope \nestimate, leading biased estimation. , sl3 trying well \nfull regression curve left, instead focusing small ticks \nright. ’s , sampling distribution approach \nasymptotically linear, therefore inference possible.can see limitations illustrated estimates generated \nexample data:see Super Learner, estimates true parameter value (indicated \ndashed vertical line) accurately GLM. However, still less\naccurate TMLE, valid inference possible. contrast, TMLE\nachieves less biased estimator valid inference.","code":""},{"path":"tmle3.html","id":"tmle","chapter":"7 The TMLE Framework","heading":"7.4 Targeted Maximum Likelihood Estimation","text":"TMLE takes initial estimate \\(\\overline{Q}_n(,W)\\) well estimate \npropensity score \\(g_n(\\mid W) = \\mathbb{P}(= 1 \\mid W)\\) produces \nupdated estimate \\(\\overline{Q}^{\\star}_n(,W)\\) “targeted” \nparameter interest. TMLE keeps benefits substitution estimators (\none), augments original, potentially erratic estimates correct \nbias also resulting asymptotically linear (thus normally\ndistributed) estimator accommodates inference via asymptotically consistent\nWald-style confidence intervals.","code":""},{"path":"tmle3.html","id":"tmle-updates","chapter":"7 The TMLE Framework","heading":"7.4.1 TMLE Updates","text":"different types TMLEs (, sometimes, multiple set \ntarget parameters) – , give example algorithm TML\nestimation ATE. \\(\\overline{Q}^{\\star}_n(,W)\\) TMLE-augmented\nestimate \\(f(\\overline{Q}^{\\star}_n(,W)) = f(\\overline{Q}_n(,W)) + \\epsilon \\cdot H_n(,W)\\), \\(f(\\cdot)\\) appropriate link function (e.g.,\n\\(\\text{logit}(x) = \\log(x / (1 - x))\\)), estimate \\(\\epsilon_n\\) \ncoefficient \\(\\epsilon\\) “clever covariate” \\(H_n(,W)\\) computed. \nform covariate \\(H_n(,W)\\) differs across target parameters; case\nATE, \\(H_n(,W) = \\frac{}{g_n(\\mid W)} - \\frac{1-}{1-g_n(, W)}\\), \\(g_n(,W) = \\mathbb{P}(=1 \\mid W)\\) estimated propensity\nscore, estimator depends initial fit (sl3) \noutcome regression (\\(\\overline{Q}_n\\)) propensity score (\\(g_n\\)).several robust augmentations used across tlverse,\nincluding use additional layer cross-validation avoid\n-fitting bias (.e., CV-TMLE) well approaches consistently\nestimating several parameters simultaneously (e.g., points survival\ncurve).","code":""},{"path":"tmle3.html","id":"tmle-infer","chapter":"7 The TMLE Framework","heading":"7.4.2 Statistical Inference","text":"Since TMLE yields asymptotically linear estimator, obtaining statistical\ninference convenient. TML estimator corresponding\n(efficient) influence function (often, “EIF”, short) describes \nasymptotic distribution estimator. using estimated EIF, Wald-style\ninference (asymptotically correct confidence intervals) can constructed\nsimply plugging form EIF initial estimates\n\\(\\overline{Q}_n\\) \\(g_n\\), computing sample standard error.following sections describe simple detailed way \nspecifying estimating TMLE tlverse. designing tmle3, \nsought replicate closely possible general estimation framework\nTMLE, theoretical object relevant TMLE encoded \ncorresponding software object/method. First, present simple\napplication tmle3 WASH Benefits example, go describe\nunderlying objects greater detail.","code":""},{"path":"tmle3.html","id":"easy-bake-example-tmle3-for-ate","chapter":"7 The TMLE Framework","heading":"7.5 Easy-Bake Example: tmle3 for ATE","text":"’ll illustrate basic use TMLE using WASH Benefits data\nintroduced earlier estimating average treatment effect.","code":""},{"path":"tmle3.html","id":"load-the-data-1","chapter":"7 The TMLE Framework","heading":"7.5.1 Load the Data","text":"’ll use WASH Benefits data earlier chapters:","code":"\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(tmle3)\nlibrary(sl3)\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)"},{"path":"tmle3.html","id":"define-the-variable-roles","chapter":"7 The TMLE Framework","heading":"7.5.2 Define the variable roles","text":"’ll use common \\(W\\) (covariates), \\(\\) (treatment/intervention), \\(Y\\)\n(outcome) data structure. tmle3 needs know variables dataset\ncorrespond roles. use list character vectors tell\n. call “Node List” corresponds nodes Directed\nAcyclic Graph (DAG), way displaying causal relationships variables.","code":"\nnode_list <- list(\n  W = c(\n    \"month\", \"aged\", \"sex\", \"momage\", \"momedu\",\n    \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\",\n    \"elec\", \"floor\", \"walls\", \"roof\", \"asset_wardrobe\",\n    \"asset_table\", \"asset_chair\", \"asset_khat\",\n    \"asset_chouki\", \"asset_tv\", \"asset_refrig\",\n    \"asset_bike\", \"asset_moto\", \"asset_sewmach\",\n    \"asset_mobile\"\n  ),\n  A = \"tr\",\n  Y = \"whz\"\n)"},{"path":"tmle3.html","id":"handle-missingness","chapter":"7 The TMLE Framework","heading":"7.5.3 Handle Missingness","text":"Currently, missingness tmle3 handled fairly simple way:Missing covariates median- (continuous) mode- (discrete)\nimputed, additional covariates indicating imputation generated, just\ndescribed sl3 chapter.Missing treatment variables excluded – observations dropped.Missing outcomes efficiently handled automatic calculation (\nincorporation estimators) inverse probability censoring weights\n(IPCW); also known IPCW-TMLE may thought joint\nintervention remove missingness analogous procedure used \nclassical inverse probability weighted estimators.steps implemented process_missing function tmle3:","code":"\nprocessed <- process_missing(washb_data, node_list)\nwashb_data <- processed$data\nnode_list <- processed$node_list"},{"path":"tmle3.html","id":"create-a-spec-object","chapter":"7 The TMLE Framework","heading":"7.5.4 Create a “Spec” Object","text":"tmle3 general, allows components TMLE procedure \nspecified modular way. However, users interested \nmanually specifying components. Therefore, tmle3 implements \ntmle3_Spec object bundles set components specification\n(“Spec”) , minimal additional detail, can run fit TMLE.’ll start using one specs, work way \ninternals tmle3.","code":"\nate_spec <- tmle_ATE(\n  treatment_level = \"Nutrition + WSH\",\n  control_level = \"Control\"\n)"},{"path":"tmle3.html","id":"define-the-learners","chapter":"7 The TMLE Framework","heading":"7.5.5 Define the learners","text":"Currently, thing user must define sl3 learners used\nestimate relevant factors likelihood: Q g.takes form list sl3 learners, one likelihood factor\nestimated sl3:, use Super Learner defined previous chapter. future,\nplan include reasonable defaults learners.","code":"\n# choose base learners\nlrnr_mean <- make_learner(Lrnr_mean)\nlrnr_rf <- make_learner(Lrnr_ranger)\n\n# define metalearners appropriate to data types\nls_metalearner <- make_learner(Lrnr_nnls)\nmn_metalearner <- make_learner(\n  Lrnr_solnp, metalearner_linear_multinomial,\n  loss_loglik_multinomial\n)\nsl_Y <- Lrnr_sl$new(\n  learners = list(lrnr_mean, lrnr_rf),\n  metalearner = ls_metalearner\n)\nsl_A <- Lrnr_sl$new(\n  learners = list(lrnr_mean, lrnr_rf),\n  metalearner = mn_metalearner\n)\nlearner_list <- list(A = sl_A, Y = sl_Y)"},{"path":"tmle3.html","id":"fit-the-tmle","chapter":"7 The TMLE Framework","heading":"7.5.6 Fit the TMLE","text":"now everything need fit tmle using tmle3:","code":"tmle_fit <- tmle3(ate_spec, washb_data, node_list, learner_list)\nprint(tmle_fit)\nA tmle3_Fit that took 1 step(s)\n   type                                    param  init_est tmle_est      se\n1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.005233 0.007111 0.05025\n      lower  upper psi_transformed lower_transformed upper_transformed\n1: -0.09139 0.1056        0.007111          -0.09139            0.1056"},{"path":"tmle3.html","id":"evaluate-the-estimates","chapter":"7 The TMLE Framework","heading":"7.5.7 Evaluate the Estimates","text":"can see summary results printing fit object. Alternatively, \ncan extra results summary indexing :","code":"estimates <- tmle_fit$summary$psi_transformed\nprint(estimates)\n[1] 0.007111"},{"path":"tmle3.html","id":"tmle3-components","chapter":"7 The TMLE Framework","heading":"7.6 tmle3 Components","text":"Now ’ve successfully used spec obtain TML estimate, let’s look\nhood components. spec number functions \ngenerate objects necessary define fit TMLE.","code":""},{"path":"tmle3.html","id":"tmle3_task","chapter":"7 The TMLE Framework","heading":"7.6.1 tmle3_task","text":"First , tmle3_Task, analogous sl3_Task, containing data ’re\nfitting TMLE , well NPSEM generated node_list\ndefined , describing variables relationships.","code":"\ntmle_task <- ate_spec$make_tmle_task(washb_data, node_list)tmle_task$npsem\n$W\ntmle3_Node: W\n    Variables: month, aged, sex, momedu, hfiacat, Nlt18, Ncomp, watmin, elec, floor, walls, roof, asset_wardrobe, asset_table, asset_chair, asset_khat, asset_chouki, asset_tv, asset_refrig, asset_bike, asset_moto, asset_sewmach, asset_mobile, momage, momheight, delta_momage, delta_momheight\n    Parents: \n\n$A\ntmle3_Node: A\n    Variables: tr\n    Parents: W\n\n$Y\ntmle3_Node: Y\n    Variables: whz\n    Parents: A, W"},{"path":"tmle3.html","id":"initial-likelihood","chapter":"7 The TMLE Framework","heading":"7.6.2 Initial Likelihood","text":"Next, object representing likelihood, factorized according \nNPSEM described :components likelihood indicate factors estimated: \nmarginal distribution \\(W\\) estimated using NP-MLE, conditional\ndistributions \\(\\) \\(Y\\) estimated using sl3 fits (defined \nlearner_list) .can use tandem tmle_task object obtain likelihood\nestimates observation:","code":"\ninitial_likelihood <- ate_spec$make_initial_likelihood(\n  tmle_task,\n  learner_list\n)\nprint(initial_likelihood)\nW: Lf_emp\nA: LF_fit\nY: LF_fitinitial_likelihood$get_likelihoods(tmle_task)\n             W      A       Y\n   1: 0.000213 0.3302 -0.3550\n   2: 0.000213 0.3398 -0.9297\n   3: 0.000213 0.3287 -0.8058\n   4: 0.000213 0.3247 -0.9373\n   5: 0.000213 0.3238 -0.5755\n  ---                        \n4691: 0.000213 0.2131 -0.5868\n4692: 0.000213 0.2130 -0.2243\n4693: 0.000213 0.2073 -0.7393\n4694: 0.000213 0.2580 -0.9151\n4695: 0.000213 0.1821 -1.0360"},{"path":"tmle3.html","id":"targeted-likelihood-updater","chapter":"7 The TMLE Framework","heading":"7.6.3 Targeted Likelihood (updater)","text":"also need define “Targeted Likelihood” object. special type\nlikelihood able updated using tmle3_Update object. \nobject defines update strategy (e.g., submodel, loss function, CV-TMLE \n).constructing targeted likelihood, can specify different update\noptions. See documentation tmle3_Update details different\noptions. example, can disable CV-TMLE (default tmle3) \nfollows:","code":"\ntargeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)\ntargeted_likelihood_no_cv <-\n  Targeted_Likelihood$new(initial_likelihood,\n    updater = list(cvtmle = FALSE)\n  )"},{"path":"tmle3.html","id":"parameter-mapping","chapter":"7 The TMLE Framework","heading":"7.6.4 Parameter Mapping","text":"Finally, need define parameters interest. , spec defines \nsingle parameter, ATE. next section, ’ll see add additional\nparameters.","code":"tmle_params <- ate_spec$make_params(tmle_task, targeted_likelihood)\nprint(tmle_params)\n[[1]]\nParam_ATE: ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}]"},{"path":"tmle3.html","id":"putting-it-all-together","chapter":"7 The TMLE Framework","heading":"7.6.5 Putting it all together","text":"used spec manually generate components, can now\nmanually fit tmle3:result equivalent fitting using tmle3 function .","code":"tmle_fit_manual <- fit_tmle3(\n  tmle_task, targeted_likelihood, tmle_params,\n  targeted_likelihood$updater\n)\nprint(tmle_fit_manual)\nA tmle3_Fit that took 1 step(s)\n   type                                    param  init_est tmle_est      se\n1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.004549  0.01096 0.05046\n      lower  upper psi_transformed lower_transformed upper_transformed\n1: -0.08794 0.1099         0.01096          -0.08794            0.1099"},{"path":"tmle3.html","id":"fitting-tmle3-with-multiple-parameters","chapter":"7 The TMLE Framework","heading":"7.7 Fitting tmle3 with multiple parameters","text":", fit tmle3 just one parameter. tmle3 also supports fitting\nmultiple parameters simultaneously. illustrate , ’ll use \ntmle_TSM_all spec:spec generates Treatment Specific Mean (TSM) level \nexposure variable. Note must first generate new targeted likelihood,\nold one targeted ATE. However, can recycle initial\nlikelihood fit , saving us super learner step.","code":"tsm_spec <- tmle_TSM_all()\ntargeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)\nall_tsm_params <- tsm_spec$make_params(tmle_task, targeted_likelihood)\nprint(all_tsm_params)\n[[1]]\nParam_TSM: E[Y_{A=Control}]\n\n[[2]]\nParam_TSM: E[Y_{A=Handwashing}]\n\n[[3]]\nParam_TSM: E[Y_{A=Nutrition}]\n\n[[4]]\nParam_TSM: E[Y_{A=Nutrition + WSH}]\n\n[[5]]\nParam_TSM: E[Y_{A=Sanitation}]\n\n[[6]]\nParam_TSM: E[Y_{A=WSH}]\n\n[[7]]\nParam_TSM: E[Y_{A=Water}]"},{"path":"tmle3.html","id":"delta-method","chapter":"7 The TMLE Framework","heading":"7.7.1 Delta Method","text":"can also define parameters based Delta Method Transformations \nparameters. instance, can estimate ATE using delta method two\nTSM parameters:can similarly used estimate derived parameters like Relative\nRisks, Population Attributable Risks","code":"ate_param <- define_param(\n  Param_delta, targeted_likelihood,\n  delta_param_ATE,\n  list(all_tsm_params[[1]], all_tsm_params[[4]])\n)\nprint(ate_param)\nParam_delta: E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}]"},{"path":"tmle3.html","id":"fit","chapter":"7 The TMLE Framework","heading":"7.7.2 Fit","text":"can now fit TMLE simultaneously TSM parameters, well \ndefined ATE parameter","code":"all_params <- c(all_tsm_params, ate_param)\n\ntmle_fit_multiparam <- fit_tmle3(\n  tmle_task, targeted_likelihood, all_params,\n  targeted_likelihood$updater\n)\n\nprint(tmle_fit_multiparam)\nA tmle3_Fit that took 1 step(s)\n   type                                       param  init_est tmle_est      se\n1:  TSM                            E[Y_{A=Control}] -0.592825 -0.62093 0.02978\n2:  TSM                        E[Y_{A=Handwashing}] -0.615693 -0.65811 0.04155\n3:  TSM                          E[Y_{A=Nutrition}] -0.608009 -0.60779 0.04187\n4:  TSM                    E[Y_{A=Nutrition + WSH}] -0.597373 -0.60990 0.04095\n5:  TSM                         E[Y_{A=Sanitation}] -0.582596 -0.57852 0.04220\n6:  TSM                                E[Y_{A=WSH}] -0.517360 -0.44815 0.04515\n7:  TSM                              E[Y_{A=Water}] -0.562570 -0.53743 0.03909\n8:  ATE E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}] -0.004549  0.01103 0.05045\n      lower   upper psi_transformed lower_transformed upper_transformed\n1: -0.67929 -0.5626        -0.62093          -0.67929           -0.5626\n2: -0.73955 -0.5767        -0.65811          -0.73955           -0.5767\n3: -0.68986 -0.5257        -0.60779          -0.68986           -0.5257\n4: -0.69015 -0.5296        -0.60990          -0.69015           -0.5296\n5: -0.66123 -0.4958        -0.57852          -0.66123           -0.4958\n6: -0.53664 -0.3597        -0.44815          -0.53664           -0.3597\n7: -0.61405 -0.4608        -0.53743          -0.61405           -0.4608\n8: -0.08785  0.1099         0.01103          -0.08785            0.1099"},{"path":"tmle3.html","id":"exercises-2","chapter":"7 The TMLE Framework","heading":"7.8 Exercises","text":"","code":""},{"path":"tmle3.html","id":"tmle3-ex1","chapter":"7 The TMLE Framework","heading":"7.8.1 Estimation of the ATE with tmle3","text":"Follow steps estimate average treatment effect using data \nCollaborative Perinatal Project (CPP), available sl3 package. \nsimplify example, define binary intervention variable, parity01 –\nindicator one children current child \nbinary outcome, haz01 – indicator average height \nage.Define variable roles \\((W,,Y)\\) creating list nodes.\nInclude following baseline covariates \\(W\\): apgar1, apgar5,\ngagebrth, mage, meducyrs, sexn. \\(\\) \\(Y\\) specified\n. missingness data (specifically, missingness \ncolumns specified node list) need taking care .\nprocess_missing function can used accomplish , like \nwashb_data example .Define tmle3_Spec object ATE, tmle_ATE().Using base learning libraries defined , specify sl3 base\nlearners estimation \\(\\overline{Q}_0 = \\mathbb{E}_0(Y \\mid , W)\\) \n\\(g_0 = \\mathbb{P}(= 1 \\mid W)\\).Define metalearner like .Define one super learner estimating \\(\\overline{Q}_0\\) another \nestimating \\(g_0\\). Use metalearner super learners.Create list two super learners defined step call\nobject learner_list. list names (defining super\nlearner estimation \\(g_0\\)) Y (defining super learner \nestimation \\(\\overline{Q}_0\\)).Fit TMLE tmle3 function specifying (1) tmle3_Spec,\ndefined Step 2; (2) data; (3) list nodes, \nspecified Step 1; (4) list super learners estimation \n\\(g_0\\) \\(\\overline{Q}_0\\), defined Step 6. Note: Like ,\nneed explicitly make copy data (work around\ndata.table optimizations), e.g., (cpp2 <- data.table::copy(cpp)), \nuse cpp2 data going forward.","code":"\n# load the data set\ndata(cpp)\ncpp <- cpp %>%\n  as_tibble() %>%\n  dplyr::filter(!is.na(haz)) %>%\n  mutate(\n    parity01 = as.numeric(parity > 0),\n    haz01 = as.numeric(haz > 0)\n  )\nmetalearner <- make_learner(\n  Lrnr_solnp,\n  loss_function = loss_loglik_binomial,\n  learner_function = metalearner_logistic_binomial\n)"},{"path":"tmle3.html","id":"summary-1","chapter":"7 The TMLE Framework","heading":"7.9 Summary","text":"tmle3 general purpose framework generating TML estimates. easiest\nway use use predefined spec, allowing just fill \nblanks data, variable roles, sl3 learners. However, digging \nhood allows users specify wide range TMLEs. next sections,\n’ll see framework can used estimate advanced parameters \noptimal treatments stochastic shift interventions.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"dynamic-and-optimal-individualized-treatment-regimes","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8 Dynamic and Optimal Individualized Treatment Regimes","text":"Ivana MalenicaBased tmle3mopttx R package\nIvana Malenica, Jeremy Coyle, Mark van der Laan.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"learning-objectives-2","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.1 Learning Objectives","text":"Differentiate dynamic optimal dynamic treatment interventions static\ninterventions.Explain benefits, challenges, associated using optimal\nindividualized treatment regimes practice.Contrast impact implementing optimal individualized treatment\nregime population impact implementing static dynamic\ntreatment regimes population.Estimate causal effects optimal individualized treatment regimes \ntmle3mopttx R package.Assess mean optimal individualized treatment resource\nconstraints.Implement optimal individualized treatment rules based sub-optimal\nrules, “simple” rules, recognize practical benefit rules.Construct “realistic” optimal individualized treatment regimes respect\nreal data subject-matter knowledge limitations interventions \nconsidering interventions supported data.Interpret estimated optimal individualized treatment rule.Measure variable importance defined terms optimal individualized\ntreatment interventions.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"introduction-to-optimal-individualized-interventions","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.2 Introduction to Optimal Individualized Interventions","text":"Identifying intervention effective patient based \nlifestyle, genetic environmental factors common goal precision\nmedicine. put context, Abacavir Tenofovir commonly prescribed\npart antiretroviral therapy Human Immunodeficiency Virus (HIV)\npatients. However, individuals benefit two medications equally.\nparticular, patients renal dysfunction might deteriorate \nprescribed Tenofovir, due high nephrotoxicity caused medication.\nTenofovir still highly effective treatment option HIV patients, \norder maximize patient’s well-, beneficial prescribe\nTenofovir individuals healthy kidney function. another example,\nconsider HIV trial goal improve retention HIV care.\nrandomized clinical trial, several interventions show efficacy- including\nappointment reminders text messages, small cash incentives time\nclinic visits, peer health workers. Ideally, want improve effectiveness\nassigning patient intervention likely benefit ,\nwell improve efficiency allocating resources individuals need\n, benefit .\nFIGURE 5.1: Dynamic Treatment Regime Clinical Setting\nOne opts administer intervention individuals profit ,\ninstead assigning treatment population level. know \nintervention works patient? aim motivates different type \nintervention, opposed static exposures described previous chapters.\nparticular, chapter learn dynamic “individualized”\ninterventions tailor treatment decision based collected\ncovariates. Formally, dynamic treatments represent interventions \ntreatment-decision stage allowed respond currently available\ntreatment covariate history. dynamic treatment rule can thought \nrule input available set collected covariates, \noutput individualized treatment patient\n(Bembom van der Laan, 2007; Chakraborty Moodie, 2013; J Robins, 1986).statistics community treatment strategy termed \nindividualized treatment regime (ITR), also known optimal\ndynamic treatment rule, optimal treatment regime, optimal strategy,\noptimal policy (Murphy, 2003; Robins, 2004). (counterfactual)\npopulation mean outcome ITR value ITR (Murphy, 2003; Robins, 2004).\nEven , suppose one wishes maximize population mean \noutcome, individual access set measured\ncovariates. means, example, can learn individual\ncharacteristics assigning treatment increases probability beneficial\noutcome. ITR maximal value referred \noptimal ITR optimal individualized treatment. Consequently, value\noptimal ITR termed optimal value, \nmean optimal individualized treatment.problem estimating optimal individualized treatment received much\nattention statistics literature years, especially \nadvancement precision medicine; see Murphy (2003), Robins (2004), Zhang et al. (2016),\nZhao et al. (2012), Chakraborty Moodie (2013) Robins Rotnitzky (2014) name . However, much \nearly work depends parametric assumptions. , even randomized\ntrial, statistical inference optimal individualized treatment relies\nassumptions generally believed false, can lead biased\nresults.chapter, consider estimation mean outcome optimal\nindividualized treatment candidate rules restricted depend \nuser-supplied subset baseline covariates. estimation problem \naddressed statistical model data distribution \nnonparametric, places restrictions probability patient\nreceiving treatment given covariates (randomized trial). , \ndon’t need make assumptions relationship outcome \ntreatment covariates, relationship treatment \ncovariates. , provide Targeted Maximum Likelihood Estimator \nmean optimal individualized treatment allows us generate valid\ninference parameter, without parametric assumptions.following, provide brief overview methodology focus \nbuilding intuition target parameter importance — aided simulations,\ndata examples software demonstrations. information technical aspects\nalgorithm, practical advice overview, interested reader invited \nadditionally consult van der Laan Luedtke (2015), Luedtke van der Laan (2016), Montoya, van der Laan, Luedtke, et al. (2023) \nMontoya, van der Laan, Skeem, et al. (2023).","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"data-structure-and-notation","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.3 Data Structure and Notation","text":"Suppose observe \\(n\\) independent identically distributed observations \nform \\(O=(W,,Y) \\sim P_0\\). denote \\(\\) categorical treatment, \\(Y\\)\nfinal outcome. particular, define \\(\\\\mathcal{}\\) \n\\(\\mathcal{} \\equiv \\{a_1, \\cdots, a_{n_A} \\}\\) \\(n_A = |\\mathcal{}|\\), \n\\(n_A\\) denoting number categories (possibly two, binary setup).\nNote treat \\(W\\) vector-valued, representing collected\nbaseline covariates. Therefore, single random individual \\(\\), \nobserved data \\(O_i\\): corresponding baseline covariates \\(W_i\\),\ntreatment \\(A_i\\), final outcome \\(Y_i\\). Let \\(O^n = \\{O_i\\}_{=1}^n\\) denote\n\\(n\\) observed samples. , say \\(O^n \\sim P_0\\), \ndata drawn true probability distribution \\(P_0\\). Let \\(\\mathcal{M}\\)\ndenote statistical model probability distribution data \nnonparametric, beyond possible knowledge treatment mechanism. words, \nmeans make assumptions relationship variables, might\nable say something relationship \\(\\) \\(W\\), case \nrandomized trial. general, know, willing assume \nexperiment produces data, smaller model. true data generating\ndistribution \\(P_0\\) part statistical model \\(\\mathcal{M}\\), write\n\\(P_0 \\\\mathcal{M}\\). previous chapters, denote \\(P_n\\) empirical distribution\ngives observation weight \\(1/n\\).use structural equation model (SEM) order define\nprocess gives rise observed (endogenous) observed\n(exogenous) variables, described Pearl (2009). particular, \ndenote \\(U=(U_W,U_A,U_Y)\\) exogenous random variables, drawn \\(U \\sim P_U\\).\nendogenous variables, written \\(O=(W,,Y)\\), correspond observed data.\ncan define relationships variables following structural equations:\n\\[\\begin{align}\n  W &= f_W(U_W) \\\\ &= f_A(W, U_A) \\\\ Y &= f_Y(, W, U_Y),\n  \\tag{8.1}\n\\end{align}\\]\ncollection \\(f=(f_W,f_A,f_Y)\\) denotes unspecified functions, beyond possible\nknowledge treatment mechanism function, \\(f_A\\). Note \ncase randomized trial, can write NPSEM \n\\[\\begin{align}\n  W &= f_W(U_W) \\\\ &= U_A \\\\ Y &= f_Y(, W, U_Y),\n  \\tag{8.2}\n\\end{align}\\]\n\\(U_A\\) known distribution \\(U_A\\) independent \\(U_W\\). discuss\nlater sections identifiability.likelihood data admits factorization, implied time ordering\n\\(O\\). denote true density \\(O\\) \\(p_0\\), corresponding \ndistribution \\(P_0\\) dominating measure \\(\\mu\\).\n\\[\\begin{equation}\n  p_0(O) = p_{Y,0}(Y \\mid ,W) p_{,0}(\\mid W) p_{W,0}(W) =\n    q_{Y,0}(Y \\mid ,W) g_{,0}(\\mid W) q_{W,0}(W),\n  \\tag{8.3}\n\\end{equation}\\]\n\\(p_{Y,0}(Y|,W)\\) conditional density \\(Y\\) given \\((, W)\\) \nrespect dominating measure \\(\\mu_Y\\), \\(p_{,0}\\) conditional density\n\\(\\) given \\(W\\) respect counting measure \\(\\mu_A\\), \\(p_{W,0}\\) \ndensity \\(W\\) respect dominating measure \\(\\mu_W\\). order \nmatch relevant Targeted Learning literature, also\nwrite \\(P_{Y,0}(Y \\mid , W) = Q_{Y,0}(Y \\mid ,W)\\), \\(P_{,0}(\\mid W) = g_0(\\mid W)\\)\n\\(P_{W,0}(W)=Q_{W,0}(W)\\) corresponding conditional\ndistribution \\(Y\\) given \\((,W)\\), treatment mechanism \\(\\) given \\(W\\), \ndistribution baseline covariates. notational simplicity, additionally define\n\\(\\bar{Q}_{Y,0}(,W) \\equiv \\E_0[Y \\mid ,W]\\) conditional expectation \n\\(Y\\) given \\((,W)\\).Lastly, define \\(V\\) subset baseline covariates optimal\nindividualized rule depends , \\(V \\W\\). Note \\(V\\) \n\\(W\\), empty set, depending subject matter knowledge. particular,\nresearcher might want consider known effect modifiers available time\ntreatment decision possible \\(V\\) covariates, consider dynamic treatment\nrules based measurments can easily obtained clinical setting.\nDefining \\(V\\) restrictive set baseline covariates allows us consider\npossibly sub-optimal rules easier estimate, thereby allows \nstatistical inference counterfactual mean outcome sub-optimal rule;\nelaborate later sections.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"defining-the-causal-effect-of-an-optimal-individualized-intervention","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.4 Defining the Causal Effect of an Optimal Individualized Intervention","text":"Consider dynamic treatment rules, denoted \\(d\\), set possible rules\n\\(\\mathcal{D}\\). , point treatment setting, \\(d\\) deterministic function\ntakes input \\(V\\) outputs treatment decision \n\\(V \\rightarrow d(V) \\\\{a_1, \\cdots, a_{n_A} \\}\\). use dynamic treatment rules,\ncorresponding treatment decision, describe intervention \ntreatment mechanism corresponding outcome dynamic treatment rule.mentioned previous section, causal effects defined terms \nhypothetical interventions SEM (8.1). given\nrule \\(d\\), modified system takes following form:\n\\[\\begin{align}\n  W &= f_W(U_W) \\\\ &= d(V) \\\\ Y_{d(V)} &= f_Y(d(V), W, U_Y),\n  \\tag{8.4}\n\\end{align}\\]\ndynamic treatment regime may viewed intervention \\(\\)\nset equal value based hypothetical regime \\(d(V)\\). couterfactual outcome\n\\(Y_{d(V)}\\) denotes outcome patient treatment assigned using \ndynamic rule \\(d(V)\\), possibly contrary fact. Similarly, counterfactual\noutcomes patients assigned treatment (\\(=1\\)), given control (\\(=0\\)), \nwritten \\(Y_1\\) \\(Y_0\\). Finally, denote distribution counterfactual outcomes\n\\(P_{U,X}\\), implied distribution exogenous variables \\(U\\) structural\nequations \\(f\\). set possible counterfactual distributions encompased\ncausal model \\(\\mathcal{M}^F\\), \\(P_{U,X} \\\\mathcal{M}^F\\).goal causal analysis motivated dynamic interventions \nestimate parameter defined counterfactual mean outcome \nrespect modified intervention distribution. , subject’s outcome ,\npossibly contrary fact, subject received treatment \nassigned rule \\(d(V)\\). Equivalently, ask following causal question:\n“expected outcome every subject received treatment according \n(optimal) individualized treatment?” order estimate optimal individualized\ntreatment, set following optimization problem:\\[d_{opt}(V) \\equiv \\text{argmax}_{d(V) \\\\mathcal{D}}\n\\E_{P_{U,X}}[Y_{d(V)}], \\]\noptimal individualized rule rule maximal value. note , case\nproblem hand requires minimizing mean outcome, optimal individualized\nrule rule minimal value instead.mind, can consider different\ntreatment rules, set \\(\\mathcal{D}\\):true rule, \\(d_{0,\\text{opt}}\\), corresponding causal parameter\n\\(\\E_{U,X}[Y_{d_{0,\\text{opt}}(V)}]\\) denoting expected outcome \ntrue optimal treatment rule \\(d_{0,\\text{opt}}(V)\\).true rule, \\(d_{0,\\text{opt}}\\), corresponding causal parameter\n\\(\\E_{U,X}[Y_{d_{0,\\text{opt}}(V)}]\\) denoting expected outcome \ntrue optimal treatment rule \\(d_{0,\\text{opt}}(V)\\).estimated rule, \\(d_{n,\\text{opt}}\\), corresponding causal parameter\n\\(\\E_{U,X}[Y_{d_{n,\\text{opt}}(V)}]\\) denoting expected outcome \nestimated optimal treatment rule \\(d_{n,\\text{opt}}(V)\\).estimated rule, \\(d_{n,\\text{opt}}\\), corresponding causal parameter\n\\(\\E_{U,X}[Y_{d_{n,\\text{opt}}(V)}]\\) denoting expected outcome \nestimated optimal treatment rule \\(d_{n,\\text{opt}}(V)\\).chapter, focus value estimated optimal rule \\(d_{n,\\text{opt}}\\),\ndata-adaptive parameter. Note true value depends sample! Finally,\ncausal target parameter interest expected outcome \nestimated optimal individualized rule:\\[\\Psi_{d_{n, \\text{opt}}(V)}(P_{U,X}) \\coloneqq \\E_{P_{U,X}}[Y_{d_{n,\n\\text{opt}}(V)}].\\]","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"identification-and-statistical-estimand","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.4.1 Identification and Statistical Estimand","text":"optimal individualized rule, well value optimal\nindividualized rule, causal parameters based unobserved\ncounterfactuals. order causal quantities estimated \nobserved data, need identified statistical parameters. step\nroadmap requires make assumptions:Strong ignorability: \\(\\indep Y^{d_{n, \\text{opt}}(v)} \\mid W\\), \\(\\\\mathcal{}\\).Positivity (overlap): \\(P_0(\\min_{\\\\mathcal{}} g_0(\\mid W) > 0) = 1\\)assumptions, can identify causal target parameter\nobserved data using G-computation formula. value individualized\nrule can now expressed \\[\\E_0[Y_{d_{n, \\text{opt}}(V)}] = \\E_{0,W}[\\bar{Q}_{Y,0}(=d_{n, \\text{opt}}(V),W)],\\], assumptions, interpreted mean outcome \n(possibly contrary fact), treatment assigned according optimal rule.\nFinally, statistical counterpart causal parameter interest \ndefined \\[\\psi_0 = \\E_{0,W}[\\bar{Q}_{Y,0}(=d_{n,\\text{opt}}(V),W)].\\]Inference optimal value shown difficult exceptional\nlaws, defined probability distributions positive\nprobability set \\(W\\) values conditional expectation \\(Y\\)\ngiven \\(\\) \\(W\\) constant \\(\\) - treatments equally\nbenefitial. Inference similarly difficult finite samples \ntreatment effect small strata, even though valid asymptotic\nestimators exist setting. mind, address estimation\nproblem assumption non-exceptional laws effect.Many methods learning optimal rule data developed\n(Chakraborty Moodie, 2013; Murphy, 2003; Robins, 2004; Zhang et al., 2016; Zhao et al., 2012). \nchapter, focus methods discussed Luedtke van der Laan (2016) \nvan der Laan Luedtke (2015). Note however, tmle3mopttx also supports widely\nused Q-learning approach, optimal individualized rule based \ninitial estimate \\(\\bar{Q}_{Y,0}(,W)\\) (Sutton et al., 1998).follow methodology outlined Luedtke van der Laan (2016) \nvan der Laan Luedtke (2015), learn optimal ITR using Super Learner\n(van der Laan et al., 2007), estimate value cross-validated Targeted Minimum\nLoss-based Estimation (CV-TMLE) (Zheng van der Laan, 2011). great generality, first\nneed estimate true individual treatment regime, \\(d_0(V)\\), \ncorresponds dynamic treatment rule takes subset covariates\n\\(V\\) assigns treatment individual based observed\ncovariates \\(v\\). estimate true optimal ITR hand, can\nestimate corresponding value.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"binary-treatment","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.4.2 Binary treatment","text":"estimate optimal individualized treatment regime? case \nbinary treatment, key quantity optimal ITR blip function. One can\nshow optimal ITR assigns treatment individuals falling strata \nstratum specific average treatment effect, blip, \npositive assign treatment individuals quantity \nnegative. Therefore binary treatment, causal assumptions, define\nblip function :\n\\[\\bar{Q}_0(V) \\equiv \\E_0[Y_1-Y_0 \\mid V] \\equiv \\E_0[\\bar{Q}_{Y,0}(1,W) -\n\\bar{Q}_{Y,0}(0,W) \\mid V],\\]\naverage treatment effect within stratum \\(V\\). note \noptimal individualized rule can now derived \\(d_{n,\\text{opt}}(V) = \\mathbb{}(\\bar{Q}_{n}(V) > 0)\\).package tmle3mopttx relies using Super Learner estimate blip\nfunction. mind, loss function utilized learning optimal\nindividualized rule corresponds conditional mean type losses. however worth\nmentioning Luedtke van der Laan (2016) present three different approaches learning optimal\nrule. Namely, focus :Super Learner blip function using squared error loss,Super Learner blip function using squared error loss,Super Learner \\(d_0\\) using weighted classification loss function,Super Learner \\(d_0\\) using weighted classification loss function,Super Learner \\(d_0\\) uses library candidate estimators \nimplied estimators blip well estimators directly go \n\\(d_0\\) weighted classification.Super Learner \\(d_0\\) uses library candidate estimators \nimplied estimators blip well estimators directly go \n\\(d_0\\) weighted classification.benefit relying blip function, implemented tmle3mopttx, \none can look distribution predicted outcomes blip given\nsample. estimate blip allows one identify patients sample\nbenefit (least) treatment. Additionally, blip-based approach\nallows straight-forward extension categorical treatment, interpretable rules,\nOIT resource constrains, percent population can receive\ntreatment (AR Luedtke van der Laan, 2016).Relying Targeted Maximum Likelihood (TML) estimator Super Learner\nestimate blip function, follow steps order obtain\nvalue ITR:Estimate \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(\\mid W)\\) using sl3. denote \nestimates \\(\\bar{Q}_{Y,n}(,W)\\) \\(g_n(\\mid W)\\).Apply doubly robust Augmented-Inverse Probability Weighted (-IPW)\ntransform outcome (double-robust pseudo-outcome), define:\n\\[D_{\\bar{Q}_Y,g,}(O) \\equiv \\frac{\\mathbb{}(=)}{g(\\mid W)} (Y -\n\\bar{Q}_Y(,W)) + \\bar{Q}_Y(=,W).\\]Note randomization positivity assumptions \n\\(\\E[D_{\\bar{Q}_Y,g,}(O) \\mid V] = \\E[Y_a \\mid V]\\). emphasize double\nrobust nature -IPW transform — consistency \\(\\E[Y_a \\mid V]\\) depend\ncorrect estimation either \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(\\mid W)\\). \n, randomized trial, guaranteed consistent estimate \\(\\E[Y_a \\mid V]\\)\neven get \\(\\bar{Q}_{Y,0}(,W)\\) wrong! alternative double-robust pseudo-outcome\njust presented single stage Q-learning, estimate \\(\\bar{Q}_{Y,0}(,W)\\)\nused predict \\(\\bar{Q}_{Y,n}(=1,W)\\) \\(\\bar{Q}_{Y,n}(=0,W)\\). provides\nestimate blip function, \\(\\bar{Q}_{Y,n}(=1,W) - \\bar{Q}_{Y,n}(=0,W)\\), \nrelies good job estimating \\(\\bar{Q}_{Y,0}(,W)\\).Using double-robust pseudo-outcome, can define following contrast:\n\\[D_{\\bar{Q}_Y,g}(O) = D_{\\bar{Q}_Y, g, =1}(O) - D_{\\bar{Q}_Y, g, =0}(O).\\]estimate blip function, \\(\\bar{Q}_{0,}(V)\\), regressing\n\\(D_{\\bar{Q}_Y,g}(O)\\) \\(V\\) using specified sl3 library learners \nappropriate loss function. Finally, ready final steps.estimated rule corresponds \\(\\text{argmax}_{\\\\mathcal{}} \\bar{Q}_{0,}(V)\\).estimated rule corresponds \\(\\text{argmax}_{\\\\mathcal{}} \\bar{Q}_{0,}(V)\\).obtain inference mean outcome estimated optimal rule\nusing CV-TMLE.obtain inference mean outcome estimated optimal rule\nusing CV-TMLE.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"categorical-treatment","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.4.3 Categorical treatment","text":"line approach considered binary treatment, extend blip\nfunction allow categorical treatment. denote blip function\nextensions pseudo-blips, new estimation targets \ncategorical setting. define pseudo-blips vector-valued entities \noutput given \\(V\\) vector length equal number treatment\ncategories, \\(n_A\\). , define :\n\\[\\bar{Q}_0^{pblip}(V) = \\{\\bar{Q}_{0,}^{pblip}(V): \\\\mathcal{} \\}\\]implement three different pseudo-blips tmle3mopttx.Blip1 corresponds choosing reference category treatment, \ndefining blip categories relative specified\nreference. Hence :\n\\[\\bar{Q}_{0,}^{pblip-ref}(V) \\equiv \\E_0[Y_a-Y_0 \\mid V]\\] \\(Y_0\\) \nspecified reference category \\(=0\\). Note , case \nbinary treatment, strategy reduces approach described \nbinary setup.Blip1 corresponds choosing reference category treatment, \ndefining blip categories relative specified\nreference. Hence :\n\\[\\bar{Q}_{0,}^{pblip-ref}(V) \\equiv \\E_0[Y_a-Y_0 \\mid V]\\] \\(Y_0\\) \nspecified reference category \\(=0\\). Note , case \nbinary treatment, strategy reduces approach described \nbinary setup.Blip2 approach corresponds defining blip relative average \ncategories. , can define \\(\\bar{Q}_{0,}^{pblip-avg}(V)\\) :\n\\[\\bar{Q}_{0,}^{pblip-avg}(V) \\equiv \\E_0 [Y_a - \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_a \\mid V].\\]\ncase subject-matter knowledge regarding reference category\nuse available, blip2 might viable option.Blip2 approach corresponds defining blip relative average \ncategories. , can define \\(\\bar{Q}_{0,}^{pblip-avg}(V)\\) :\n\\[\\bar{Q}_{0,}^{pblip-avg}(V) \\equiv \\E_0 [Y_a - \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_a \\mid V].\\]\ncase subject-matter knowledge regarding reference category\nuse available, blip2 might viable option.Blip3 reflects extension Blip2, average now weighted\naverage:\n\\[\\bar{Q}_{0,}^{pblip-wavg}(V) \\equiv \\E_0 [ Y_a - \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_{} P(=\\mid V) \\mid V ].\\]Blip3 reflects extension Blip2, average now weighted\naverage:\n\\[\\bar{Q}_{0,}^{pblip-wavg}(V) \\equiv \\E_0 [ Y_a - \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_{} P(=\\mid V) \\mid V ].\\]Just like binary case, pseudo-blips estimated regressing contrasts\ncomposed using -IPW transform \\(V\\).","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"technical-note-inference-and-data-adaptive-parameter","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.4.4 Technical Note: Inference and data-adaptive parameter","text":"randomized trial, statistical inference relies second-order\ndifference estimate optimal individualized treatment \noptimal individualized treatment asymptotically negligible. \nreasonable condition consider rules depend small number \ncovariates, willing make smoothness assumptions. Alternatively,\ncan consider TMLEs statistical inference data-adaptive target\nparameters defined terms estimate optimal individualized\ntreatment. particular, instead trying estimate mean true\noptimal individualized treatment, aim estimate mean \nestimated optimal individualized treatment. , develop cross-validated\nTMLE approach provides asymptotic inference minimal conditions \nmean estimate optimal individualized treatment. \nparticular, considering data adaptive parameter allows us avoid\nconsistency rate condition fitted optimal rule, required \nasymptotic linearity TMLE mean actual, true optimal\nrule. Practically, estimated (data-adaptive) rule preferred, \npossibly sub-optimal rule one implemented population.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"technical-note-why-cv-tmle","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.4.5 Technical Note: Why CV-TMLE?","text":"discussed van der Laan Luedtke (2015), CV-TMLE necessary \nnon-cross-validated TMLE biased upward mean outcome rule,\ntherefore overly optimistic. generally however, using CV-TMLE allows us\nfreedom estimation therefore greater data adaptivity, without\nsacrificing inference.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"interpreting-the-causal-effect-of-an-optimal-individualized-intervention","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.5 Interpreting the Causal Effect of an Optimal Individualized Intervention","text":"summary, mean outcome optimal individualized treatment \ncounterfactual quantity interest representing mean outcome \neverybody, contrary fact, received treatment optimized\noutcome. optimal individualized treatment regime rule \noptimizes mean outcome dynamic treatment, candidate\nrules restricted respond user-supplied subset baseline\ncovariates. essence, target parameter answers key\naim precision medicine: allocating available treatment tailoring \nindividual characteristics patient, goal optimizing \nfinal outcome.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"oit-eval-bin","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.6 Evaluating the Causal Effect of an OIT with Binary Treatment","text":"Finally, demonstrate evaluate mean outcome optimal\nindividualized treatment using tmle3mopptx. start, let’s load packages\n’ll use set seed:","code":"\nlibrary(data.table)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3mopttx)\nlibrary(devtools)\n\nset.seed(111)"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"simulated-data","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.6.1 Simulated Data","text":"First, load simulated data. start general setup\ntreatment binary variable; later chapter consider\nanother data-generating distribution \\(\\) categorical. example,\ndata generating distribution following form:\n\\[\\begin{align*}\n  W &\\sim \\mathcal{N}(\\bf{0},I_{3 \\times 3})\\\\\n  \\P(=1 \\mid W) &= \\frac{1}{1+\\exp^{(-0.8*W_1)}}\\\\\n  \\P(Y=1 \\mid ,W) &= 0.5\\text{logit}^{-1}[-5I(=1)(W_1-0.5)+5I(=0)(W_1-0.5)] +\n     0.5\\text{logit}^{-1}(W_2W_3)\n\\end{align*}\\]composes observed data structure \\(O = (W, , Y)\\). Note \ntruth \\(\\psi=0.578\\) data generating distribution.formally express fact using tlverse grammar introduced \ntmle3 package, create single data object specify functional\nrelationships nodes directed acyclic graph (DAG) via\nstructural equation models (SEMs), reflected node list\nset :now observed data structure (data) specification role\nvariable dataset plays nodes DAG.","code":"\ndata(\"data_bin\")\n# organize data and nodes for tmle3\ndata <- data_bin\nnode_list <- list(\n  W = c(\"W1\", \"W2\", \"W3\"),\n  A = \"A\",\n  Y = \"Y\"\n)"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"constructing-optimal-stacked-regressions-with-sl3","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.6.2 Constructing Optimal Stacked Regressions with sl3","text":"easily incorporate ensemble machine learning estimation procedure,\nrely facilities provided sl3 R\npackage. Using framework provided sl3\npackage, nuisance parameters TML estimator\nmay fit ensemble learning, using cross-validation framework \nSuper Learner algorithm van der Laan et al. (2007).seen , generate three different ensemble learners must fit,\ncorresponding learners outcome regression (Q), propensity score\n(g), blip function (B). make explicit respect \nstandard notation bundling ensemble learners list object :learner_list object specifies role ensemble\nlearners ’ve generated play computing initial estimators. Recall \nneed initial estimators relevant parts likelihood order \nbuild TMLE parameter interest. particular, learner_list\nmakes explicit fact Y used fitting outcome regression,\nused fitting treatment mechanism regression, finally B\nused fitting blip function.","code":"\n# Define sl3 library and metalearners:\nlrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)\nlrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)\nlrn_xgboost_500 <- Lrnr_xgboost$new(nrounds = 500)\n\nlrn_mean <- Lrnr_mean$new()\nlrn_glm <- Lrnr_glm_fast$new()\nlrn_lasso <- Lrnr_glmnet$new()\n\n## Define the Q learner:\nQ_learner <- Lrnr_sl$new(\n  learners = list(lrn_lasso, lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n## Define the g learner:\ng_learner <- Lrnr_sl$new(\n  learners = list(lrn_lasso, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n## Define the B learner:\nb_learner <- Lrnr_sl$new(\n  learners = list(lrn_lasso,lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n# specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.6.3 Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects","text":"start, initialize specification TMLE parameter \ninterest simply calling tmle3_mopttx_blip_revere. specify argument\nV = c(\"W1\", \"W2\", \"W3\") initializing tmle3_Spec object order \ncommunicate ’re interested learning rule dependent V\ncovariates. Note don’t specify V — result rule\nbased collected covariates; see example like \nshortly. also need specify type\n(pseudo) blip use estimation problem, list learners used\nestimate blip function, whether want maximize minimize final\noutcome, advanced features including searching less\ncomplex rule, realistic interventions possible resource constraints.seen , tmle3_mopttx_blip_revere specification object\n(like tmle3_Spec objects) store data \nspecific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally.elaborate initialization specifications. initializing \nspecification TMLE parameter interest, specified \nset covariates rule depends (V), type (pseudo) blip use\n(type), learners used estimating relevant parts \nlikelihood blip function. addition, need specify whether \nwant maximize mean outcome rule (maximize), whether \nwant estimate rule covariates \\(V\\) provided user\n(complex). FALSE, tmle3mopttx instead consider possible\nrules smaller set covariates including static rules, optimize\nmean outcome subsets \\(V\\). , user might \nprovided full set collected covariates input \\(V\\), possible\ntrue rule depends subset set provided user. \ncase, returned mean optimal individualized rule based\nsmaller subset. addition, provide option search realistic\noptimal individualized interventions via realistic specification. \nTRUE, treatments supported data considered, therefore\nalleviating concerns regarding practical positivity issues. Finally, can incorporate\nsource constrains setting resource argument less 1. explore \nimportant extensions tmle3mopttx later sections.studying output generated, can see confidence interval covers \ntrue parameter, expected.","code":"\n# initialize a tmle specification\ntmle_spec <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\"), type = \"blip1\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE,\n  realistic = FALSE, resource = 1\n)# fit the TML estimator\nfit <- tmle3(tmle_spec, data, node_list, learner_list)\nfit\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est      se  lower  upper psi_transformed\n1:  TSM E[Y_{A=NULL}]   0.3504   0.5508 0.02622 0.4994 0.6022          0.5508\n   lower_transformed upper_transformed\n1:            0.4994            0.6022"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"resource-constraint","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.6.3.1 Resource constraint","text":"can restrict number individuals get treatment \ntreating \\(k\\) percent samples. , patients biggest benefit (according\nestimated blip) receive treatment. order impose \nresource constraint, specify percent individuals can\nget treatment. example, resource=1, \nindividuals blip higher zero get treatment; resource=0,\nnoone treated.can compare number individuals got treatment without \nresource constraint:","code":"\n# initialize a tmle specification\ntmle_spec_resource <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\"), type = \"blip1\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE,\n  realistic = FALSE, resource = 0.90\n)# fit the TML estimator\nfit_resource <- tmle3(tmle_spec_resource, data, node_list, learner_list)\nfit_resource\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est      se  lower  upper psi_transformed\n1:  TSM E[Y_{A=NULL}]   0.3566   0.5579 0.02577 0.5074 0.6084          0.5579\n   lower_transformed upper_transformed\n1:            0.5074            0.6084# Number of individuals getting treatment (no resource constraint):\ntable(tmle_spec$return_rule)\n\n  0   1 \n275 725 \n\n# Number of individuals getting treatment (resource constraint):\ntable(tmle_spec_resource$return_rule)\n\n  0   1 \n274 726 "},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"empty-v","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.6.3.2 Empty V","text":"show example \\(V\\) specified, \nresource constraint.","code":"\n# initialize a tmle specification\ntmle_spec_V_empty <- tmle3_mopttx_blip_revere(\n  type = \"blip1\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE,\n  realistic = FALSE, resource = 0.90\n)# fit the TML estimator\nfit_V_empty <- tmle3(tmle_spec_V_empty, data, node_list, learner_list)\nfit_V_empty\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est      se  lower  upper psi_transformed\n1:  TSM E[Y_{A=NULL}]   0.3259   0.5321 0.01034 0.5118 0.5523          0.5321\n   lower_transformed upper_transformed\n1:            0.5118            0.5523"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"oit-eval-cat","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.7 Evaluating the Causal Effect of an optimal ITR with Categorical Treatment","text":"section, consider evaluate mean outcome optimal\nindividualized treatment \\(\\) two categories. \nprocedure analogous previously described binary treatment, now need\npay attention type blip define estimation stage, well\nconstruct learners.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"simulated-data-1","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.7.1 Simulated Data","text":"First, load simulated data. data generating distribution \nfollowing form:\n\\[\\begin{align*}\n  W &\\sim \\mathcal{N}(\\bf{0},I_{4 \\times 4})\\\\\n  \\P(=\\mid W) &= \\frac{1}{1+\\exp^{(-0.8*W_1)}}\\\\\n  \\P(Y=1 \\mid ,W) = 0.5\\text{logit}^{-1}[15I(=1)(W_1-0.5) - \\\\\n    3I(=2)(2W_1+0.5) + \\\\\n    3I(=3)(3W_1-0.5)] +\\text{logit}^{-1}(W_2W_1) \\\\\n\\end{align*}\\]can just load data available part package follows:composes observed data structure \\(O = (W, , Y)\\). Note \ntruth now \\(\\psi_0=0.658\\), quantity aim estimate.can see number observed categories treatment :","code":"\ndata(\"data_cat_realistic\")\n# organize data and nodes for tmle3\ndata <- data_cat_realistic\nnode_list <- list(\n  W = c(\"W1\", \"W2\", \"W3\", \"W4\"),\n  A = \"A\",\n  Y = \"Y\"\n)# organize data and nodes for tmle3\ntable(data$A)\n\n  1   2   3 \n 24 528 448 "},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"constructing-optimal-stacked-regressions-with-sl3-1","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.7.2 Constructing Optimal Stacked Regressions with sl3","text":"QUESTION: categorical treatment, dimension blip now?\ndimension current example? go estimating ?now create new ensemble learners using \nsl3 learners initialized previously:seen , generate three different ensemble learners must fit,\ncorresponding learners outcome regression, propensity score, \nblip function. Note need estimate \\(g_0(\\mid W)\\) \ncategorical \\(\\) — therefore, use multinomial Super Learner option\navailable within sl3 package learners can address multi-class\nclassification problems. order see learners can used estimate\n\\(g_0(\\mid W)\\) sl3, run following:Since corresponding blip vector valued, \ncolumn additional level treatment. , need create\nmultivariate learners helper function create_mv_learners takes \nlist initialized learners input.make explicit respect standard notation bundling \nensemble learners list object :","code":"\n# Initialize few of the learners:\nlrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)\nlrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)\nlrn_xgboost_500 <- Lrnr_xgboost$new(nrounds = 500)\nlrn_mean <- Lrnr_mean$new()\nlrn_glm <- Lrnr_glm_fast$new()\n\n## Define the Q learner, which is just a regular learner:\nQ_learner <- Lrnr_sl$new(\n  learners = list(lrn_xgboost_100, lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n# Define the g learner, which is a multinomial learner:\n# specify the appropriate loss of the multinomial learner:\nmn_metalearner <- make_learner(Lrnr_solnp,\n  eval_function = loss_loglik_multinomial,\n  learner_function = metalearner_linear_multinomial\n)\ng_learner <- make_learner(Lrnr_sl, list(lrn_xgboost_100, lrn_xgboost_500, lrn_mean), mn_metalearner)\n\n# Define the Blip learner, which is a multivariate learner:\nlearners <- list(lrn_xgboost_50, lrn_xgboost_100, lrn_xgboost_500, lrn_mean, lrn_glm)\nb_learner <- create_mv_learners(learners = learners)# See which learners support multi-class classification:\nsl3_list_learners(c(\"categorical\"))\n [1] \"Lrnr_bound\"                \"Lrnr_caret\"               \n [3] \"Lrnr_cv_selector\"          \"Lrnr_ga\"                  \n [5] \"Lrnr_glmnet\"               \"Lrnr_grf\"                 \n [7] \"Lrnr_gru_keras\"            \"Lrnr_h2o_glm\"             \n [9] \"Lrnr_h2o_grid\"             \"Lrnr_independent_binomial\"\n[11] \"Lrnr_lightgbm\"             \"Lrnr_lstm_keras\"          \n[13] \"Lrnr_mean\"                 \"Lrnr_multivariate\"        \n[15] \"Lrnr_nnet\"                 \"Lrnr_optim\"               \n[17] \"Lrnr_polspline\"            \"Lrnr_pooled_hazards\"      \n[19] \"Lrnr_randomForest\"         \"Lrnr_ranger\"              \n[21] \"Lrnr_rpart\"                \"Lrnr_screener_correlation\"\n[23] \"Lrnr_solnp\"                \"Lrnr_svm\"                 \n[25] \"Lrnr_xgboost\"             \n# specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"oit-eval-cat-v1","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.7.3 Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects","text":"can see confidence interval covers truth.NOTICE distribution assigned treatment! need shortly.","code":"\n# initialize a tmle specification\ntmle_spec_cat <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\", \"W4\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = FALSE\n)# fit the TML estimator\nfit_cat <- tmle3(tmle_spec_cat, data, node_list, learner_list)\nfit_cat\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est      se  lower  upper psi_transformed\n1:  TSM E[Y_{A=NULL}]   0.5347   0.6213 0.06628 0.4914 0.7512          0.6213\n   lower_transformed upper_transformed\n1:            0.4914            0.7512\n\n# How many individuals got assigned each treatment?\ntable(tmle_spec_cat$return_rule)\n\n  1   2   3 \n249 432 319 "},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"extensions-to-causal-effect-of-an-oit","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.8 Extensions to Causal Effect of an OIT","text":"section, consider two extensions procedure described \nestimating value OIT. First one considers setting user\nmight interested grid possible sub-optimal rules, corresponding \npotentially limited knowledge potential effect modifiers. second\nextension concerns implementation realistic optimal individual\ninterventions certain regimes might preferred, due practical \nglobal positivity restraints, realistic implement.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"oit-eval-cat-v2","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.8.1 Simpler Rules","text":"order consider ambitious fully \\(V\\)-optimal rule, \ndefine \\(S\\)-optimal rules optimal rule considers possible subsets\n\\(V\\) covariates, card(\\(S\\)) \\(\\leq\\) card(\\(V\\)) \\(\\emptyset \\S\\). \nparticular, allows us define Super Learner \\(d_0\\) includes\nrange estimators simple (e.g., statis rules) complex\n(e.g. full \\(V\\)), let discrete Super Learner select simple rule \nappropriate. allows us consider sub-optimal rules easier estimate \npotentially provide realistic rules. Within tmle3mopttx paradigm, just need\nchange complex parameter FALSE:Even though specified baseline covariates basis\nrule estimation, simpler rule sufficient maximize mean outcome.QUESTION: set covariates picked tmle3mopttx\ncompare baseline covariates true rule depends ?","code":"\n# initialize a tmle specification\ntmle_spec_cat_simple <- tmle3_mopttx_blip_revere(\n  V = c(\"W4\", \"W3\", \"W2\", \"W1\"), type = \"blip2\",\n  learners = learner_list,\n  maximize = TRUE, complex = FALSE, realistic = FALSE\n)# fit the TML estimator\nfit_cat_simple <- tmle3(tmle_spec_cat_simple, data, node_list, learner_list)\nfit_cat_simple\nA tmle3_Fit that took 1 step(s)\n   type                   param init_est tmle_est      se  lower  upper\n1:  TSM E[Y_{d(V=W4,W3,W2,W1)}]   0.5301   0.5497 0.05822 0.4356 0.6638\n   psi_transformed lower_transformed upper_transformed\n1:          0.5497            0.4356            0.6638"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"oit-eval-cat-v3","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.8.2 Realistic Optimal Individual Regimes","text":"addition considering less complex rules, tmle3mopttx also provides \noption estimate mean realistic, implementable, optimal\nindividualized treatment. often case assigning particular regime\nmight ability fully maximize (minimize) desired outcome, \ndue global practical positivity constrains, treatment can never \nimplemented real life (highly unlikely). , specifying\nrealistic TRUE, consider possibly suboptimal treatments optimize\noutcome question supported data.QUESTION: Referring back data-generating distribution, \nthink distribution allocated treatment changed distribution\n“non-realistic”” rule?","code":"\n# initialize a tmle specification\ntmle_spec_cat_realistic <- tmle3_mopttx_blip_revere(\n  V = c(\"W4\", \"W3\", \"W2\", \"W1\"), type = \"blip2\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE, realistic = TRUE\n)# fit the TML estimator\nfit_cat_realistic <- tmle3(tmle_spec_cat_realistic, data, node_list, learner_list)\nfit_cat_realistic\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est      se  lower upper psi_transformed\n1:  TSM E[Y_{A=NULL}]   0.5377   0.6582 0.02135 0.6163   0.7          0.6582\n   lower_transformed upper_transformed\n1:            0.6163               0.7\n\n# How many individuals got assigned each treatment?\ntable(tmle_spec_cat_realistic$return_rule)\n\n  2   3 \n506 494 "},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"missingness-and-tmle3mopttx","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.8.3 Missingness and tmle3mopttx","text":"section, present use tmle3mopttx package data subject\nmissingness \\(Y\\). Let’s start add missingness outcome, first.start, must first add library — now also need estimate \nmissigness process well.learner_list object specifies role ensemble\nlearners ’ve generated play computing initial estimators needed\nbuilding TMLE parameter interest. particular, makes\nexplicit fact Y used fitting outcome regression\nused fitting treatment mechanism regression,\nB fitting blip function, delta_Y fits missing outcome process.Now, additional estimation step associated missingness added, can\nproceed usual.","code":"data_missing <- data_cat_realistic\n\n#Add some random missingless:\nrr <- sample(nrow(data_missing), 100, replace = FALSE)\ndata_missing[rr,\"Y\"]<-NA\n\nsummary(data_missing$Y)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    0.00    0.00    0.46    1.00    1.00     100 \ndelta_learner <- Lrnr_sl$new(\n  learners = list(lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n# specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner, delta_Y=delta_learner)\n# initialize a tmle specification\ntmle_spec_cat_miss <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\", \"W4\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = FALSE\n)\n# fit the TML estimator\nfit_cat_miss <- tmle3(tmle_spec_cat_miss, data_missing, node_list, learner_list)\nfit_cat_miss"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"q-learning","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.8.4 Q-learning","text":"Alternatively, estimate mean optimal individualized\ntreatment using Q-learning. optimal rule can learned fitting \nlikelihood, consequently estimating optimal rule fit \nlikelihood (Murphy, 2003; Sutton et al., 1998).outline use tmle3mopttx package order estimate mean\nITR using Q-learning. demonstrated previous sections, \nfirst need initialize specification TMLE parameter \ninterest. opposed previous section however, now use\ntmle3_mopttx_Q instead tmle3_mopttx_blip_revere order indicate \nwant use Q-learning instead TMLE.","code":"\n# initialize a tmle specification\ntmle_spec_Q <- tmle3_mopttx_Q(maximize = TRUE)\n\n# Define data:\ntmle_task <- tmle_spec_Q$make_tmle_task(data, node_list)\n\n# Define likelihood:\ninitial_likelihood <- tmle_spec_Q$make_initial_likelihood(\n  tmle_task,\n  learner_list\n)\n\n# Estimate the parameter:\nQ_learning(tmle_spec_Q, initial_likelihood, tmle_task)[1]"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"variable-importance-analysis-with-oit","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.9 Variable Importance Analysis with OIT","text":"Suppose one wishes assess importance observed covariate, \nterms maximizing (minimizing) population mean outcome \noptimal individualized treatment regime. particular, covariate \nmaximizes (minimizes) population mean outcome optimal\nindividualized treatment considered covariates optimal\nassignment might considered important outcome. put \ncontext, perhaps optimal allocation treatment 1, denoted \\(A_1\\), results \nlarger mean outcome optimal allocation another treatment 2, denoted \\(A_2\\).\nTherefore, label \\(A_1\\) higher variable importance \nregard maximizing (minimizing) mean outcome optimal\nindividualized treatment.","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"simulated-data-2","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.9.1 Simulated Data","text":"illustration purpose, bin baseline covariates corresponding \ndata-generating distribution described previously:node list now includes \\(W_1\\) treatments well! Don’t worry,\nstill properly adjust baseline covariates.","code":"\n# bin baseline covariates to 3 categories:\ndata$W1<-ifelse(data$W1<quantile(data$W1)[2],1,ifelse(data$W1<quantile(data$W1)[3],2,3))\n\nnode_list <- list(\n  W = c(\"W3\", \"W4\", \"W2\"),\n  A = c(\"W1\", \"A\"),\n  Y = \"Y\"\n)"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"variable-importance-using-targeted-estimation-of-the-value-of-the-itr","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.9.2 Variable Importance using Targeted Estimation of the value of the ITR","text":"previous sections seen obtain contrast mean\noptimal individualized rule mean observed outcome\nsingle covariate — now ready run variable importance analysis\nspecified covariates. order run variable importance\nanalysis, first need initialize specification TMLE \nparameter interest done . addition, need specify\ndata corresponding list nodes, well appropriate\nlearners outcome regression, propensity score, blip function.\nFinally, need specify whether adjust \ncovariates assessing variable importance . adjust \\(W\\)s\nanalysis, adjust_for_other_A=TRUE, also \\(\\) covariates\ntreated exposure variable importance loop.start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle3_mopttx_vim. First, indicate method used learning optimal\nindividualized treatment specifying method argument \ntmle3_mopttx_vim. method=\"Q\", using Q-learning rule\nestimation, need specify V, type learners arguments\nspec, since important Q-learning. However, \nmethod=\"SL\", corresponds learning optimal individualized\ntreatment using outlined methodology, need specify type\n(pseudo) blip use estimation problem, whether want \nmaximize minimize outcome, complex realistic rules, resource constraint.\nFinally, method=\"SL\" also need communicate ’re interested learning \nrule dependent V covariates specifying V argument. \nmethod=\"Q\" method=\"SL\", need indicate whether want maximize\nminimize mean optimal individualized rule. Finally, also\nneed specify whether final comparison mean optimal\nindividualized rule mean observed outcome \nmultiplicative scale (risk ratio) linear (similar average treatment\neffect).final result tmle3_vim tmle3mopttx spec ordered list\nmean outcomes optimal individualized treatment categorical\ncovariates dataset.","code":"\n# initialize a tmle specification\ntmle_spec_vim <- tmle3_mopttx_vim(\n  V=c(\"W2\"),\n  type = \"blip2\",\n  learners = learner_list,\n  maximize = FALSE,\n  method = \"SL\",\n  complex = TRUE,\n  realistic = FALSE\n)# fit the TML estimator\nvim_results <- tmle3_vim(tmle_spec_vim, data, node_list, learner_list,\n  adjust_for_other_A = TRUE\n)\n\nprint(vim_results)\n   type                param  init_est tmle_est      se    lower    upper\n1:  ATE E[Y_{A=NULL}] - E[Y] -0.013019 -0.06474 0.02171 -0.10730 -0.02218\n2:  ATE E[Y_{A=NULL}] - E[Y]  0.000332  0.05371 0.01688  0.02062  0.08679\n   psi_transformed lower_transformed upper_transformed  A           W Z_stat\n1:        -0.06474          -0.10730          -0.02218 W1  W3,W4,W2,A -2.982\n2:         0.05371           0.02062           0.08679  A W3,W4,W2,W1  3.182\n        p_nz p_nz_corrected\n1: 0.0014338       0.001434\n2: 0.0007326       0.001434"},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"exercises-3","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.10 Exercises","text":"","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"real-world-data-and-tmle3mopttx","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.10.1 Real World Data and tmle3mopttx","text":"Finally, cement everything learned far real data application.previous sections, using WASH Benefits data,\ncorresponding effect water quality, sanitation, hand washing, \nnutritional interventions child development rural Bangladesh.main aim cluster-randomized controlled trial assess \nimpact six intervention groups, including:control;control;hand-washing soap;hand-washing soap;improved nutrition counseling provision lipid-based nutrient\nsupplements;improved nutrition counseling provision lipid-based nutrient\nsupplements;combined water, sanitation, hand-washing, nutrition;combined water, sanitation, hand-washing, nutrition;improved sanitation;improved sanitation;combined water, sanitation, hand-washing;combined water, sanitation, hand-washing;chlorinated drinking water.chlorinated drinking water.aim estimate optimal ITR corresponding value optimal\nITR main intervention WASH Benefits data.outcome interest weight--height Z-score, whereas primary\ntreatment six intervention groups aimed improving living conditions.Questions:Define \\(V\\) mother’s education (momedu), current living conditions (floor),\npossession material items including refrigerator (asset_refrig).\nthink use covariates \\(V\\)? want minimize \nmaximize outcome? (pseudo) blip type use?Define \\(V\\) mother’s education (momedu), current living conditions (floor),\npossession material items including refrigerator (asset_refrig).\nthink use covariates \\(V\\)? want minimize \nmaximize outcome? (pseudo) blip type use?Load WASH Benefits data, define appropriate nodes treatment\noutcome. Use rest covariates \\(W\\) except \nmomheight now. Construct appropriate sl3 library \\(\\), \\(Y\\) \n\\(B\\).Load WASH Benefits data, define appropriate nodes treatment\noutcome. Use rest covariates \\(W\\) except \nmomheight now. Construct appropriate sl3 library \\(\\), \\(Y\\) \n\\(B\\).Based \\(V\\) defined previous question, estimate mean \nITR main randomized intervention used WASH Benefits trial\nweight--height Z-score outcome. ’s TMLE value \noptimal ITR? change initial estimate? \nintervention prominent? think ?Based \\(V\\) defined previous question, estimate mean \nITR main randomized intervention used WASH Benefits trial\nweight--height Z-score outcome. ’s TMLE value \noptimal ITR? change initial estimate? \nintervention prominent? think ?Using formulation questions 1 2, estimate realistic\noptimal ITR corresponding value realistic ITR. results\nchange? intervention prominent realistic rules? \nthink ?Using formulation questions 1 2, estimate realistic\noptimal ITR corresponding value realistic ITR. results\nchange? intervention prominent realistic rules? \nthink ?Consider simpler rules WASH benefits data example. covariates \nfinal rule depend ?Consider simpler rules WASH benefits data example. covariates \nfinal rule depend ?Change treatment binary variable (asset_sewmach), estimate \nvalue ITR setting \\(60\\%\\) resource constraint. \nresults indicate?Change treatment binary variable (asset_sewmach), estimate \nvalue ITR setting \\(60\\%\\) resource constraint. \nresults indicate?Change treatment , now mother’s education (momedu), \nestimate value ITR setting. results\nindicate? Can intervene variable?Change treatment , now mother’s education (momedu), \nestimate value ITR setting. results\nindicate? Can intervene variable?","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"review-of-key-concepts-1","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.10.2 Review of Key Concepts","text":"difference dynamic optimal individualized regimes?difference dynamic optimal individualized regimes?’s intuition behind using different blip types? switch\nblip1 blip2 considering categorical treatment? \nadvantages ?’s intuition behind using different blip types? switch\nblip1 blip2 considering categorical treatment? \nadvantages ?Look back results generated section categorical\ntreatments, compare mean optimal\nindividualized treatment section complex categorical\ntreatments. set covariates picked tmle3mopttx\ncompare baseline covariates true rule depends ?Look back results generated section categorical\ntreatments, compare mean optimal\nindividualized treatment section complex categorical\ntreatments. set covariates picked tmle3mopttx\ncompare baseline covariates true rule depends ?Compare distribution treatments assigned true optimal\nindividualized treatment realistic optimal individualized treatment.\nReferring back data-generating distribution, think \ndistribution allocated treatment changed?Compare distribution treatments assigned true optimal\nindividualized treatment realistic optimal individualized treatment.\nReferring back data-generating distribution, think \ndistribution allocated treatment changed?Using simulation, perform variable importance analysis using\nQ-learning. results change ?Using simulation, perform variable importance analysis using\nQ-learning. results change ?","code":""},{"path":"dynamic-and-optimal-individualized-treatment-regimes.html","id":"advanced-topics-1","chapter":"8 Dynamic and Optimal Individualized Treatment Regimes","heading":"8.10.3 Advanced Topics","text":"can extend current approach include exceptional laws?can extend current approach include exceptional laws?can extend current approach continuous interventions?can extend current approach continuous interventions?","code":""},{"path":"shift.html","id":"shift","chapter":"9 Stochastic Treatment Regimes","heading":"9 Stochastic Treatment Regimes","text":"Nima HejaziFeaturing tmle3shift R package.Learning ObjectivesDifferentiate stochastic treatment regimes static, dynamic, optimal\ndynamic treatment regimes.Describe real-world data analysis may incorporate assessing causal\neffects stochastic treatment regimes.Contrast population-level (general) stochastic treatment regime \n(individualized) modified treatment policy.Estimate population-level causal effects modified treatment policies\ntmle3shift R package.Specify interpret set causal effects based upon differing modified\ntreatment policies arising grid counterfactual shifts.Construct marginal structural models measure variable importance terms\nstochastic interventions, using grid counterfactual shifts.Implement, tmle3shift R package, modified treatment policies\nshift individual units extent supported observed\ndata.","code":""},{"path":"shift.html","id":"why-stochastic-interventions","chapter":"9 Stochastic Treatment Regimes","heading":"9.1 Why Stochastic Interventions?","text":"Stochastic treatment regimes, stochastic interventions, constitute \nrelatively simple yet extremely flexible expressive framework defining\nrealistic causal effects. contrast intervention regimens discussed\npreviously, stochastic interventions may applied nearly manner \ntreatment variable – binary, ordinal, continuous – allowing rich set \ncausal effects defined formalism. chapter focuses \nexamining types stochastic interventions may applied \ncontinuous treatment variables, static dynamic treatment regimes\neasily applied. Notably, resultant causal effects conveniently \nendowed interpretation echoing ordinary regression adjustment.next chapter, introduce two alternative uses stochastic\ninterventions – recently formulated intervention applicable binary\ntreatment variables (Kennedy, 2019) definition causal\neffects presence post-treatment, mediating, variables. , \nfocus tools provided tmle3shift R\npackage, exposes targeted minimum\nloss-based estimators causal effects stochastic interventions \nadditively shift observed value treatment variable. \ncomprehensive, technical presentations aspects material \nchapter appear Dı́az van der Laan (2012), Dı́az van der Laan (2018),\nHejazi, van der Laan, et al. (2020), Hejazi (2021).","code":""},{"path":"shift.html","id":"data-structure-and-notation-1","chapter":"9 Stochastic Treatment Regimes","heading":"9.2 Data Structure and Notation","text":"Let us return familiar data unit \\(O = (W, , Y)\\), \\(W\\) denote\nbaseline covariates (e.g., age, biological sex, education level), \\(\\) \ntreatment variable (e.g., dose nutritional supplements), \\(Y\\) outcome\ninterest (e.g., disease status). , consider \\(\\) \ncontinuous-valued (.e., \\(\\\\R\\)) ordinal many levels. given\nstudy, consider observing \\(n\\) independent identically distributed units\n\\(O_1, \\ldots, O_n\\).Following roadmap, let \\(O \\sim P_0 \\\\M\\), \\(\\M\\) \nnonparametric statistical model, minimizing restrictions form \ndata-generating distribution \\(P_0\\). formalize definition stochastic\ninterventions corresponding causal effects, introduce structural\ncausal model (SCM), based Pearl (2009), define system\nchanges posited interventions:\n\\[\\begin{align}\n  W &= f_W(U_W) \\\\ \\nonumber\n  &= f_A(W, U_A) \\\\ \\nonumber\n  Y &= f_Y(, W, U_Y).\n  \\tag{9.1}\n\\end{align}\\]\nset structural equations provide mechanistic model describing \nrelationships variables composing observed data unit \\(O\\). SCM\ndescribes temporal ordering variables (.e., \\(Y\\) occurs \n\\(\\), occurs \\(W\\)); specifies deterministic functions \\(\\{f_W, f_A, f_Y\\}\\) generating variable \\(\\{W, , Y\\}\\) based preceding \nexogenous (unobserved) variable \\(\\{U_W, U_A, U_Y\\}\\); requires \nexogenous variable assumed contain unobserved causes \ncorresponding observed variable.can factorize likelihood data unit \\(O\\) follows, revealing\northogonal components density, \\(p_0\\), evaluated typical\nobservation \\(o\\):\n\\[\\begin{align}\n  p_0(o) = &q_{0,Y}(y \\mid = , W = w) \\\\ \\nonumber\n    &g_{0,}(\\mid W = w) \\\\ \\nonumber\n    &q_{0,W}(w),\\\\ \\nonumber\n  \\tag{9.2}\n\\end{align}\\]\n\\(q_{0, Y}\\) conditional density \\(Y\\) given \\(\\{, W\\}\\) respect\ndominating measure, \\(g_{0, }\\) conditional density \\(\\) given\n\\(W\\) respect dominating measure \\(\\mu\\), \\(q_{0, W}\\) density \n\\(W\\) respect dominating measure \\(\\nu\\). interest continuing \nuse familiar notation, let \\(\\overline{Q}(, W) = \\E[Y \\mid , W]\\), \\(g(\\mid W) = g_{}(\\mid W)\\), \\(q_W\\) marginal distribution \\(W\\). Importantly,\nSCM parameterizes \\(p_0\\) terms distribution random variables\n\\((O, U)\\) modeled system equations. turn, implies model \ndistribution counterfactual random variables generated interventions\ndata-generating process.","code":""},{"path":"shift.html","id":"defining-the-causal-effect-of-a-stochastic-intervention","chapter":"9 Stochastic Treatment Regimes","heading":"9.3 Defining the Causal Effect of a Stochastic Intervention","text":"Causal effects defined terms contrasts hypothetical interventions\nSCM (9.1). Stochastic interventions modifying\ncomponents SCM may thought two equivalent ways. general\nstochastic intervention replaces equation \\(f_A\\), gives rise \\(\\),\n\\(g(\\mid W)\\), natural conditional density , candidate\ndensity \\(g_{A_{\\delta}}(\\mid W)\\). absence intervention, \nconsider given value \\(\\\\mathcal{}\\), support \\(\\) – , \nresult evaluating function \\(f_A\\) given value \\(W = w\\) – \nresult random draw distribution defined conditional density\n\\(g(\\mid W)\\), , \\(A_{\\delta} \\sim g_{A_{\\delta}}(\\cdot \\mid W)\\). \napplying intervention, simply remove structural equation \\(f_A\\),\ninstead drawing post-intervention value \\(A_{\\delta}\\) distribution\ndefined candidate density \\(g_{A_{\\delta}}(\\mid W)\\). \npost-intervention value \\(A_{\\delta}\\) stochastically modified sense\ndrawn arbitrary (practice, user-defined)\ndistribution. Note familiar case static interventions can \nrecovered choosing degenerate candidate distributions, place mass\njust single value. Stock (1989) first considered estimating \ntotal effects stochastic interventions.restrictions choice candidate post-treatment\ndensity \\(g_{A_{\\delta}}(\\mid W)\\), practice, often chosen based \nknowledge natural (pre-intervention) density \\(g(\\mid W)\\). \n\\(g_{A_{\\delta}}(\\mid W)\\) piecewise smooth invertible ()\n(Haneuse Rotnitzky, 2013), direct correspondence \npost-intervention density \\(g_{A_{\\delta}}(\\mid W)\\) function \\(d(, W; \\delta)\\) maps observed pair \\(\\{, W\\}\\) post-intervention quantity\n\\(A_{\\delta}\\). cases, stochastic intervention, defined \\(d(, W; \\delta)\\), said depend natural value treatment \ntermed modified treatment policy (MTP) (Dı́az van der Laan, 2018; Haneuse Rotnitzky, 2013; Hejazi, 2021). Haneuse Rotnitzky (2013) \nYoung et al. (2014) provide detailed discussions contrasting \ninterpretations causal effects modified treatment policies \ngeneral stochastic interventions.Definition 9.1  (Piecewise Smooth Invertibility) \\(w \\\\mathcal{W}\\), assume interval \\(\\mathcal{}(w) = (l(w,), u(w))\\) may partitioned subintervals \\(\\mathcal{}_{\\delta,j}(w): j = 1, \\ldots, J(w)\\) \\(d(, w; \\delta)\\) equal \\(d_j(, w; \\delta)\\) \\(\\mathcal{}_{\\delta,j}(w)\\) \\(d_j(\\cdot,w; \\delta)\\) inverse\nfunction \\(b_j(\\cdot, w; \\delta)\\) derivative \\(b_j'(\\cdot, w; \\delta)\\).stochastic intervention gives rise counterfactual random variable\n\\(Y_{A_{\\delta}} := f_Y(A_{\\delta}, W, U_Y)\\), counterfactual outcome\n\\(Y_{A_{\\delta}} \\sim \\mathcal{P}_0^{A_{\\delta}}\\) arises replacing \nnatural value \\(\\) \\(A_{\\delta}\\) (whether draw \n\\(g_{A_{\\delta}}(\\mid W)\\) evaluating \\(d(, W; \\delta)\\)). \nremainder chapter, focus additive MTPs form\n\\[\\begin{equation}\n  d(, w; \\delta) =\n  \\begin{cases}\n    + \\delta & \\text{} + \\delta \\leq u(w) \\\\\n    & \\text{} + \\delta > u(w),\n  \\end{cases}\n  \\tag{9.3}\n\\end{equation}\\]\n\\(\\delta \\\\mathbb{R}\\) defines degree observed \\(= \\)\nshifted, context stratum \\(W = w\\), \\(l(w)\\) \n\\(u(w)\\) minimum maximum values treatment \\(\\) stratum\n\\(W = w\\). Consider, example, case \\(\\) denotes (continuous-valued)\ndosage nutritional supplements (e.g., number vitamin pills) assume\ndistribution \\(\\) conditional \\(W = w\\) support interval\n\\((l(w), u(w))\\). , minimum number pills taken individual\ncovariate stratum defined \\(W = w\\) \\(l(w)\\); similarly, \nmaximum \\(u(w)\\). stochastic intervention may interpreted \nresult clinic policy encouraging individuals consume \\(\\delta\\) \nvitamin pills (\\(\\delta\\)) normally recommended (\\(\\)) based\nbaseline characteristics \\(W\\). class stochastic interventions\nintroduced Dı́az van der Laan (2012) discussed \nHaneuse Rotnitzky (2013), Dı́az van der Laan (2018), Hejazi, van der Laan, et al. (2020), \nHejazi (2021). class interventions may expressed \ngeneral stochastic intervention, per Dı́az van der Laan (2012), considering \nrandom draw \\(\\P_{A_{\\delta}}(g_{0, })(= \\mid W) = g_{0,}(- \\delta(W) \\mid W)\\).order evaluate causal effect intervention, consider \nparameter interest counterfactual mean outcome \nstochastically modified intervention distribution. target causal estimand\n\\(\\psi_{0, \\delta} \\coloneqq \\E_{P_0^{A_{\\delta}}}\\{Y_{A_{\\delta}}\\}\\), \nmean counterfactual outcome variable \\(Y_{A_{\\delta}}\\).\nDı́az van der Laan (2018) showed \\(\\psi_{0, \\delta}\\) may identified \nfunctional distribution \\(O\\):\n\\[\\begin{align}\n  \\psi_{0,\\delta} = \\int_{\\mathcal{W}} \\int_{\\mathcal{}} & \\E_{P_0}\n   \\{Y \\mid = d(, w), W = w\\} \\nonumber \\\\ &q_{0, }(\\mid W = w)\n   q_{0, W}(w) d\\mu()d\\nu(w).\n  \\tag{9.4}\n\\end{align}\\]\ncertain identification conditions, enumerate shortly, \nstatistical parameter Equation (9.4) matches exactly\ncounterfactual mean \\(\\psi_{0, \\delta}\\). book concerned\nidentification causal parameters – , establishing\nstatistical functionals observed data causal interpretations\ncertain assumptions – review key assumptions identifying \ncounterfactual mean \\(\\psi_{0, \\delta}\\) . SCM introduced prior\ngenerates independent identically distributed units \\(O\\), common\nidentification assumptions consistency (\\(Y^{A_{\\delta,}}_i = Y_i\\) \nevent \\(A_i = d(a_i, w_i)\\), \\(= 1, \\ldots, n\\)) lack interference\n(\\(Y^{A_{\\delta,}}_i\\) depend \\(d(a_j, w_j)\\) \\(= 1, \\ldots, n\\)\n\\(j \\neq \\)) hold. Beyond , require unmeasured confounding (\nanalog randomization assumption observational studies) positivity.Definition 9.2  (Unmeasured Confounding) \\(A_i \\indep Y^{A_{\\delta,}}_i \\mid W_i\\), \\(= 1, \\ldots, n\\). \nobservational study analog well-known randomization assumption.Definition 9.3  (Treatment Positivity) \\(a_i \\\\mathcal{} \\implies d(a_i, w_i) \\\\mathcal{}\\) \\(w \\\\mathcal{W}\\), \\(\\mathcal{}\\) denotes support \\(\\mid W = w_i \\quad \\forall = 1, \\ldots n\\).","code":""},{"path":"shift.html","id":"estimating-the-causal-effect-of-a-stochastic-intervention","chapter":"9 Stochastic Treatment Regimes","heading":"9.4 Estimating the Causal Effect of a Stochastic Intervention","text":"Dı́az van der Laan (2012) provided derivation efficient influence function\n(EIF), key quantity constructing efficient estimators, \nnonparametric model \\(\\M\\) developed classical efficient estimators\nquantity, including substitution, inverse probability weighted, one-step\ntargeted maximum likelihood (TML) estimators. one-step TML\nestimators allow semiparametric-efficient estimation inference \ntarget quantity interest \\(\\psi_{0, \\delta}\\). described \nDı́az van der Laan (2018), EIF \\(\\psi_{0, \\delta}\\), respect \nnonparametric model \\(\\M\\), \n\\[\\begin{equation}\n  D(P_0)(x) = H(, w)({y - \\overline{Q}(, w)}) +\n  \\overline{Q}(d(, w), w) - \\Psi(P_0),\n  \\tag{9.5}\n\\end{equation}\\]\nauxiliary covariate \\(H(,w)\\) may expressed\n\\[\\begin{equation}\n  H(,w) = \\mathbb{}(+ \\delta < u(w)) \\frac{g_0(- \\delta \\mid w)}\n  {g_0(\\mid w)} + \\mathbb{}(+ \\delta \\geq u(w)),\n  \\tag{9.6}\n\\end{equation}\\]\nmay reduced \n\\[\\begin{equation}\n  H(,w) = \\frac{g_0(- \\delta \\mid w)}{g_0(\\mid w)} + 1\n  \\tag{9.7}\n\\end{equation}\\]\ntreatment \\(\\) lies within limits defined covariate strata\n\\(W\\), , \\(A_i \\(u(w) - \\delta, u(w))\\). efficient influence\nfunction key ingredient construction semiparametric-efficient\nestimators. Next, focus targeted maximum likelihood (TML) estimator, \nDı́az van der Laan (2018) give following recipe:Construct initial estimators \\(g_n\\) \\(g_0(, W)\\) \\(\\overline{Q}_n\\) \n\\(\\overline{Q}_0(, W)\\), ideally using data-adaptive regression techniques.observation \\(\\), compute estimate \\(H_n(a_i, w_i)\\) \nauxiliary covariate \\(H(a_i,w_i)\\).Construct one-dimensional logistic regression model,\n\\[ \\text{logit}\\overline{Q}_{\\epsilon, n}(, w) =\n\\text{logit}\\overline{Q}_n(, w) + \\epsilon H_n(, w),\\]\nanalogous regression model incorporating \\(H_n\\) weights. Estimate \nregression model’s parameter \\(\\epsilon\\), obtaining \\(\\epsilon_n\\). outcome\nregression model yields \\(\\overline{Q}_n^{\\star}\\).Compute TML estimator \\(\\Psi_n\\) target parameter, defining update\n\\(\\overline{Q}_n^{\\star}\\) initial estimate\n\\(\\overline{Q}_{n, \\epsilon_n}\\):\n\\[\\begin{equation}\n  \\psi_n = \\Psi(P_n^{\\star}) = \\frac{1}{n} \\sum_{= 1}^n\n  \\overline{Q}_n^{\\star}(d(A_i, W_i), W_i).\n  \\tag{9.8}\n\\end{equation}\\]discussed previously, TML estimators constructed \nasymptotically linear usually doubly robust. Asymptotic linearity\nmeans asymptotic difference estimator \\(\\psi_n\\) \ntarget parameter \\(\\psi_0\\) can expressed terms EIF, ,\n\\[\\begin{equation}\n  \\sqrt{n}(\\psi_n - \\psi_0) = \\frac{1}{\\sqrt{n}}\\sum_{=1}^n D(P_0)(O_i) +\n    o_p(1).\n  \\tag{9.9}\n\\end{equation}\\]\nTogether regularity, asymptotic linearity establishes class estimators\nwhose asymptotic variance bounded asymptotic variance \nEIF. means estimators solutions EIF estimating\nequation (.e., plugging TML estimator \\(\\psi_n\\) EIF equation\nresults solution close zero) sampling variance may \napproximated variance EIF closed form. latter fact \ncomputationally convenient, resampling methods (e.g., bootstrap) \nstrictly necessary variance estimation. central limit theorem establishes\nasymptotic distribution estimator \\(\\psi_n\\) centered \n\\(\\psi_0\\) Gaussian:\n\\[\\begin{equation}\n  \\sqrt{n}(\\psi_n - \\psi_0) \\\\text{Normal}(0, \\sigma^2(D(P_0))).\n  \\tag{9.10}\n\\end{equation}\\]\nThus, estimate \\(\\sigma_n^2\\) variance \\(\\sigma^2(D(P_0))\\) may \ncomputed\n\\[\\begin{equation}\n  \\sigma_n^2 = \\frac{1}{n} \\sum_{= 1}^{n} D^2(\\overline{Q}_n^{\\star},\n  g_n)(O_i),\n  \\tag{9.11}\n\\end{equation}\\]\nallowing Wald-style confidence intervals coverage level \\((1 - \\alpha)\\) \ncomputed \\(\\psi_n \\pm z_{(1 - \\alpha/2)} \\cdot \\sigma_n / \\sqrt{n}\\). \ncertain conditions, resampling based bootstrap may also used \ncompute \\(\\sigma_n^2\\) (van der Laan Rose, 2011).","code":""},{"path":"shift.html","id":"interpreting-the-causal-effect-of-a-stochastic-intervention","chapter":"9 Stochastic Treatment Regimes","heading":"9.5 Interpreting the Causal Effect of a Stochastic Intervention","text":"\nFIGURE 5.1: counterfactual outcome changes natural treatment distribution shifted simple stochastic intervention\n","code":""},{"path":"shift.html","id":"evaluating-the-causal-effect-of-a-stochastic-intervention","chapter":"9 Stochastic Treatment Regimes","heading":"9.6 Evaluating the Causal Effect of a Stochastic Intervention","text":"start, let’s load packages ’ll using throughout simple data exampleWe need estimate two components likelihood order construct TML\nestimator. first components outcome regression,\n\\(\\overline{Q}_n\\), simple regression form \\(\\E[Y \\mid ,W]\\). \nestimate quantity may constructed using Super Learner\nalgorithm. construct components sl3-style Super Learner \nregression , using small variety parametric nonparametric\nregression techniques:second estimate treatment mechanism, \\(g_n\\), .e., \ngeneralized propensity score. case continuous intervention node\n\\(\\), quantity takes form \\(p(\\mid W)\\), conditional\ndensity. Generally speaking, conditional density estimation challenging\nproblem received much attention literature. estimate \ntreatment mechanism, must make use learning algorithms specifically suited\nconditional density estimation; list learners may extracted \nsl3 using sl3_list_learners():proceed, ’ll select two learners, Lrnr_haldensify using\nhighly adaptive lasso conditional density estimation, based \nalgorithm given Dı́az van der Laan (2011) implemented Hejazi, Benkeser, et al. (2022), \nsemiparametric location-scale conditional density estimators implemented \nsl3 package. Super Learner may \nconstructed pooling estimates modified conditional density\nregression techniques (note exclude approach based \nhaldensify learner Super Learner account computationally\nintensive nature approach).Finally, construct learner_list object use constructing TML\nestimator target parameter interest:learner_list object specifies role ensemble\nlearners generated play computing initial estimators \nused building TMLE parameter interest . particular, \nmakes explicit fact Q_learner used fitting outcome\nregression g_learner used estimating treatment mechanism.","code":"\nlibrary(data.table)\nlibrary(haldensify)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3shift)\n# learners used for conditional mean of the outcome\nmean_lrnr <- Lrnr_mean$new()\nfglm_lrnr <- Lrnr_glm_fast$new()\nrf_lrnr <- Lrnr_ranger$new()\nhal_lrnr <- Lrnr_hal9001$new(max_degree = 3, n_folds = 3)\n\n# SL for the outcome regression\nsl_reg_lrnr <- Lrnr_sl$new(\n  learners = list(mean_lrnr, fglm_lrnr, rf_lrnr, hal_lrnr),\n  metalearner = Lrnr_nnls$new()\n)sl3_list_learners(\"density\")\n[1] \"Lrnr_density_discretize\"     \"Lrnr_density_hse\"           \n[3] \"Lrnr_density_semiparametric\" \"Lrnr_haldensify\"            \n[5] \"Lrnr_solnp_density\"         \n# learners used for conditional densities for (g_n)\nhaldensify_lrnr <- Lrnr_haldensify$new(\n  n_bins = c(5, 10, 20),\n  lambda_seq = exp(seq(-1, -10, length = 200))\n)\n# semiparametric density estimator with homoscedastic errors (HOSE)\nhose_hal_lrnr <- make_learner(Lrnr_density_semiparametric,\n  mean_learner = hal_lrnr\n)\n# semiparametric density estimator with heteroscedastic errors (HESE)\nhese_rf_glm_lrnr <- make_learner(Lrnr_density_semiparametric,\n  mean_learner = rf_lrnr,\n  var_learner = fglm_lrnr\n)\n\n# SL for the conditional treatment density\nsl_dens_lrnr <- Lrnr_sl$new(\n  learners = list(hose_hal_lrnr, hese_rf_glm_lrnr),\n  metalearner = Lrnr_solnp_density$new()\n)\nlearner_list <- list(Y = sl_reg_lrnr, A = sl_dens_lrnr)"},{"path":"shift.html","id":"example-with-simulated-data","chapter":"9 Stochastic Treatment Regimes","heading":"9.6.1 Example with Simulated Data","text":"composes observed data structure \\(O = (W, , Y)\\). formally\nexpress fact using tlverse grammar introduced tmle3 package,\ncreate single data object specify functional relationships \nnodes directed acyclic graph (DAG) via SCM, reflected \nnode list set .now observed data structure (data) specification role\nvariable data set plays nodes DAG.start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle_shift. specify argument shift_val = 0.5 initializing \ntmle3_Spec object communicate ’re interested shift \\(0.5\\) \nscale treatment \\(\\) – , specify \\(\\delta = 0.5\\) (\narbitrarily chosen value example).seen , tmle_shift specification object (like tmle3_Spec\nobjects) store data specific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally (see tmle3 documentation details).","code":"# simulate simple data for tmle-shift sketch\nn_obs <- 400 # number of observations\ntx_mult <- 2 # multiplier for the effect of W = 1 on the treatment\n\n## baseline covariates -- simple, binary\nW <- replicate(2, rbinom(n_obs, 1, 0.5))\n\n## create treatment based on baseline W\nA <- rnorm(n_obs, mean = tx_mult * W, sd = 1)\n\n## create outcome as a linear function of A, W + white noise\nY <- rbinom(n_obs, 1, prob = plogis(A + W))\n\n# organize data and nodes for tmle3\ndata <- data.table(W, A, Y)\nsetnames(data, c(\"W1\", \"W2\", \"A\", \"Y\"))\nnode_list <- list(\n  W = c(\"W1\", \"W2\"),\n  A = \"A\",\n  Y = \"Y\"\n)\nhead(data)\n   W1 W2        A Y\n1:  1  1  0.27165 1\n2:  0  0 -0.66337 1\n3:  0  0  0.11337 0\n4:  0  1 -0.73256 0\n5:  1  1  0.38884 1\n6:  0  0  0.04399 0\n# initialize a tmle specification\ntmle_spec <- tmle_shift(\n  shift_val = 0.5,\n  shift_fxn = shift_additive,\n  shift_fxn_inv = shift_additive_inv\n)"},{"path":"shift.html","id":"targeted-estimation-of-stochastic-interventions-effects","chapter":"9 Stochastic Treatment Regimes","heading":"9.6.2 Targeted Estimation of Stochastic Interventions Effects","text":"print method resultant tmle_fit object conveniently displays \nresults computing TML estimator \\(\\psi_n\\). standard error estimate\ncomputed based estimated EIF.","code":"tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)\n\nIter: 1 fn: 548.8338     Pars:  0.94735 0.05265\nIter: 2 fn: 548.8338     Pars:  0.94736 0.05264\nsolnp--> Completed in 2 iterations\ntmle_fit\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est      se  lower  upper psi_transformed\n1:  TSM E[Y_{A=NULL}]   0.7645   0.7601 0.02284 0.7154 0.8049          0.7601\n   lower_transformed upper_transformed\n1:            0.7154            0.8049"},{"path":"shift.html","id":"selecting-stable-stochastic-interventions","chapter":"9 Stochastic Treatment Regimes","heading":"9.7 Selecting Stable Stochastic Interventions","text":"times, particular choice shift parameter \\(\\delta\\) may lead \npositivity violations downstream instability estimation process. \norder curb issues, can make choices \\(\\delta\\) based impact\ncandidate values estimator. Recall simplified expression \nauxiliary covariate TMLE \\(\\psi\\) \\(H = \\frac{g(- \\delta \\mid w)}{g(\\mid w)}\\), \\(g(- \\delta \\mid w)\\) defined stochastic\nintervention interest. can design stochastic intervention avoid\nviolations positivity assumption considering bound \\(C(\\delta) = \\frac{g(- \\delta \\mid w)}{g(\\mid w)} < M\\), \\(M\\) potentially\nuser-specified upper bound \\(C(\\delta)\\). Note \\(C(\\delta)\\) corresponds \ninverse weight assigned unit counterfactual treatment value \\(= + \\delta\\), natural treatment value \\(= \\), covariates \\(W = w\\). ,\n\\(C(\\delta)\\) can viewed measure influence given observation\nestimator \\(\\psi_n\\). limiting \\(C(\\delta)\\), whether choice\n\\(M\\) \\(\\delta\\), can limit potential instability estimator. \ncan formalize procedure defining new shift function \\(\\delta(, W)\\):\n\\[\\begin{equation}\n  \\delta(, w) =\n    \\begin{cases}\n      \\delta, & \\delta_{\\text{min}}(,w) \\leq \\delta \\leq\n        \\delta_{\\text{max}}(,w) \\\\\n      \\delta_{\\text{max}}(,w), & \\delta \\geq \\delta_{\\text{max}}(,w) \\\\\n      \\delta_{\\text{min}}(,w), & \\delta \\leq \\delta_{\\text{min}}(,w) \\\\\n    \\end{cases},\n    \\tag{9.12}\n\\end{equation}\\]\n\\[\\delta_{\\text{max}}(, w) = \\text{argmax}_{\\left\\{\\delta \\geq 0,\n\\frac{g(- \\delta \\mid w)}{g(\\mid w)} \\leq M \\right\\}} \\frac{g(- \\delta\n\\mid w)}{g(\\mid w)}\\] \n\\[\\delta_{\\text{min}}(, w) = \\text{argmin}_{\\left\\{\\delta \\leq 0,\n\\frac{g(- \\delta \\mid w)}{g(\\mid w)} \\leq M \\right\\}} \\frac{g(- \\delta\n\\mid w)}{g(\\mid w)}.\\]provides strategy implementing shift level given\nobservation \\((a_i, w_i)\\), thereby allowing observations shifted \nappropriate value, whether \\(\\delta_{\\text{min}}\\), \\(\\delta\\), \n\\(\\delta_{\\text{max}}\\). tmle3shift\npackage implements functions shift_additive_bounded \nshift_additive_bounded_inv, define variation strategy:\n\\[\\begin{equation}\n  \\delta(, w) =\n    \\begin{cases}\n      \\delta, & C(\\delta) \\leq M \\\\\n      0, \\text{otherwise} \\\\\n    \\end{cases},\n  \\tag{9.13}\n\\end{equation}\\]\ncorresponding intervention natural value treatment \\(= \\)\nshifted value \\(\\delta\\) ratio \\(C(\\delta)\\) \npost-intervention density \\(g(- \\delta \\mid w)\\) natural treatment\ndensity \\(g(\\mid w)\\) exceed bound \\(M\\). \\(C(\\delta)\\) exceeds \nbound \\(M\\), stochastic intervention exempts given unit treatment\nmodification, leaving natural value treatment \\(= \\).","code":""},{"path":"shift.html","id":"initializing-vimshift-through-its-tmle3_spec","chapter":"9 Stochastic Treatment Regimes","heading":"9.7.1 Initializing vimshift through its tmle3_Spec","text":"start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle_shift. specify argument shift_grid = seq(-1, 1, = 1)\ninitializing tmle3_Spec object communicate ’re interested\nassessing mean counterfactual outcome grid shifts \\(\\delta \\\\{-1, 0, 1\\}\\) scale treatment \\(\\) (n.b., make arbitrary\nchoice shift values example).seen , tmle_vimshift specification object (like tmle3_Spec\nobjects) store data specific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally (see tmle3 documentation details).","code":"\n# what's the grid of shifts we wish to consider?\ndelta_grid <- seq(-1, 1, 1)\n\n# initialize a tmle specification\ntmle_spec <- tmle_vimshift_delta(\n  shift_grid = delta_grid,\n  max_shifted_ratio = 2\n)"},{"path":"shift.html","id":"targeted-estimation-of-stochastic-interventions-effects-1","chapter":"9 Stochastic Treatment Regimes","heading":"9.7.2 Targeted Estimation of Stochastic Interventions Effects","text":"One may walk step--step procedure fitting TML estimator\nmean counterfactual outcome shift grid, using \nmachinery exposed tmle3 R package.One may invoke tmle3 wrapper function (user-facing convenience utility)\nfit series TML estimators (one parameter defined grid\ndelta) single function call:Remark: print method resultant tmle_fit object conveniently\ndisplays results computing TML estimator.","code":"tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)\n\nIter: 1 fn: 547.4323     Pars:  0.9998973 0.0001027\nIter: 2 fn: 547.4323     Pars:  0.99997059 0.00002941\nsolnp--> Completed in 2 iterations\ntmle_fit\nA tmle3_Fit that took 1 step(s)\n         type          param init_est tmle_est       se  lower  upper\n1:        TSM  E[Y_{A=NULL}]   0.5681   0.5718 0.021416 0.5299 0.6138\n2:        TSM  E[Y_{A=NULL}]   0.6975   0.6975 0.022996 0.6524 0.7426\n3:        TSM  E[Y_{A=NULL}]   0.8167   0.8155 0.017110 0.7819 0.8490\n4: MSM_linear MSM(intercept)   0.6941   0.6949 0.019107 0.6575 0.7324\n5: MSM_linear     MSM(slope)   0.1243   0.1218 0.008603 0.1049 0.1387\n   psi_transformed lower_transformed upper_transformed\n1:          0.5718            0.5299            0.6138\n2:          0.6975            0.6524            0.7426\n3:          0.8155            0.7819            0.8490\n4:          0.6949            0.6575            0.7324\n5:          0.1218            0.1049            0.1387"},{"path":"shift.html","id":"estimation-and-inference-with-marginal-structural-models","chapter":"9 Stochastic Treatment Regimes","heading":"9.7.3 Estimation and Inference with Marginal Structural Models","text":"can challenging select value shift parameter \\(\\delta\\) \nadvance. One solution specify grid shifts \\(\\delta\\) used\ndefining set related stochastic interventions (Hejazi, van der Laan, et al., 2020).\nconsider estimating counterfactual mean \\(\\psi_n\\) several\nchoices \\(\\delta\\), single summary measure estimated quantities can\nestablished working marginal structural models (MSMs). Summarizing\nestimates \\(\\psi_n\\) working MSM allows inference trend\nappearing grid \\(\\delta\\), may evaluating simple\nhypothesis test slope parameter \\(\\beta_0\\) working MSM. Consider \ngrid \\(\\delta\\), \\(\\{\\delta_1, \\ldots, \\delta_k\\}\\), corresponding \ncounterfactual means \\(\\{\\psi_{\\delta_1}, \\ldots, \\psi_{\\delta_k}\\}\\). Next, let\n\\(\\psi(\\delta) = (\\psi_{\\delta}: \\delta)\\) denote grid \ncounterfactual means grid defined \\(\\delta\\) let \\(\\psi_n(\\delta)\\)\ndenote TML estimators \\(\\psi(\\delta)\\). MSM summarizing change \n\\(\\psi_n\\) function \\(\\delta\\) may expressed \\(m_{\\beta}(\\psi_{\\delta}) = \\beta_0 + \\beta_1 \\delta\\). simple working model summarizes changes \n\\(\\psi_{\\delta}\\) function parameters \\((\\beta_0, \\beta_1)\\), \nlatter slope line resulting projecting counterfactual\nmeans onto simple two-parameter working model.general expression MSM \\(m_{\\beta}(\\delta)\\) \\(\\beta_0 = \\text{argmin}_{\\beta} \\sum_{\\delta}(\\psi_{\\delta}(P_0) - m_{\\beta}(\\delta))^2 h(\\delta)\\), solution estimating equation\n\\[u(\\beta, (\\psi_{\\delta}: \\delta)) = \\sum_{\\delta}h(\\delta)\n\\left(\\psi_{\\delta}(P_0) - m_{\\beta}(\\delta) \\right) \\frac{d}{d\\beta}\nm_{\\beta}(\\delta) = 0.\\]\n\nNow, say, \\(\\psi = (\\psi(\\delta): \\delta)\\) d-dimensional. may express \nEIF MSM parameter \\(\\beta_0\\) terms EIFs individual\ncounterfactual means:\n\\[\\begin{align}\n   D_{\\beta}(O) = &\\left(\\sum_{\\delta} h(\\delta) \\frac{d}{d\\beta}\n   m_{\\beta}(\\delta) \\frac{d}{d\\beta} m_{\\beta}(\\delta)^t \\right)^{-1}\n   \\\\ \\nonumber\n   &\\sum_{\\delta} h(\\delta) \\frac{d}{d\\beta} m_{\\beta}(\\delta)\n   D_{\\psi_{\\delta}}(O).\n   \\tag{9.14}\n\\end{align}\\]\n, Equation (9.14), first component dimension\n\\(d \\times d\\) second dimension \\(d \\times 1\\). , \nassume linear working MSM; however, analogous procedure may applied \nworking MSMs based GLMs., utilized straightforward application delta method obtain\nEIF \\(\\beta\\). Inference parameter working MSM follows \nevaluation EIF \\(D_{\\beta}\\), expressed terms EIFs \ncorresponding estimates \\(\\psi_n(\\delta)\\). limit distribution \n\\(\\beta_n\\) may expressed \\[\\sqrt{n}(\\beta_n - \\beta_0) \\N(0, \\Sigma),\\]\n\\(\\Sigma\\) empirical covariance matrix \\(D_{\\beta}(O)\\). ,\ncan estimate trend counterfactual means across \ngrid \\(\\delta\\), can also evaluate whether slope estimate \nstatistically significant, terms hypothesis tests form \\((H_0: \\beta_0 = 0; H_1: \\beta_0 \\neq 0)\\) equivalent Wald-style confidence\nintervals. Note estimator \\(\\beta_n\\) parameter \\(\\beta_0\\) \nMSM asymptotically linear (, fact, TML estimator) consequence \nconstruction individual TML estimators.strategy just discussed constructs estimate \\(\\beta_n\\) working MSM\nslope \\(\\beta_0\\) first evaluating TML estimates counterfactual\nmeans \\(\\psi_{n,\\delta}\\) grid \\(\\{\\delta_1, \\ldots, \\delta_k\\}\\); however,\nnecessarily best strategy, especially giving consideration\nestimation stability small samples. smaller samples, may prudent\nperform TML estimation targeting directly parameter \\(\\beta_0\\), opposed\nconstructing applying delta method several independently\ntargeted TML estimates., consider TML estimator targeting \\(\\beta_0\\) (parameter \nworking MSM \\(m_{\\beta}\\)), uses targeting update step form\n\\(\\overline{Q}_{n, \\epsilon}(,W) = \\overline{Q}_n(,W) + \\epsilon (H_{\\beta_0}(g), H_{\\beta_1}(g))\\), \\(\\delta\\), \\(H_{\\beta_0}(g)\\) \nauxiliary covariate \\(\\beta_0\\) (intercept) \\(H_{\\beta}(g)\\) \nauxiliary covariate \\(\\beta_1\\) (slope). Note forms \nauxiliary covariates depend EIF \\(D_{\\beta}\\). TML estimator avoids\nestimating \\(\\psi_{\\delta}\\) grid directly, instead cleverly\nconcatenating auxiliary covariates appropriate \\(\\beta_0\\)\n\\(\\beta_1\\). construct targeted maximum likelihood estimator \ndirectly targets parameters working MSM, may use \ntmle_vimshift_msm Spec (instead tmle_vimshift_delta Spec).","code":"\n# initialize a tmle specification\ntmle_msm_spec <- tmle_vimshift_msm(\n  shift_grid = delta_grid,\n  max_shifted_ratio = 2\n)\n\n# fit the TML estimator and examine the results\ntmle_msm_fit <- tmle3(tmle_msm_spec, data, node_list, learner_list)\ntmle_msm_fit"},{"path":"shift.html","id":"example-with-the-wash-benefits-data","chapter":"9 Stochastic Treatment Regimes","heading":"9.7.4 Example with the WASH Benefits Data","text":"complete walk , let’s turn using stochastic interventions \ninvestigate data WASH Benefits trial. start, let’s load \ndata, convert columns class numeric, take quick look itNext, specify NPSEM via node_list object. example analysis,\n’ll consider outcome weight--height Z-score (previous\nchapters), intervention interest mother’s age time \nchild’s birth, take covariates potential confounders.consider counterfactual weight--height Z-score shifts \nage mother child’s birth, interpret estimates \nparameter? simplify interpretation, consider shift just year \nmother’s age (.e., \\(\\delta = 1\\)); setting, stochastic\nintervention correspond policy advocating potential mothers\ndefer child single calendar year, possibly implemented \nencouragement design deployed clinical setting.example, ’ll use variable importance strategy considering \ngrid stochastic interventions evaluate weight--height Z-score \nshift mother’s age two years (\\(\\delta = -2\\)) two years\n(\\(\\delta = 2\\)). , simply initialize Spec tmle_vimshift_delta\njust previous example:Prior running analysis, ’ll modify learner_list object \ncreated density estimation procedure rely \nlocation-scale conditional density estimation procedure, nonparametric\nconditional density approach based highly adaptive lasso (Benkeser van der Laan, 2016; Coyle et al., 2022; Dı́az van der Laan, 2011; Hejazi, Coyle, et al., 2020; Hejazi, Benkeser, et al., 2022)\ncurrently unable accommodate larger datasets.made preparations, ’re now ready estimate \ncounterfactual mean weight--height Z-score small grid \nshifts mother’s age child’s birth. Just , \nsimple call tmle3 wrapper function:","code":"washb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\nwashb_data <- washb_data[!is.na(momage), lapply(.SD, as.numeric)]\nhead(washb_data, 3)\n     whz tr fracode month aged sex momage momedu momheight hfiacat Nlt18 Ncomp\n1:  0.00  1       4     9  268   2     30      2     146.4       1     3    11\n2: -1.16  1       4     9  286   2     25      2     148.8       3     2     4\n3: -1.05  1      20     9  264   2     25      2     152.2       1     1    10\n   watmin elec floor walls roof asset_wardrobe asset_table asset_chair\n1:      0    1     0     1    1              0           1           1\n2:      0    1     0     1    1              0           1           0\n3:      0    0     0     1    1              0           0           1\n   asset_khat asset_chouki asset_tv asset_refrig asset_bike asset_moto\n1:          1            0        1            0          0          0\n2:          1            1        0            0          0          0\n3:          0            1        0            0          0          0\n   asset_sewmach asset_mobile\n1:             0            1\n2:             0            1\n3:             0            1\nnode_list <- list(\n  W = names(washb_data)[!(names(washb_data) %in%\n    c(\"whz\", \"momage\"))],\n  A = \"momage\",\n  Y = \"whz\"\n)\n# initialize a tmle specification for the variable importance parameter\nwashb_vim_spec <- tmle_vimshift_delta(\n  shift_grid = c(-2, 2),\n  max_shifted_ratio = 2\n)\n# we need to turn on cross-validation for the HOSE learner\ncv_hose_hal_lrnr <- Lrnr_cv$new(\n  learner = hose_hal_lrnr,\n  full_fit = TRUE\n)\n\n# modify learner list, using existing SL for Q fit\nlearner_list <- list(Y = sl_reg_lrnr, A = cv_hose_hal_lrnr)\nwashb_tmle_fit <- tmle3(washb_vim_spec, washb_data, node_list, learner_list)\nwashb_tmle_fit"},{"path":"shift.html","id":"exercises-4","chapter":"9 Stochastic Treatment Regimes","heading":"9.8 Exercises","text":"","code":""},{"path":"shift.html","id":"the-ideas-in-action","chapter":"9 Stochastic Treatment Regimes","heading":"9.8.1 The Ideas in Action","text":"Exercise 9.1  Set sl3 library algorithms Super Learner simple,\ninterpretable library use new library estimate counterfactual\nmean mother’s age child’s birth (momage) shift \\(\\delta = 0\\).\ncounterfactual mean equate terms observed data?Solution. ForthcomingExercise 9.2  Using grid values shift parameter \\(\\delta\\) (e.g., \\(\\{-1, 0, +1\\}\\)),\nrepeat analysis variable chosen preceding question,\nsummarizing trend sequence shifts using marginal structural\nmodel.Solution. ForthcomingExercise 9.3  Repeat preceding analysis, using grid shifts, instead\ndirectly targeting parameters marginal structural model. Interpret\nresults – , slope marginal structural model\ntell us trend across chosen sequence shifts?Solution. Forthcoming","code":""},{"path":"shift.html","id":"review-of-key-concepts-2","chapter":"9 Stochastic Treatment Regimes","heading":"9.8.2 Review of Key Concepts","text":"Exercise 9.4  Describe two (equivalent) ways causal effects stochastic\ninterventions may interpreted.Solution. ForthcomingExercise 9.5  can information provided estimates across several shifts \\(\\{ \\delta_1, \\ldots, \\delta_k \\}\\) marginal structural model parameter\nsummarizing trend \\(\\delta\\) used enrich interpretation \nfindings?Solution. ForthcomingExercise 9.6  advantages, , targeting directly parameters \nmarginal structural model?Solution. Forthcoming","code":""},{"path":"causal-mediation-analysis.html","id":"causal-mediation-analysis","chapter":"10 Causal Mediation Analysis","heading":"10 Causal Mediation Analysis","text":"Nima HejaziFeaturing tmle3mediate R\npackage.Learning ObjectivesExamine presence post-treatment mediating variables can complicate\ncausal analysis, direct indirect effects can defined \nresolve complications.Describe essential similarities differences direct \nindirect causal effects, including definition terms stochastic\ninterventions.Differentiate joint interventions required define direct indirect\neffects static, dynamic, stochastic interventions yield\ntotal causal effects.Describe assumptions needed identification natural direct \nindirect effects, well limitations effect definitions.Estimate natural direct indirect effects binary treatment using\ntmle3mediate R package.Differentiate population intervention direct indirect effects \nstochastic interventions natural direct indirect effects,\nincluding differences assumptions required identification.Estimate population intervention direct effect binary treatment\nusing tmle3mediate R package.","code":""},{"path":"causal-mediation-analysis.html","id":"causal-mediation-analysis-1","chapter":"10 Causal Mediation Analysis","heading":"10.1 Causal Mediation Analysis","text":"applications ranging biology epidemiology economics \npsychology, scientific inquires often concerned ascertaining effect\ntreatment outcome variable particular pathways \ntwo. presence post-treatment intermediate variables affected \nexposure (, mediators), path-specific effects allow complex,\nmechanistic relationships teased apart. causal effects \nwide interest definition identification object \nstudy statistics nearly century – indeed, earliest examples \nmodern causal mediation analysis can traced back work path analysis\n(Wright, 1934). recent decades, renewed interest resulted \nformulation novel direct indirect effects within potential\noutcomes nonparametric structural equation modeling frameworks\n(Dawid, 2000; Pearl, 1995, 2009; JM Robins, 1986; Spirtes et al., 2000). Generally, indirect effect (IE) portion \ntotal effect found work mediating variables, direct\neffect (DE) encompasses components total effect, including\neffect treatment directly outcome effect\npaths explicitly involving mediators. mechanistic\nknowledge conveyed direct indirect effects can used improve\nunderstanding treatments may efficacious.Modern approaches causal inference allowed significant advances \nmethodology traditional path analysis, overcoming significant\nrestrictions imposed use parametric modeling approaches\n(VanderWeele, 2015). Using distinct frameworks,\nRobins Greenland (1992) Pearl (2001) provided equivalent\nnonparametric decompositions average treatment effect natural\ndirect indirect effects. VanderWeele (2015) provides \ncomprehensive overview classical causal mediation analysis. provide \nalternative perspective, focusing instead construction efficient\nestimators quantities, appeared recently\n(Tchetgen Tchetgen Shpitser, 2012; Zheng van der Laan, 2012), well flexible\ndirect indirect definitions based upon stochastic interventions\n(Dı́az Hejazi, 2020).","code":""},{"path":"causal-mediation-analysis.html","id":"data-structure-and-notation-2","chapter":"10 Causal Mediation Analysis","heading":"10.2 Data Structure and Notation","text":"Let us return familiar sample \\(n\\) units \\(O_1, \\ldots, O_n\\), \nnow consider slightly complex data structure \\(O = (W, , Z, Y)\\) \ngiven observational unit. , \\(W\\) represents vector observed\ncovariates, \\(\\) binary continuous treatment, \\(Y\\) binary continuous\noutcome; new post-treatment variable \\(Z\\) represents (possibly\nmultivariate) set mediators. Avoiding assumptions unsupported background\nscientific knowledge, assume \\(O \\sim P_0 \\\\M\\), \\(\\M\\) \nnonparametric statistical model places assumptions form \ndata-generating distribution \\(P_0\\).preceding chapters, structural causal model (SCM) (Pearl, 2009)\nhelps formalize definition counterfactual variables:\n\\[\\begin{align}\n  W &= f_W(U_W) \\\\ \\nonumber\n  &= f_A(W, U_A) \\\\ \\nonumber\n  Z &= f_Z(W, , U_Z) \\\\ \\nonumber\n  Y &= f_Y(W, , Z, U_Y).\n  \\tag{10.1}\n\\end{align}\\]\nset equations\nconstitutes mechanistic model generating observed data \\(O\\); furthermore,\nSCM encodes several fundamental assumptions. Firstly, implicit\ntemporal ordering: \\(W\\) occurs first, depending exogenous factors \\(U_W\\);\n\\(\\) happens next, based \\(W\\) exogenous factors \\(U_A\\); come \nmediators \\(Z\\), depend \\(\\), \\(W\\), another set exogenous factors\n\\(U_Z\\); finally appears outcome \\(Y\\). assume neither access set\nexogenous factors \\(\\{U_W, U_A, U_Z, U_Y\\}\\) knowledge forms \ndeterministic generating functions \\(\\{f_W, f_A, f_Z, f_Y\\}\\). practice, \navailable knowledge data-generating experiment incorporated\nmodel – example, data randomized controlled trial\n(RCT), form \\(f_A\\) may known. SCM corresponds following DAG:factorizing likelihood data \\(O\\), can express \\(p_0\\), \ndensity \\(O\\) respect product measure, evaluated \nparticular observation \\(o\\), terms several orthogonal components:\n\\[\\begin{align}\n  p_0(o) = &q_{0,Y}(y \\mid Z = z, = , W = w) \\\\ \\nonumber\n    &q_{0,Z}(z \\mid = , W = w) \\\\ \\nonumber\n    &g_{0,}(\\mid W = w) \\\\ \\nonumber\n    &q_{0,W}(w).\\\\ \\nonumber\n  \\tag{10.2}\n\\end{align}\\]\nEquation (10.2), \\(q_{0, Y}\\) \nconditional density \\(Y\\) given \\(\\{Z, , W\\}\\), \\(q_{0, Z}\\) conditional\ndensity \\(Z\\) given \\(\\{, W\\}\\), \\(g_{0, }\\) conditional density \\(\\)\ngiven \\(W\\), \\(q_{0, W}\\) marginal density \\(W\\). convenience \nconsistency notation, define \\(\\overline{Q}_Y(Z, , W) := \\E[Y \\mid Z, , W]\\) \\(g(\\mid W) := \\P(\\mid W)\\) (.e., propensity score).explicitly excluded potential confounders mediator-outcome\nrelationship affected exposure (.e., variables affected \\(\\) affecting\n\\(Z\\) \\(Y\\)). Mediation analysis presence variables \nchallenging (Avin et al., 2005); thus, efforts develop\ndefinitions causal direct indirect effects explicitly assume absence\nconfounders. Without assumptions, common mediation parameters\n(natural direct indirect effects) identified presence\nconfounding, though Tchetgen Tchetgen VanderWeele (2014) discuss monotonicity\nassumption may useful justified available scientific knowledge\nsystem study. interested reader may wish consult recent\nadvances vast quickly growing literature causal mediation\nanalysis, including interventional direct indirect effects\n(Didelez et al., 2006; Lok, 2016; Nguyen et al., 2019; Rudolph et al., 2017; VanderWeele et al., 2014; Vansteelandt Daniel, 2017),\nwhose identification robust complex form post-treatment\nconfounding. Within thread literature, Dı́az et al. (2020) \nBenkeser Ran (2021) provide considerations nonparametric effect\ndecompositions efficiency theory, Hejazi, Rudolph, et al. (2022) formulate \nnovel class effects utilizing stochastic interventions.","code":""},{"path":"causal-mediation-analysis.html","id":"defining-the-natural-direct-and-indirect-effects","chapter":"10 Causal Mediation Analysis","heading":"10.3 Defining the Natural Direct and Indirect Effects","text":"","code":""},{"path":"causal-mediation-analysis.html","id":"decomposing-the-average-treatment-effect","chapter":"10 Causal Mediation Analysis","heading":"10.3.1 Decomposing the Average Treatment Effect","text":"natural direct indirect effects arise decomposition ATE:\n\\[\\begin{align*}\n  \\E[Y(1) - Y(0)] =\n    &\\underbrace{\\E[Y(1, Z(0)) - Y(0, Z(0))]}_{\\text{NDE}} \\\\ &+\n    \\underbrace{\\E[Y(1, Z(1)) - Y(1, Z(0))]}_{\\text{NIE}}.\n\\end{align*}\\]\nparticular, natural indirect effect (NIE) measures effect \ntreatment \\(\\\\{0, 1\\}\\) outcome \\(Y\\) mediators \\(Z\\), \nnatural direct effect (NDE) measures effect treatment \noutcome pathways. Identification natural direct \nindirect effects requires following non-testable causal assumptions. Note\nstandard assumptions consistency interference (.e., SUTVA\n(Rubin, 1978, 1980)) hold owing fact (1)\nSCM consider restricted give rise independent \nidentically distributed (iid) units; (2) consistency implied property\nSCM, counterfactuals derived quantities (opposed primitive\nquantities potential outcomes framework); Pearl (2010) provides \nilluminating discussion latter point.Definition 10.1  (Exchangeability) \\(Y(, z) \\indep (, Z) \\mid W\\), implies \\(\\E\\{Y(, z) \\mid =, W=w, Z=z\\} \\equiv \\E\\{Y(, z) \\mid W=w\\}\\). special, \nrestrictive case standard assumption unmeasured counfounding \npresence mediators. analogous randomization assumption simply \nstandard randomization assumption applied joint intervention \ntreatment \\(\\) mediators \\(Z\\).Definition 10.2  (Treatment Positivity) \\(\\\\mathcal{}\\) \\(w \\\\mathcal{W}\\), conditional probability\ntreatment \\(g(\\mid w)\\) bounded away limits unit interval\nsmall factor \\(\\xi > 0\\). precisely, \\(\\xi < g(\\mid w) < 1 - \\xi\\). \nmirrors standard positivity assumption required static interventions,\ndiscussed previously.Definition 10.3  (Mediator Positivity) \\(z \\\\mathcal{Z}\\), \\(\\\\mathcal{}\\), \\(w \\\\mathcal{W}\\), \nconditional mediator density must bounded away zero small factor\n\\(\\epsilon > 0\\), specifically, \\(\\epsilon < q_{0,Z}(z \\mid , w)\\). Essentially,\nrequires conditional mediator density bounded away zero\n\\(\\{z, , w\\}\\) joint support \\(\\mathcal{Z} \\times \\mathcal{} \\times \\mathcal{W}\\), say must possible observe \ngiven mediator value across strata defined treatment \\(\\) \nbaseline covariates \\(W\\). less restrictive form assumption also\npossible – specifically, ratio mediator densities \ntreatment contrasts bounded two realizations mediator density\ndiffering treatment contrasts.Definition 10.4  (Cross-world Counterfactual Independence) \\(\\neq '\\), \\(, ' \\\\mathcal{}\\), \\(z \\\\mathcal{Z}\\),\n\\(Y(', z)\\) must independent \\(Z()\\), given \\(W\\). , counterfactual\noutcome treatment contrast \\(' \\\\mathcal{}\\) counterfactual\nmediator value \\(Z() \\\\mathcal{Z}\\) (alternative contrast \\(\\\\mathcal{}\\)) must observable. term “cross-world” refers two\ncounterfactuals \\(Z()\\) \\(Y(', z)\\) existing two differing treatment\ncontrasts. Though joint distribution well-defined, \ncounterfactuals can never jointly realized.first three assumptions may familiar based analogs \nsimpler settings, cross-world independence requirement unique \nidentification natural direct indirect effects. assumption\nresolves challenging complication identification path-specific\neffects, termed “recanting witness” \nAvin et al. (2005), introduce graphical resolution equivalent \nassumption. independence counterfactuals indexed distinct\ninterventions , fact, serious limitation scientific relevance \neffect definitions, results NDE NIE unidentifiable\nrandomized trials (Robins Richardson, 2010), implying corresponding\nscientific claims falsified experimentation (Dawid, 2000; Popper, 1934) , consequently, directly contradicting foundational\npillar scientific method.many attempts made weaken last assumption\n(Imai et al., 2010; Petersen et al., 2006; Vansteelandt et al., 2012; Vansteelandt VanderWeele, 2012), results either impose stringent modeling\nassumptions, propose alternative interpretations natural effects, \nprovide limited degree additional flexibility developing conditions \nmay easily satisfied. example, Petersen et al. (2006) weaken \nassumption requiring conditional means (rather distinct\ncounterfactuals) adopt view natural direct effect weighted\naverage another type direct effect, controlled direct effect. \nmotivated reader may wish examine details independently. \nnext review estimation NDE NIE, remain widely used modern\napplications causal mediation analysis.","code":""},{"path":"causal-mediation-analysis.html","id":"estimating-the-natural-direct-effect","chapter":"10 Causal Mediation Analysis","heading":"10.3.2 Estimating the Natural Direct Effect","text":"NDE defined \n\\[\\begin{align*}\n  \\psi_{\\text{NDE}} =&~\\E[Y(1, Z(0)) - Y(0, Z(0))] \\\\\n  =& \\sum_w \\sum_z\n  [\\underbrace{\\E(Y \\mid = 1, z, w)}_{\\overline{Q}_Y(= 1, z, w)} -\n  \\underbrace{\\E(Y \\mid = 0, z, w)}_{\\overline{Q}_Y(= 0, z, w)}] \\\\\n  &\\times \\underbrace{p(z \\mid = 0, w)}_{q_Z(Z \\mid 0, w))}\n  \\underbrace{p(w)}_{q_W},\n\\end{align*}\\]\nlikelihood factors arise factorization joint\nlikelihood:\n\\[\\begin{equation*}\n  p(w, , z, y) = \\underbrace{p(y \\mid w, , z)}_{q_Y(, W, Z)}\n  \\underbrace{p(z \\mid w, )}_{q_Z(Z \\mid , W)}\n  \\underbrace{p(\\mid w)}_{g(\\mid W)}\n  \\underbrace{p(w)}_{q_W}.\n\\end{equation*}\\]process estimating NDE begins constructing \\(\\overline{Q}_{Y, n}\\),\nestimate conditional mean outcome, given \\(Z\\), \\(\\), \\(W\\).\nestimate conditional mean hand, predictions \nquantities \\(\\overline{Q}_Y(Z, 1, W)\\) (setting \\(= 1\\)) ,\nlikewise, \\(\\overline{Q}_Y(Z, 0, W)\\) (setting \\(= 0\\)) readily obtained. \ndenote difference conditional means \\(\\overline{Q}_{\\text{diff}} = \\overline{Q}_Y(Z, 1, W) - \\overline{Q}_Y(Z, 0, W)\\), \nfunctional parameter data distribution. \\(\\overline{Q}_{\\text{diff}}\\)\ncaptures differences conditional mean \\(Y\\) across contrasts \\(\\).procedure constructing targeted maximum likelihood (TML) estimator \nNDE treats \\(\\overline{Q}_{\\text{diff}}\\) nuisance parameter,\nregressing estimate \\(\\overline{Q}_{\\text{diff}, n}\\) baseline covariates\n\\(W\\), among observations control condition (.e., \n\\(= 0\\) observed); goal step remove part marginal\nimpact \\(Z\\) \\(\\overline{Q}_{\\text{diff}}\\), since covariates \\(W\\) precede\nmediators \\(Z\\) time. Regressing difference \\(W\\) among controls\nrecovers expected \\(\\overline{Q}_{\\text{diff}}\\), setting \nindividuals treated falling control condition \\(= 0\\). \nresidual additive effect \\(Z\\) \\(\\overline{Q}_{\\text{diff}}\\) removed\nTML estimation step using auxiliary (“clever”) covariate,\naccounts mediators \\(Z\\). auxiliary covariate takes form\\[\\begin{equation*}\n  C_Y(q_Z, g)(O) = \\Bigg\\{\\frac{\\mathbb{}(= 1)}{g(1 \\mid W)}\n  \\frac{q_Z(Z \\mid 0, W)}{q_Z(Z \\mid 1, W)} -\n  \\frac{\\mathbb{}(= 0)}{g(0 \\mid W)} \\Bigg\\} \\ .\n\\end{equation*}\\]\nBreaking , \\(\\mathbb{}(= 1) / g(1 \\mid W)\\) inverse propensity\nscore weight \\(= 1\\) , likewise, \\(\\mathbb{}(= 0) / g(0 \\mid W)\\) \ninverse propensity score weight \\(= 0\\). middle term ratio \nconditional densities mediator control (\\(= 0\\)) treatment\n(\\(= 1\\)) conditions (n.b., recall mediator positivity condition ).subtle appearance ratio conditional densities concerning –\ntools estimate quantities sparse statistics literature\n(Dı́az van der Laan, 2011; Hejazi, Benkeser, et al., 2022), unfortunately, problem still\ncomplicated (computationally taxing) \\(Z\\) high-dimensional. \nratio conditional densities required, convenient\nre-parametrization may achieved, ,\n\\[\\begin{equation*}\n  \\frac{p(= 0 \\mid Z, W)}{g(0 \\mid W)}\n  \\frac{g(1 \\mid W)}{p(= 1 \\mid Z, W)} \\ .\n\\end{equation*}\\]\nGoing forward, denote re-parameterized conditional probability\nfunctional \\(e(\\mid Z, W) := p(\\mid Z, W)\\). re-parameterization\ntechnique used Zheng van der Laan (2012), Tchetgen Tchetgen (2013),\nDı́az Hejazi (2020), Dı́az et al. (2020), Hejazi, Rudolph, et al. (2022) similar\ncontexts. reformulation particularly useful fact reduces\nestimation problem one requiring estimation conditional\nmeans, opening door use wide range machine learning\nalgorithms, discussed previously.Underneath hood, mean outcome difference \\(\\overline{Q}_{\\text{diff}}\\)\n\\(e(\\mid Z, W)\\), conditional probability \\(\\) given \\(Z\\) \\(W\\), \nused constructing auxiliary covariate TML estimation. nuisance\nparameters play important role bias-correcting update step TML\nestimation procedure.","code":""},{"path":"causal-mediation-analysis.html","id":"estimating-the-natural-indirect-effect","chapter":"10 Causal Mediation Analysis","heading":"10.3.3 Estimating the Natural Indirect Effect","text":"Derivation estimation NIE analogous NDE. Recall\nNIE effect \\(\\) \\(Y\\) mediator \\(Z\\).\ncounterfactual quantity, may expressed \\(\\E(Y(Z(1), 1) - \\E(Y(Z(0), 1)\\), corresponds difference conditional mean \\(Y\\)\ngiven \\(= 1\\) \\(Z(1)\\) (values mediator take \\(= 1\\)) \nconditional expectation \\(Y\\) given \\(= 1\\) \\(Z(0)\\) (values \nmediator take \\(= 0\\)).NDE, re-parameterization can used replace \\(q_Z(Z \\mid , W)\\)\n\\(e(\\mid Z, W)\\) estimation process, avoiding estimation \npossibly multivariate conditional density. However, case, mediated\nmean outcome difference, previously computed regressing\n\\(\\overline{Q}_{\\text{diff}}\\) \\(W\\) among control units (\\(= 0\\) \nobserved) instead replaced two-step process. First, \\(\\overline{Q}_Y(Z, 1, W)\\), conditional mean \\(Y\\) given \\(Z\\) \\(W\\) \\(= 1\\), regressed\n\\(W\\), among treated units (.e., \\(= 1\\) observed). ,\nquantity, \\(\\overline{Q}_Y(Z, 1, W)\\) regressed \\(W\\), \ntime among control units (.e., \\(= 0\\) observed). mean\ndifference two functionals data distribution valid\nestimator NIE. can thought additive marginal effect \ntreatment conditional mean \\(Y\\) given \\((W, = 1, Z)\\) \neffect \\(Z\\). , case NIE, estimand\n\\(\\psi_{\\text{NIE}}\\) different, estimation techniques useful \nconstructing efficient estimators NDE come play.","code":""},{"path":"causal-mediation-analysis.html","id":"the-population-intervention-direct-and-indirect-effects","chapter":"10 Causal Mediation Analysis","heading":"10.4 The Population Intervention Direct and Indirect Effects","text":"times, natural direct indirect effects may prove limiting, \neffect definitions based static interventions (.e., setting\n\\(= 0\\) \\(= 1\\)), may unrealistic real-world interventions. \ncases, one may turn instead population intervention direct effect\n(PIDE) population intervention indirect effect (PIIE), based\ndecomposing effect population intervention effect (PIE) \nflexible stochastic interventions (Dı́az Hejazi, 2020).previously discussed stochastic interventions considering \nintervene continuous-valued treatments; however, intervention\nschemes may applied manner treatment variables.\nparticular type stochastic intervention well-suited working binary\ntreatments incremental propensity score intervention (IPSI), first\nproposed Kennedy (2019). interventions \ndeterministically set treatment level observed unit fixed\nquantity (.e., setting \\(= 1\\)), instead alter odds receiving \ntreatment fixed amount (\\(0 \\leq \\delta \\leq \\infty\\)) individual.\nparticular, intervention takes form\n\\[\\begin{equation*}\n  g_{\\delta}(1 \\mid w) = \\frac{\\delta g(1 \\mid w)}{\\delta g(1 \\mid w) + 1\n  - g(1\\mid w)},\n\\end{equation*}\\]\nscalar \\(0 < \\delta < \\infty\\) specifies change odds \nreceiving treatment. described Dı́az Hejazi (2020) context causal\nmediation analysis, identification assumptions required PIDE \nPIIE significantly lax required NDE NIE. \nidentification assumptions include following. Importantly, assumption \ncross-world counterfactual independence required.Definition 10.5  (Conditional Exchangeability Treatment Mediators) Assume \\(\\E\\{Y(, z) \\mid Z, , W\\} = \\E\\{Y(, z) \\mid Z, W\\}~\\forall~(, z) \\\\mathcal{} \\times \\mathcal{Z}W\\). assumption \nstronger implied assumption \\(Y(, z) \\indep (,Z) \\mid W\\),\noriginally proposed Vansteelandt VanderWeele (2012) identification mediated\neffects among treated. introducing assumption Dı́az Hejazi (2020) state\n“assumption satisfied pre-exposure \\(W\\) \nrandomized experiment exposure mediators randomized. Thus, \ndirect effect population intervention corresponds contrasts \ntreatment regimes randomized experiment via interventions \\(\\) \\(Z\\),\nunlike natural direct effect average treatment effect\n(Robins Richardson, 2010).”Definition 10.6  (Common Support Treatment Mediators) Assume \\(\\text{supp}\\{g_{\\delta}(\\cdot \\mid w)\\} \\subseteq \\text{supp}\\{g(\\cdot \\mid w)\\}~\\forall~w \\\\mathcal{W}\\). assumption \nstandard requires post-intervention value \\(\\) supported\ndata. Note significantly weaker treatment \nmediator positivity conditions required natural direct indirect\neffects, direct consequence using stochastic (rather static)\ninterventions.","code":""},{"path":"causal-mediation-analysis.html","id":"decomposing-the-population-intervention-effect","chapter":"10 Causal Mediation Analysis","heading":"10.4.1 Decomposing the Population Intervention Effect","text":"may decompose population intervention effect (PIE) terms \npopulation intervention direct effect (PIDE) population\nintervention indirect effect (PIIE):\n\\[\\begin{equation*}\n  \\mathbb{E}\\{Y(A_\\delta)\\} - \\mathbb{E}Y =\n    \\overbrace{\\mathbb{E}\\{Y(A_\\delta, Z(A_\\delta))\n      - Y(A_\\delta, Z)\\}}^{\\text{PIIE}} +\n    \\overbrace{\\mathbb{E}\\{Y(A_\\delta, Z) - Y(, Z)\\}}^{\\text{PIDE}}.\n\\end{equation*}\\]decomposition PIE sum population intervention direct\nindirect effects interpretation analogous corresponding\nstandard decomposition average treatment effect. sequel, \ncompute components direct indirect effects using\nappropriate estimators followsFor \\(\\E\\{Y(, Z)\\}\\), sample mean \\(\\frac{1}{n}\\sum_{=1}^n Y_i\\) \nconsistent;\\(\\E\\{Y(A_{\\delta}, Z)\\}\\), TML estimator effect joint\nintervention altering treatment mechanism mediation mechanism,\nbased proposal Dı́az Hejazi (2020); ,\\(\\E\\{Y(A_{\\delta}, Z_{A_{\\delta}})\\}\\), efficient estimator \neffect joint intervention treatment mediation mechanisms,\nper Kennedy (2019).","code":""},{"path":"causal-mediation-analysis.html","id":"estimating-the-effect-decomposition-term","chapter":"10 Causal Mediation Analysis","heading":"10.4.2 Estimating the Effect Decomposition Term","text":"described Dı́az Hejazi (2020), statistical functional identifying \ndecomposition term appears PIDE PIIE \\(\\E\\{Y(A_{\\delta}, Z)\\}\\), corresponds altering treatment mechanism keeping \nmediation mechanism fixed, \n\\[\\begin{equation*}\n  \\psi_0(\\delta) = \\int \\overline{Q}_{0,Y}(, z, w)\n    g_{0,\\delta}(\\mid w) p_0(z, w) d\\nu(, z, w),\n\\end{equation*}\\]\nTML estimator available. case \\(\\\\{0, 1\\}\\), \nefficient influence function (EIF) respect nonparametric\nstatistical model \\(\\mathcal{M}\\) \\(D_{\\delta}(o) = D^Y_{\\delta}(o) + D^A_{\\delta}(o) + D^{Z,W}_{\\delta}(o) - \\psi(\\delta)\\), \northogonal components EIF defined follows:\\(D^Y_{\\delta}(o) = \\{g_{\\delta}(\\mid w) / e(\\mid z, w)\\} \\{y - \\overline{Q}_{Y}(z,,w)\\}\\),\\(D^A_{\\delta}(o) = \\{\\delta\\phi(w) (- g(1 \\mid w))\\} / \\{(\\delta g(1 \\mid w) + g(0 \\mid w))^2\\}\\), \\(\\phi(w) := \\E\\{\\overline{Q}_{Y}(1, Z, W) - \\overline{Q}_{Y}(0, Z, W) \\mid W = w\\}\\),\\(D^{Z,W}_{\\delta}(o) = \\int_{\\mathcal{}} \\overline{Q}_{Y}(z, , w) g_{\\delta}(\\mid w) d\\kappa()\\).TML estimator may computed fluctuating initial estimates \nnuisance parameters solve EIF estimating equation. resultant\nTML estimator \n\\[\\begin{equation*}\n  \\psi_{n}^{\\star}(\\delta) = \\int_{\\mathcal{}} \\frac{1}{n} \\sum_{=1}^n\n  \\overline{Q}_{Y,n}^{\\star}(Z_i, , W_i)\n  g_{\\delta, n}^{\\star}(\\mid W_i) d\\kappa(),\n\\end{equation*}\\]\n\\(g_{\\delta,n}^{\\star}(\\mid w)\\) \\(\\overline{Q}_{Y,n}^{\\star}(z,,w)\\)\ngenerated regressions fluctuate (tilt) initial\nnuisance parameter estimates towards solutions score equations \\(n^{-1} \\sum_{=1}^n D^A_{\\delta}(O_i) = 0\\) \\(n^{-1} \\sum_{=1}^n D^Y_{\\delta}(O_i) = 0\\), respectively. TML estimator \\(\\psi_{n}^{\\star}(\\delta)\\) implemented\ntmle3mediate package. demonstrate use tmle3mediate obtain\n\\(\\E\\{Y(A_{\\delta}, Z)\\}\\) via TML estimator following worked code\nexamples.","code":""},{"path":"causal-mediation-analysis.html","id":"evaluating-the-direct-and-indirect-effects","chapter":"10 Causal Mediation Analysis","heading":"10.5 Evaluating the Direct and Indirect Effects","text":"now turn estimating natural direct indirect effects, well \npopulation intervention direct effect, using WASH Benefits data,\nintroduced earlier chapters. Let’s first load data:’ll next define baseline covariates \\(W\\), treatment \\(\\), mediators \\(Z\\),\noutcome \\(Y\\) nodes NPSEM via “Node List” object:, node_list encodes parents node – example, \\(Z\\) (\nmediators) parents \\(\\) (treatment) \\(W\\) (baseline confounders),\n\\(Y\\) (outcome) parents \\(Z\\), \\(\\), \\(W\\). ’ll also handle \nmissingness data invoking process_missing:’ll now construct ensemble learner using handful popular machine\nlearning algorithms:","code":"\nlibrary(data.table)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3mediate)\n\n# download data\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\n\n# make intervention node binary and subsample\nwashb_data <- washb_data[sample(.N, 600), ]\nwashb_data[, tr := as.numeric(tr != \"Control\")]\nnode_list <- list(\n  W = c(\n    \"momage\", \"momedu\", \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\",\n    \"elec\", \"floor\", \"walls\", \"roof\"\n  ),\n  A = \"tr\",\n  Z = c(\"sex\", \"month\", \"aged\"),\n  Y = \"whz\"\n)\nprocessed <- process_missing(washb_data, node_list)\nwashb_data <- processed$data\nnode_list <- processed$node_list\n# SL learners used for continuous data (the nuisance parameter Z)\nenet_contin_learner <- Lrnr_glmnet$new(\n  alpha = 0.5, family = \"gaussian\", nfolds = 3\n)\nlasso_contin_learner <- Lrnr_glmnet$new(\n  alpha = 1, family = \"gaussian\", nfolds = 3\n)\nfglm_contin_learner <- Lrnr_glm_fast$new(family = gaussian())\nmean_learner <- Lrnr_mean$new()\ncontin_learner_lib <- Stack$new(\n  enet_contin_learner, lasso_contin_learner, fglm_contin_learner, mean_learner\n)\nsl_contin_learner <- Lrnr_sl$new(learners = contin_learner_lib)\n\n# SL learners used for binary data (nuisance parameters G and E in this case)\nenet_binary_learner <- Lrnr_glmnet$new(\n  alpha = 0.5, family = \"binomial\", nfolds = 3\n)\nlasso_binary_learner <- Lrnr_glmnet$new(\n  alpha = 1, family = \"binomial\", nfolds = 3\n)\nfglm_binary_learner <- Lrnr_glm_fast$new(family = binomial())\nbinary_learner_lib <- Stack$new(\n  enet_binary_learner, lasso_binary_learner, fglm_binary_learner, mean_learner\n)\nsl_binary_learner <- Lrnr_sl$new(learners = binary_learner_lib)\n\n# create list for treatment and outcome mechanism regressions\nlearner_list <- list(\n  Y = sl_contin_learner,\n  A = sl_binary_learner\n)"},{"path":"causal-mediation-analysis.html","id":"targeted-estimation-of-the-natural-indirect-effect","chapter":"10 Causal Mediation Analysis","heading":"10.5.1 Targeted Estimation of the Natural Indirect Effect","text":"demonstrate calculation NIE , starting instantiating “Spec”\nobject encodes exactly learners use nuisance parameters\n\\(e(\\mid Z, W)\\) \\(\\psi_Z\\). pass Spec object tmle3\nfunction, alongside data, node list (created ), learner list\nindicating machine learning algorithms use estimating nuisance\nparameters based \\(\\) \\(Y\\).Based output, conclude indirect effect treatment\nmediators (sex, month, aged) \n0.0034.","code":"tmle_spec_NIE <- tmle_NIE(\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  psi_Z_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE),\n  max_iter = 1\n)\nwashb_NIE <- tmle3(\n  tmle_spec_NIE, washb_data, node_list, learner_list\n)\nwashb_NIE\nA tmle3_Fit that took 1 step(s)\n   type                  param init_est tmle_est      se    lower   upper\n1:  NIE NIE[Y_{A=1} - Y_{A=0}] 0.002419 0.003376 0.04342 -0.08173 0.08848\n   psi_transformed lower_transformed upper_transformed\n1:        0.003376          -0.08173           0.08848"},{"path":"causal-mediation-analysis.html","id":"targeted-estimation-of-the-natural-direct-effect","chapter":"10 Causal Mediation Analysis","heading":"10.5.2 Targeted Estimation of the Natural Direct Effect","text":"analogous procedure applies estimation NDE, replacing \nSpec object NIE tmle_spec_NDE define learners NDE\nnuisance parameters:, can draw conclusion direct effect treatment\n(paths involving mediators (sex, month, aged)) \n0.0144. Note , together, estimates \nnatural direct indirect effects approximately recover average\ntreatment effect, , based estimates NDE NIE, \nATE roughly\n0.0178.","code":"tmle_spec_NDE <- tmle_NDE(\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  psi_Z_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE),\n  max_iter = 1\n)\nwashb_NDE <- tmle3(\n  tmle_spec_NDE, washb_data, node_list, learner_list\n)\nwashb_NDE\nA tmle3_Fit that took 1 step(s)\n   type                  param init_est tmle_est      se   lower  upper\n1:  NDE NDE[Y_{A=1} - Y_{A=0}]  0.01438  0.01438 0.08564 -0.1535 0.1822\n   psi_transformed lower_transformed upper_transformed\n1:         0.01438           -0.1535            0.1822"},{"path":"causal-mediation-analysis.html","id":"targeted-estimation-of-the-population-intervention-direct-effect","chapter":"10 Causal Mediation Analysis","heading":"10.5.3 Targeted Estimation of the Population Intervention Direct Effect","text":"previously noted, assumptions underlying natural direct indirect\neffects may challenging justify; moreover, effect definitions\ndepend application static intervention treatment,\nsharply limiting flexibility. considering binary treatments,\nincremental propensity score shifts provide alternative class flexible,\nstochastic interventions. ’ll now consider estimating PIDE IPSI\nmodulates odds receiving treatment \\(\\delta = 3\\). \nintervention may interpreted (hypothetically) effect design \nencourages study participants opt receiving treatment, thus\nincreasing relative odds receiving said treatment. exemplify \napproach, postulate motivational intervention triples odds\n(.e., \\(\\delta = 3\\)) receiving treatment individual:Recall , based decomposition outlined previously, PIDE may \ndenoted \\(\\beta_{0,\\text{PIDE}}(\\delta) = \\psi_0(\\delta) - \\E Y\\). Thus,\nTML estimator PIDE, \\(\\beta_{n,\\text{PIDE}}(\\delta)\\) may expressed\ncomposition estimators constituent parameters:\n\\[\\begin{equation*}\n  \\beta_{n,\\text{PIDE}}({\\delta}) = \\psi^{\\star}_{n}(\\delta) -\n  \\frac{1}{n} \\sum_{= 1}^n Y_i.\n\\end{equation*}\\]","code":"\n# set the IPSI multiplicative shift\ndelta_ipsi <- 3\n\n# instantiate tmle3 spec for stochastic mediation\ntmle_spec_pie_decomp <- tmle_medshift(\n  delta = delta_ipsi,\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  phi_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE)\n)\n\n# compute the TML estimate\nwashb_pie_decomp <- tmle3(\n  tmle_spec_pie_decomp, washb_data, node_list, learner_list\n)\nwashb_pie_decomp\n\n# get the PIDE\nwashb_pie_decomp$summary$tmle_est - mean(washb_data[, get(node_list$Y)])"},{"path":"causal-mediation-analysis.html","id":"exercises-5","chapter":"10 Causal Mediation Analysis","heading":"10.6 Exercises","text":"","code":""},{"path":"causal-mediation-analysis.html","id":"review-of-key-concepts-3","chapter":"10 Causal Mediation Analysis","heading":"10.6.1 Review of Key Concepts","text":"Exercise 10.1  Examine WASH Benefits dataset choose different set potential\nmediators effect treatment weight--height Z-score. Using\nnewly chosen set mediators (single mediator), estimate natural\ndirect indirect effects. Provide interpretation estimates.Solution. ForthcomingExercise 10.2  Assess whether additivity natural direct indirect effects holds.\nUsing natural direct indirect effects estimated , sum\nrecover ATE?Solution. ForthcomingExercise 10.3  Evaluate whether assumptions required identification natural\ndirect indirect effects plausible WASH Benefits example. \nparticular, position evaluation terms empirical diagnostics \ntreatment mediator positivity.Solution. Forthcoming","code":""},{"path":"causal-mediation-analysis.html","id":"the-ideas-in-action-1","chapter":"10 Causal Mediation Analysis","heading":"10.6.2 The Ideas in Action","text":"Exercise 10.4  ForthcomingSolution. ForthcomingExercise 10.5  ForthcomingSolution. Forthcoming","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
